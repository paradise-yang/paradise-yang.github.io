<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Pytorch基本教程 | Paradise</title><meta name="author" content="Paradise"><meta name="copyright" content="Paradise"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Pytorch的简单学习，之后便可上手具体项目代码深入学习">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch基本教程">
<meta property="og:url" content="https://paradise-yang.github.io/2022/11/10/pytorch.html">
<meta property="og:site_name" content="Paradise">
<meta property="og:description" content="Pytorch的简单学习，之后便可上手具体项目代码深入学习">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://paradise-yang.github.io/image/pytorch.png">
<meta property="article:published_time" content="2022-11-10T02:00:00.000Z">
<meta property="article:modified_time" content="2022-11-10T06:18:00.674Z">
<meta property="article:author" content="Paradise">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://paradise-yang.github.io/image/pytorch.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://paradise-yang.github.io/2022/11/10/pytorch"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Pytorch基本教程',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-11-10 14:18:00'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/image/pytorch.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Paradise</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Pytorch基本教程</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">Created</span><time datetime="2022-11-10T02:00:00.000Z" title="Created 2022-11-10 10:00:00">2022-11-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Code/">Code</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Pytorch基本教程"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><strong>此博客以机器学习中的一些简单模型为基础，学习Pytorch的基本架构，不做过多深入每个函数的研究与探讨，这部分只需要面向具体的项目代码即可！文章最后面会附上一些相关链接用来参考学习。</strong></p>
<h1 id="Pytorch基础：Tensor（张量）"><a href="#Pytorch基础：Tensor（张量）" class="headerlink" title="Pytorch基础：Tensor（张量）"></a>Pytorch基础：Tensor（张量）</h1><p>对于张量（Tensor），就可以理解为多为矩阵，也只是一种特别的存储方式而已，当然也可以表示一个元素的张量（或，标量）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.tensor([<span class="number">3.1433223</span>]) </span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br><span class="line">tensor.size()</span><br><span class="line">tensor.item()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">3.1433</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>])</span><br><span class="line"><span class="number">3.143322229385376</span></span><br></pre></td></tr></table></figure>
<h2 id="Tensor-的基本类型"><a href="#Tensor-的基本类型" class="headerlink" title="Tensor 的基本类型"></a>Tensor 的基本类型</h2><p>Tensor的基本数据类型有五种： </p>
<ul>
<li>32位浮点型：torch.FloatTensor。 (默认) </li>
<li>64位整型：torch.LongTensor。</li>
<li>32位整型：torch.IntTensor。</li>
<li>16位整型：torch.ShortTensor。</li>
<li>64位浮点型：torch.DoubleTensor。</li>
</ul>
<p>除以上数字类型外，还有 byte和chart型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">3.1426</span>], dtype=torch.float16)</span><br></pre></td></tr></table></figure>
<h2 id="设备转换"><a href="#设备转换" class="headerlink" title="设备转换"></a>设备转换</h2><p>相对于 <code>Numpy</code> 中多维矩阵的表示 <code>ndarray</code> 只能在 CPU 上运行，<code>Tensor</code> 可以在 GPU 上运行。所以有时候需要统一好每个 Tensor 所在设备。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用torch.cuda.is_available()来确定是否有cuda设备</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(device)</span><br><span class="line"><span class="comment">#将tensor传送到设备</span></span><br><span class="line">gpu_b=cpu_b.to(device)</span><br><span class="line">gpu_b.<span class="built_in">type</span>()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cuda</span><br><span class="line"><span class="string">&#x27;torch.cuda.FloatTensor&#x27;</span></span><br></pre></td></tr></table></figure>
<h1 id="Autograd计算梯度数值"><a href="#Autograd计算梯度数值" class="headerlink" title="Autograd计算梯度数值"></a>Autograd计算梯度数值</h1><p>在创建张量时，可以通过设置 <code>requires_grad=True</code>来告诉 Pytorch 对该张量进行自助求导，Pytorch 会记录该张量的每一步操作历史并自动计算梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">x</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[0.0403, 0.5633, 0.2561, 0.4064, 0.9596],</span></span><br><span class="line"><span class="comment">#         [0.6928, 0.1832, 0.5380, 0.6386, 0.8710],</span></span><br><span class="line"><span class="comment">#         [0.5332, 0.8216, 0.8139, 0.1925, 0.4993],</span></span><br><span class="line"><span class="comment">#         [0.2650, 0.6230, 0.5945, 0.3230, 0.0752],</span></span><br><span class="line"><span class="comment">#         [0.0919, 0.4770, 0.4622, 0.6185, 0.2761]], requires_grad=True)</span></span><br><span class="line"></span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">z=x**<span class="number">2</span>+y**<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[3.3891e-01, 4.9468e-01, 8.0797e-02, 2.5656e-01, 2.9529e-01],</span></span><br><span class="line"><span class="comment">#        [7.1946e-01, 1.6977e-02, 1.7965e-01, 3.2656e-01, 1.7665e-01],</span></span><br><span class="line"><span class="comment">#        [3.1353e-01, 2.2096e-01, 1.2251e+00, 5.5087e-01, 5.9572e-02],</span></span><br><span class="line"><span class="comment">#        [1.3015e+00, 3.8029e-01, 1.1103e+00, 4.0392e-01, 2.2055e-01],</span></span><br><span class="line"><span class="comment">#        [8.8726e-02, 6.9701e-01, 8.0164e-01, 9.7221e-01, 4.2239e-04]],</span></span><br><span class="line"><span class="comment">#       grad_fn=&lt;AddBackward0&gt;)</span></span><br></pre></td></tr></table></figure>
<p>在张量进行操作后，<code>grad_fn</code> 已经被赋予了一个新的函数，这个函数引用了一个创建了这个 Tensor 类的 Function 对象。 Tensor 和 Function 互相连接生成了一个非循环图，它记录并且编码了完整的计算历史。每个张量都有一个 <code>`.grad_fn</code> 属性，如果这个张量是用户手动创建的那么这个张量的 <code>grad_fn</code> 是 <code>None</code>。</p>
<p>当计算完成后调用 <code>.backward()</code> 方法自动计算梯度并且将计算结果保存到 <code>grad</code> 属性中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">z.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad,y.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.]]) </span></span><br><span class="line"><span class="comment"># tensor([[1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们的返回值不是一个标量，所以需要输入一个大小相同的张量作为参数，</span></span><br><span class="line"><span class="comment"># 这里我们用ones_like函数根据x生成一个张量</span></span><br><span class="line">z.backward(torch.ones_like(x))</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[0.2087, 1.3554, 0.5560, 1.0009, 0.9931],</span></span><br><span class="line"><span class="comment">#        [1.2655, 0.1223, 0.8008, 1.1127, 0.7261],</span></span><br><span class="line"><span class="comment">#        [1.1052, 0.2579, 1.8006, 0.1544, 0.3646],</span></span><br><span class="line"><span class="comment">#        [1.8855, 1.2296, 1.9061, 0.9313, 0.0648],</span></span><br><span class="line"><span class="comment">#        [0.5952, 1.6190, 0.8430, 1.9213, 0.0322]])</span></span><br></pre></td></tr></table></figure>
<p>我们可以使用 <code>with torch.no_grad()</code> 上下文管理器临时禁止对已设置 <code>requires_grad=True</code> 的张量进行自动求导。这个方法在测试集计算准确率的时候会经常用到，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="built_in">print</span>((x +y*<span class="number">2</span>).requires_grad)</span><br><span class="line">   </span><br><span class="line"><span class="comment"># False</span></span><br></pre></td></tr></table></figure>
<p>使用 <code>.no_grad()</code> 进行嵌套后，代码不会跟踪历史记录，也就是说保存的这部分记录会减少内存的使用量并且会加快少许的运算速度。</p>
<h1 id="数据的加载和预处理"><a href="#数据的加载和预处理" class="headerlink" title="数据的加载和预处理"></a>数据的加载和预处理</h1><p>PyTorch通过 <code>torch.utils.data</code> 对一般常用的数据加载进行了封装，可以很容易地实现多线程数据预读和批量加载。 并且 <code>torchvision</code> 已经预先实现了常用图像数据集，包括前面使用过的CIFAR-10，ImageNet、COCO、MNIST、LSUN等数据集，可通过 <code>torchvision.datasets</code> 方便的调用。这里不具体介绍。</p>
<h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>Dataset 是一个抽象类，为了能够方便的读取，需要将要使用的数据包装为 Dataset 类。 自定义的 Dataset 需要继承它并且实现两个成员方法： </p>
<pre><code>- `__getitem__()` 该方法定义用索引(`0` 到 `len(self)`)获取一条数据或一个样本
-  `__len__()` 该方法返回数据集的总长度
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#引用</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义一个数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BulldozerDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 数据集演示 &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, csv_file</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;实现初始化方法，在初始化的时候将数据读载入&quot;&quot;&quot;</span></span><br><span class="line">        self.df=pd.read_csv(csv_file)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        返回df的长度</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.df)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        根据 idx 返回一行数据</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> self.df.iloc[idx].SalePrice</span><br><span class="line"><span class="comment"># ------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 实例化一个对象访问它</span></span><br><span class="line">ds_demo= BulldozerDataset(<span class="string">&#x27;median_benchmark.csv&#x27;</span>)</span><br><span class="line"><span class="comment"># 实现了 __len__ 方法所以可以直接使用len获取数据总数</span></span><br><span class="line"><span class="built_in">len</span>(ds_demo)</span><br><span class="line"><span class="comment"># 用索引可以直接访问对应的数据，对应 __getitem__ 方法</span></span><br><span class="line">ds_demo[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>自定义的数据集已经创建好了，下面我们使用官方提供的数据载入器，读取数据。</p>
<h2 id="Dataloader"><a href="#Dataloader" class="headerlink" title="Dataloader"></a>Dataloader</h2><p>DataLoader 为我们提供了对 Dataset 的读取操作，常用参数有：</p>
<ul>
<li>batch_size(每个batch的大小)、 </li>
<li>shuffle(是否进行shuffle操作)、 </li>
<li>num_workers(加载数据的时候使用几个子进程)。</li>
</ul>
<p>下面做一个简单的操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">dl = torch.utils.data.DataLoader(ds_demo, batch_size=<span class="number">10</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataLoader返回的是一个可迭代对象，我们可以使用迭代器分次获取数据</span></span><br><span class="line">idata=<span class="built_in">iter</span>(dl)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(idata))</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000.], dtype=torch.float64)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 常见的用法是使用for循环对其进行遍历</span></span><br><span class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(dl):</span><br><span class="line">    <span class="built_in">print</span>(i,data)</span><br></pre></td></tr></table></figure>
<p>我们已经可以通过 Dataset 定义数据集，并使用 Datalorder 载入和遍历数据集，除了这些以外， PyTorch 还提供能 torcvision 的计算机视觉扩展包。</p>
<h2 id="torchvision-包"><a href="#torchvision-包" class="headerlink" title="torchvision 包"></a>torchvision 包</h2><p>torchvision 是PyTorch中专门用来处理图像的库。</p>
<h3 id="torchvision-datasets"><a href="#torchvision-datasets" class="headerlink" title="torchvision.datasets"></a>torchvision.datasets</h3><p>torchvision.datasets 可以理解为PyTorch团队自定义的dataset，这些dataset帮我们提前处理好了很多的图片数据集，我们拿来就可以直接使用： - MNIST - COCO - Captions - Detection - LSUN - ImageFolder - Imagenet-12 - CIFAR - STL10 - SVHN - PhotoTour 我们可以直接使用，示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> datasets</span><br><span class="line">trainset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, <span class="comment"># 表示 MNIST 数据的加载的目录</span></span><br><span class="line">                          train=<span class="literal">True</span>,    <span class="comment"># 表示是否加载数据库的训练集，false的时候加载测试集</span></span><br><span class="line">                          download=<span class="literal">True</span>, <span class="comment"># 表示是否自动下载 MNIST 数据集</span></span><br><span class="line">                          transform=<span class="literal">None</span>)<span class="comment"># 表示是否需要对数据进行预处理，none为不进行预处理</span></span><br></pre></td></tr></table></figure>
<h3 id="torchvision-models"><a href="#torchvision-models" class="headerlink" title="torchvision.models"></a>torchvision.models</h3><p>torchvision 不仅提供了常用图片数据集，还提供了训练好的模型，可以加载之后，直接使用，或者在进行迁移学习 <code>torchvision.models</code> 模块的 子模块中包含以下模型结构。 - AlexNet - VGG - ResNet - SqueezeNet - DenseNet</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#我们直接可以使用训练好的模型，当然这个与datasets相同，都是需要从服务器下载的</span></span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line">resnet18 = models.resnet18(pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="torchvision-transforms"><a href="#torchvision-transforms" class="headerlink" title="torchvision.transforms"></a>torchvision.transforms</h3><p>transforms 模块提供了一般的图像转换操作类，用作数据处理和数据增强</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="keyword">as</span> transforms</span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.RandomCrop(<span class="number">32</span>, padding=<span class="number">4</span>), <span class="comment">#先四周填充0，在把图像随机裁剪成32*32</span></span><br><span class="line">    transforms.RandomHorizontalFlip(),    <span class="comment">#图像一半的概率翻转，一半的概率不翻转</span></span><br><span class="line">    transforms.RandomRotation((-<span class="number">45</span>,<span class="number">45</span>)),  <span class="comment">#随机旋转</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>), (<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>)), <span class="comment">#R,G,B每层的归一化用到的均值和方差</span></span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>肯定有人会问：(0.485, 0.456, 0.406), (0.2023, 0.1994, 0.2010) 这几个数字是什么意思？官方的这个帖子有详细的说明: <a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/21">https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/21</a> 这些都是根据ImageNet训练的归一化参数，可以直接使用，我们认为这个是固定值就可以。</p>
<h1 id="神经网络包nn和优化器optimizer"><a href="#神经网络包nn和优化器optimizer" class="headerlink" title="神经网络包nn和优化器optimizer"></a>神经网络包nn和优化器optimizer</h1><p><code>torch.nn</code> 是专门为神经网络设计的模块化接口。<code>nn</code> 构建于 <code>Autograd</code> 之上，可用来定义和运行神经网络。 这里我们主要介绍几个一些常用的类。除了nn别名以外，我们还引用了nn.functional，这个包中包含了神经网络中使用的一些常用函数，这些函数的特点是，不具有可学习的参数(如ReLU，pool，DropOut等)，这些函数可以放在构造函数中，也可以不放，但是这里建议不放。一般情况下我们会<strong>将nn.functional 设置为大写的F</strong>，这样缩写方便调用。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 首先要引入相关的包</span><br><span class="line">import torch</span><br><span class="line"># 引入torch.nn并指定别名</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br></pre></td></tr></table></figure>
<h2 id="定义一个网络"><a href="#定义一个网络" class="headerlink" title="定义一个网络"></a>定义一个网络</h2><p>PyTorch 中已经为我们准备好了现成的网络模型，只要继承 <code>nn.Module</code>，并实现它的 <code>forward</code> 方法，PyTorch 会根据 <code>autograd</code>，自动实现 <code>backward</code> 函数，在 <code>forward</code> 函数中可使用任何 tensor 支持的函数，还可以使用 if、for 循环、print、log 等 Python 语法，写法和标准的 Python 写法一致。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># nn.Module子类的函数必须在构造函数中执行父类的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 卷积层 &#x27;1&#x27;表示输入图片为单通道， &#x27;6&#x27;表示输出通道数，&#x27;3&#x27;表示卷积核为3*3</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">3</span>) </span><br><span class="line">        <span class="comment">#线性层，输入1350个特征，输出10个特征</span></span><br><span class="line">        self.fc1   = nn.Linear(<span class="number">1350</span>, <span class="number">10</span>)  <span class="comment">#这里的1350是如何计算的呢？这就要看后面的forward函数</span></span><br><span class="line">    <span class="comment">#正向传播 </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): </span><br><span class="line">        <span class="built_in">print</span>(x.size()) <span class="comment"># 结果：[1, 1, 32, 32]</span></span><br><span class="line">        <span class="comment"># 卷积 -&gt; 激活 -&gt; 池化 </span></span><br><span class="line">        x = self.conv1(x) <span class="comment">#根据卷积的尺寸计算公式，计算结果是30，具体计算公式后面第二章第四节 卷积神经网络 有详细介绍。</span></span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        <span class="built_in">print</span>(x.size()) <span class="comment"># 结果：[1, 6, 30, 30]</span></span><br><span class="line">        x = F.max_pool2d(x, (<span class="number">2</span>, <span class="number">2</span>)) <span class="comment">#我们使用池化层，计算结果是15</span></span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        <span class="built_in">print</span>(x.size()) <span class="comment"># 结果：[1, 6, 15, 15]</span></span><br><span class="line">        <span class="comment"># reshape，‘-1’表示自适应</span></span><br><span class="line">        <span class="comment">#这里做的就是压扁的操作 就是把后面的[1, 6, 15, 15]压扁，变为 [1, 1350]</span></span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], -<span class="number">1</span>) </span><br><span class="line">        <span class="built_in">print</span>(x.size()) <span class="comment"># 这里就是fc1层的的输入1350 </span></span><br><span class="line">        x = self.fc1(x)        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Net(</span></span><br><span class="line"><span class="comment">#   (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=1350, out_features=10, bias=True)</span></span><br><span class="line"><span class="comment"># )</span></span><br></pre></td></tr></table></figure>
<p>网络的可学习参数通过 <code>net.parameters()</code> 返回</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> parameters <span class="keyword">in</span> net.parameters():</span><br><span class="line">    <span class="built_in">print</span>(parameters)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># ----------输出----------</span></span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[[[ <span class="number">0.2745</span>,  <span class="number">0.2594</span>,  <span class="number">0.0171</span>],</span><br><span class="line">          [ <span class="number">0.0429</span>,  <span class="number">0.3013</span>, -<span class="number">0.0208</span>],</span><br><span class="line">          [ <span class="number">0.1459</span>, -<span class="number">0.3223</span>,  <span class="number">0.1797</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ <span class="number">0.1847</span>,  <span class="number">0.0227</span>, -<span class="number">0.1919</span>],</span><br><span class="line">          [-<span class="number">0.0210</span>, -<span class="number">0.1336</span>, -<span class="number">0.2176</span>],</span><br><span class="line">          [-<span class="number">0.2164</span>, -<span class="number">0.1244</span>, -<span class="number">0.2428</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ <span class="number">0.1042</span>, -<span class="number">0.0055</span>, -<span class="number">0.2171</span>],</span><br><span class="line">          [ <span class="number">0.3306</span>, -<span class="number">0.2808</span>,  <span class="number">0.2058</span>],</span><br><span class="line">          [ <span class="number">0.2492</span>,  <span class="number">0.2971</span>,  <span class="number">0.2277</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ <span class="number">0.2134</span>, -<span class="number">0.0644</span>, -<span class="number">0.3044</span>],</span><br><span class="line">          [ <span class="number">0.0040</span>,  <span class="number">0.0828</span>, -<span class="number">0.2093</span>],</span><br><span class="line">          [ <span class="number">0.0204</span>,  <span class="number">0.1065</span>,  <span class="number">0.1168</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ <span class="number">0.1651</span>, -<span class="number">0.2244</span>,  <span class="number">0.3072</span>],</span><br><span class="line">          [-<span class="number">0.2301</span>,  <span class="number">0.2443</span>, -<span class="number">0.2340</span>],</span><br><span class="line">          [ <span class="number">0.0685</span>,  <span class="number">0.1026</span>,  <span class="number">0.1754</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ <span class="number">0.1691</span>, -<span class="number">0.0790</span>,  <span class="number">0.2617</span>],</span><br><span class="line">          [ <span class="number">0.1956</span>,  <span class="number">0.1477</span>,  <span class="number">0.0877</span>],</span><br><span class="line">          [ <span class="number">0.0538</span>, -<span class="number">0.3091</span>,  <span class="number">0.2030</span>]]]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([ <span class="number">0.2355</span>,  <span class="number">0.2949</span>, -<span class="number">0.1283</span>, -<span class="number">0.0848</span>,  <span class="number">0.2027</span>, -<span class="number">0.3331</span>],</span><br><span class="line">       requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ <span class="number">2.0555e-02</span>, -<span class="number">2.1445e-02</span>, -<span class="number">1.7981e-02</span>,  ..., -<span class="number">2.3864e-02</span>,</span><br><span class="line">          <span class="number">8.5149e-03</span>, -<span class="number">6.2071e-04</span>],</span><br><span class="line">        [-<span class="number">1.1755e-02</span>,  <span class="number">1.0010e-02</span>,  <span class="number">2.1978e-02</span>,  ...,  <span class="number">1.8433e-02</span>,</span><br><span class="line">          <span class="number">7.1362e-03</span>, -<span class="number">4.0951e-03</span>],</span><br><span class="line">        [ <span class="number">1.6187e-02</span>,  <span class="number">2.1623e-02</span>,  <span class="number">1.1840e-02</span>,  ...,  <span class="number">5.7059e-03</span>,</span><br><span class="line">         -<span class="number">2.7165e-02</span>,  <span class="number">1.3463e-03</span>],</span><br><span class="line">        ...,</span><br><span class="line">        [-<span class="number">3.2552e-03</span>,  <span class="number">1.7277e-02</span>, -<span class="number">1.4907e-02</span>,  ...,  <span class="number">7.4232e-03</span>,</span><br><span class="line">         -<span class="number">2.7188e-02</span>, -<span class="number">4.6431e-03</span>],</span><br><span class="line">        [-<span class="number">1.9786e-02</span>, -<span class="number">3.7382e-03</span>,  <span class="number">1.2259e-02</span>,  ...,  <span class="number">3.2471e-03</span>,</span><br><span class="line">         -<span class="number">1.2375e-02</span>, -<span class="number">1.6372e-02</span>],</span><br><span class="line">        [-<span class="number">8.2350e-03</span>,  <span class="number">4.1301e-03</span>, -<span class="number">1.9192e-03</span>,  ..., -<span class="number">2.3119e-05</span>,</span><br><span class="line">          <span class="number">2.0167e-03</span>,  <span class="number">1.9528e-02</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([ <span class="number">0.0162</span>, -<span class="number">0.0146</span>, -<span class="number">0.0218</span>,  <span class="number">0.0212</span>, -<span class="number">0.0119</span>, -<span class="number">0.0142</span>, -<span class="number">0.0079</span>,  <span class="number">0.0171</span>,</span><br><span class="line">         <span class="number">0.0205</span>,  <span class="number">0.0164</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p><code>net.named_parameters</code> 可同时返回可学习的参数及名称</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name,parameters <span class="keyword">in</span> net.named_parameters():</span><br><span class="line"></span><br><span class="line"><span class="comment"># conv1.weight : torch.Size([6, 1, 3, 3])</span></span><br><span class="line"><span class="comment"># conv1.bias : torch.Size([6])</span></span><br><span class="line"><span class="comment"># fc1.weight : torch.Size([10, 1350])</span></span><br><span class="line"><span class="comment"># fc1.bias : torch.Size([10])</span></span><br></pre></td></tr></table></figure>
<p>forward函数的输入和输出都是Tensor</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>) <span class="comment"># 这里的对应前面fforward的输入是32</span></span><br><span class="line">out = net(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">input</span>.size()</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.Size([1, 1, 32, 32])</span></span><br><span class="line"></span><br><span class="line">out.size()</span><br><span class="line"></span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">6</span>, <span class="number">30</span>, <span class="number">30</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">6</span>, <span class="number">15</span>, <span class="number">15</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">1350</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p>在反向传播前，先要将所有参数的梯度清零</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad() </span><br><span class="line">out.backward(torch.ones(<span class="number">1</span>,<span class="number">10</span>)) <span class="comment"># 反向传播的实现是PyTorch自动实现的，我们只要调用这个函数即可</span></span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：<code>torch.nn</code> 只支持 <code>mini-batches</code>，不支持一次只输入一个样本，即一次必须是一个batch。也就是说，就算我们输入一个样本，也会对样本进行分批，所以，所有的输入都会增加一个维度，我们对比下刚才的 <code>input</code>，<code>nn</code> 中定义为3维，但是我们人工创建时多增加了一个维度，变为了4维，最前面的1即为batch-size。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>在nn中PyTorch还预制了常用的损失函数，下面我们用MSELoss用来计算均方误差：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">y = torch.arange(<span class="number">0</span>,<span class="number">10</span>).view(<span class="number">1</span>,<span class="number">10</span>).<span class="built_in">float</span>()</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">loss = criterion(out, y)</span><br><span class="line"><span class="comment">#loss是个scalar，我们可以直接用item获取到他的python类型的数值</span></span><br><span class="line"><span class="built_in">print</span>(loss.item()) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 28.92203712463379</span></span><br></pre></td></tr></table></figure>
<h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><p>在反向传播计算完所有参数的梯度后，还需要使用优化方法来更新网络的权重和参数，例如随机梯度下降法(SGD)的更新策略如下：</p>
<p><code>weight = weight - learning_rate * gradient</code></p>
<p>在 <code>torch.optim</code> 中实现大多数的优化方法，例如 RMSProp、Adam、SGD等，下面我们使用SGD做个简单的样例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim</span><br><span class="line"></span><br><span class="line">out = net(<span class="built_in">input</span>) <span class="comment"># 这里调用的时候会打印出我们在forword函数中打印的x的大小</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">loss = criterion(out, y)</span><br><span class="line"><span class="comment">#新建一个优化器，SGD只需要要调整的参数和学习率</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr = <span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># 先梯度清零(与net.zero_grad()效果一样)</span></span><br><span class="line">optimizer.zero_grad() </span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment">#更新参数</span></span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<p>这样，神经网络的数据的一个完整的传播就已经通过PyTorch实现了。</p>
<h1 id="深度学习基础"><a href="#深度学习基础" class="headerlink" title="深度学习基础"></a>深度学习基础</h1><h2 id="监督学习和无监督学习"><a href="#监督学习和无监督学习" class="headerlink" title="监督学习和无监督学习"></a>监督学习和无监督学习</h2><p>监督学习、无监督学习、半监督学习、强化学习是我们日常接触到的常见的四个机器学习方法：</p>
<ul>
<li>监督学习：通过已有的训练样本（即已知数据以及其对应的输出）去训练得到一个最优模型（这个模型属于某个函数的集合，最优则表示在某个评价准则下是最佳的），再利用这个模型将所有的输入映射为相应的输出。</li>
<li>无监督学习：它与监督学习的不同之处，在于我们事先没有任何训练样本，而需要直接对数据进行建模。</li>
<li>半监督学习 ：在训练阶段结合了大量未标记的数据和少量标签数据。与使用所有标签数据的模型相比，使用训练集的训练模型在训练时可以更为准确。</li>
<li>强化学习：我们设定一个回报函数（reward function），通过这个函数来确认否越来越接近目标，类似我们训练宠物，如果做对了就给他奖励，做错了就给予惩罚，最后来达到我们的训练目的。</li>
</ul>
<p>这里我们只着重介绍监督学习，因为我们后面的绝大部们课程都是使用的监督学习的方法，在训练和验证时输入的数据既包含输入x，又包含x对应的输出y，即学习数据已经事先给出了正确答案。</p>
<h2 id="线性回归-（Linear-Regreesion）"><a href="#线性回归-（Linear-Regreesion）" class="headerlink" title="线性回归 （Linear Regreesion）"></a>线性回归 （Linear Regreesion）</h2><p>这里不解释具体原理，直接看写法，体会训练过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意，这里我们使用了一个新库叫 seaborn 如果报错找不到包的话请使用pip install seaborn 来进行安装</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Linear, Module, MSELoss</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> SGD</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面我生成一些随机的点，来作为我们的训练数据，回归：y = 5*x + 7</span></span><br><span class="line">x = np.random.rand(<span class="number">256</span>)</span><br><span class="line">noise = np.random.randn(<span class="number">256</span>) / <span class="number">4</span></span><br><span class="line">y = x * <span class="number">5</span> + <span class="number">7</span> + noise</span><br><span class="line">df = pd.DataFrame()</span><br><span class="line">df[<span class="string">&#x27;x&#x27;</span>] = x</span><br><span class="line">df[<span class="string">&#x27;y&#x27;</span>] = y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们随机生成了一些点，下面将使用PyTorch建立一个线性的模型来对其进行拟合，这就是所说的训练的过程，由于只有一层线性模型，所以我们就直接使用了:</span></span><br><span class="line">model=Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 其中参数(1,1)代表输入输出的特征(feature)数量都是1</span></span><br><span class="line"><span class="comment"># Linear 模型的表达式是y=wx+b，其中w代表权重，b代表偏置</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数我们使用均方损失函数</span></span><br><span class="line">criterion = MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器我们选择最常见的优化方法 SGD，就是每一次迭代计算 mini-batch 的梯度，然后对参数进行更新，学习率 0.01</span></span><br><span class="line">optim = SGD(model.parameters(), lr = <span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练3000次</span></span><br><span class="line">epochs = <span class="number">3000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备训练数据: x_train, y_train 的形状是(256, 1)， </span></span><br><span class="line"><span class="comment"># 代表mini-batch大小为256，feature为1. astype(&#x27;float32&#x27;) 是为了下一步可以直接转换为 torch.float</span></span><br><span class="line">x_train = x.reshape(-<span class="number">1</span>, <span class="number">1</span>).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">y_train = y.reshape(-<span class="number">1</span>, <span class="number">1</span>).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 整理输入和输出的数据，这里输入和输出一定要是torch的Tensor类型</span></span><br><span class="line">    inputs = torch.from_numpy(x_train)</span><br><span class="line">    labels = torch.from_numpy(y_train)</span><br><span class="line">    <span class="comment">#使用模型进行预测</span></span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    <span class="comment">#梯度置0，否则会累加</span></span><br><span class="line">    optim.zero_grad()</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 使用优化器默认方法优化</span></span><br><span class="line">    optim.step()</span><br><span class="line">    <span class="keyword">if</span> (i%<span class="number">100</span>==<span class="number">0</span>):</span><br><span class="line">        <span class="comment">#每 100次打印一下损失函数，看看效果</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch &#123;&#125;, loss &#123;:1.4f&#125;&#x27;</span>.<span class="built_in">format</span>(i,loss.data.item()))</span><br></pre></td></tr></table></figure>
<p>训练完成了，看一下训练的成果是多少。用 <code>model.parameters()</code> 提取模型参数。 $w$， $b$ 是我们所需要训练的模型参数，我们期望的数据 $w=5$，$b=7$ 可以做一下对比</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[w, b] = model.parameters()</span><br><span class="line"><span class="built_in">print</span> (w.item(),b.item())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.994358062744141 7.0252156257629395</span></span><br></pre></td></tr></table></figure>
<h2 id="损失函数-Loss-Function"><a href="#损失函数-Loss-Function" class="headerlink" title="损失函数(Loss Function)"></a>损失函数(Loss Function)</h2><p>损失函数（loss function）是用来估量模型的预测值(我们例子中的output)与真实值（例子中的y_train）的不一致程度，它是一个非负实值函数，损失函数越小，模型的鲁棒性就越好。 我们训练模型的过程，就是通过不断的迭代计算，使用梯度下降的优化算法，使得损失函数越来越小。损失函数越小就表示算法达到意义上的最优。</p>
<p>这里有一个重点：因为PyTorch是使用mini-batch来进行计算的，所以损失函数的计算出来的结果已经对mini-batch取了平均。</p>
<p>常见（PyTorch内置）的损失函数有以下几个：</p>
<h3 id="nn-L1Loss"><a href="#nn-L1Loss" class="headerlink" title="nn.L1Loss:"></a>nn.L1Loss:</h3><p>输入x和目标y之间差的绝对值，要求 x 和 y 的维度要一样（可以是向量或者矩阵），得到的 loss 维度也是对应一样的</p>
<script type="math/tex; mode=display">
loss(x,y)=1/n\sum|x_i-y_i|</script><h3 id="nn-NLLLoss"><a href="#nn-NLLLoss" class="headerlink" title="nn.NLLLoss:"></a>nn.NLLLoss:</h3><p>用于多分类的负对数似然损失函数$loss(x, class) = -x[class]$；</p>
<p>NLLLoss中如果传递了weights参数，会对损失进行加权，公式就变成了</p>
<script type="math/tex; mode=display">
loss(x, class) = -weights[class] * x[class]</script><h3 id="n-MSELoss"><a href="#n-MSELoss" class="headerlink" title="n.MSELoss:"></a>n.MSELoss:</h3><p>均方损失函数 ，输入x和目标y之间均方差</p>
<script type="math/tex; mode=display">
loss(x,y)=1/n\sum(x_i-y_i)^2</script><h3 id="nn-CrossEntropyLoss"><a href="#nn-CrossEntropyLoss" class="headerlink" title="nn.CrossEntropyLoss:"></a>nn.CrossEntropyLoss:</h3><p>多分类用的交叉熵损失函数，LogSoftMax 和 NLLLoss 集成到一个类中，会调用 nn.NLLLoss 函数，我们可以理解为 CrossEntropyLoss()=log_softmax() + NLLLoss()</p>
<script type="math/tex; mode=display">
 \begin{aligned} loss(x, class) &= -\text{log}\frac{exp(x[class])}{\sum_j exp(x[j]))}\ &= -x[class] + log(\sum_j exp(x[j])) \end{aligned}</script><p>因为使用了NLLLoss，所以也可以传入weight参数，这时loss的计算公式变为：</p>
<script type="math/tex; mode=display">
loss(x, class) = weights[class] * (-x[class] + log(\sum_j exp(x[j])))</script><p>所以一般多分类的情况会使用这个损失函数；</p>
<h3 id="nn-BCELoss"><a href="#nn-BCELoss" class="headerlink" title="nn.BCELoss:"></a>nn.BCELoss:</h3><p>计算 x 与 y 之间的二进制交叉熵。$loss(o,t)=-\frac{1}{n}\sum_i(t[i] <em>log(o[i])+(1-t[i])</em> log(1-o[i]))$ 与NLLLoss类似，也可以添加权重参数：</p>
<script type="math/tex; mode=display">
loss(o,t)=-\frac{1}{n}\sum_iweights[i] *(t[i]* log(o[i])+(1-t[i])* log(1-o[i]))</script><p>用的时候需要在该层前面加上 Sigmoid 函数。</p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>在介绍损失函数的时候我们已经说了，梯度下降是一个使损失函数越来越小的优化算法，在无求解机器学习算法的模型参数，即约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一。所以梯度下降是我们目前所说的机器学习的核心，了解了它的含义，也就了解了机器学习算法的含义。</p>
<h3 id="Mini-batch的梯度下降法"><a href="#Mini-batch的梯度下降法" class="headerlink" title="Mini-batch的梯度下降法"></a>Mini-batch的梯度下降法</h3><p>对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候处理速度会很慢，而且也不可能一次的载入到内存或者显存中，所以我们会把大数据集分成小数据集，一部分一部分的训练，这个训练子集即称为Mini-batch。 在PyTorch中就是使用这种方法进行的训练，可以看看上一章中关于dataloader的介绍里面的batch_size就是我们一个Mini-batch的大小。</p>
<p>为了介绍的更简洁，使用 吴恩达老师的 <a target="_blank" rel="noopener" href="https://www.deeplearning.ai/deep-learning-specialization/">deeplearning.ai</a> 课程板书。</p>
<p>对于普通的梯度下降法，一个epoch只能进行一次梯度下降；而对于Mini-batch梯度下降法，一个epoch可以进行Mini-batch的个数次梯度下降。 <img src="https://handbook.pytorch.wiki/chapter2/2.png" alt="img"> 普通的batch梯度下降法和Mini-batch梯度下降法代价函数的变化趋势，如下图所示： <img src="https://handbook.pytorch.wiki/chapter2/3.png" alt="img"></p>
<ul>
<li><p>如果训练样本的大小比较小时，能够一次性的读取到内存中，那我们就不需要使用Mini-batch，</p>
</li>
<li><p>如果训练样本的大小比较大时，一次读入不到内存或者现存中，那我们必须要使用 Mini-batch来分批的计算 </p>
</li>
<li>Mini-batch size的计算规则如下，在内存允许的最大情况下使用2的N次方个size <img src="https://handbook.pytorch.wiki/chapter2/4.png" alt="img"></li>
</ul>
<p><code>torch.optim</code>是一个实现了各种优化算法的库。大部分常用优化算法都有实现，我们直接调用即可。</p>
<h3 id="torch-optim-SGD"><a href="#torch-optim-SGD" class="headerlink" title="torch.optim.SGD"></a>torch.optim.SGD</h3><p>随机梯度下降算法，带有动量（momentum）的算法作为一个可选参数可以进行设置，样例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#lr参数为学习率，对于SGD来说一般选择0.1 0.01.0.001，如何设置会在后面实战的章节中详细说明</span></span><br><span class="line"><span class="comment">##如果设置了momentum，就是带有动量的SGD，可以不设置</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<h3 id="torch-optim-RMSprop"><a href="#torch-optim-RMSprop" class="headerlink" title="torch.optim.RMSprop"></a>torch.optim.RMSprop</h3><p>除了以上的带有动量Momentum梯度下降法外，RMSprop（root mean square prop）也是一种可以加快梯度下降的算法，利用RMSprop算法，可以减小某些维度梯度更新波动较大的情况，使其梯度下降的速度变得更快</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#我们的课程基本不会使用到RMSprop所以这里只给一个实例</span></span><br><span class="line">optimizer = torch.optim.RMSprop(model.parameters(), lr=<span class="number">0.01</span>, alpha=<span class="number">0.99</span>)</span><br></pre></td></tr></table></figure>
<h3 id="torch-optim-Adam"><a href="#torch-optim-Adam" class="headerlink" title="torch.optim.Adam"></a>torch.optim.Adam</h3><p>Adam 优化算法的基本思想就是将 Momentum 和 RMSprop 结合起来形成的一种适用于不同深度学习结构的优化算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里的lr，betas，还有eps都是用默认值即可，所以Adam是一个使用起来最简单的优化方法</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>, betas=(<span class="number">0.9</span>, <span class="number">0.999</span>), eps=<span class="number">1e-08</span>)</span><br></pre></td></tr></table></figure>
<h2 id="方差-偏差"><a href="#方差-偏差" class="headerlink" title="方差/偏差"></a>方差/偏差</h2><ul>
<li>偏差度量了学习算法的期望预测与真实结果的偏离程序，即刻画了学习算法本身的拟合能力</li>
<li>方差度量了同样大小的训练集的变动所导致的学习性能的变化，即模型的泛化能力 <img src="https://handbook.pytorch.wiki/chapter2/5.png" alt="img"></li>
</ul>
<p>从图中我们可以看出 - 高偏差（high bias）的情况，一般称为欠拟合（underfitting），即我们的模型并没有很好的去适配现有的数据，拟合度不够。 - 高方差（high variance）的情况一般称作过拟合（overfitting），即模型对于训练数据拟合度太高了，失去了泛化的能力。</p>
<p>如何解决这两种情况呢？</p>
<ul>
<li><p>欠拟合： </p>
<ul>
<li>增加网络结构，如增加隐藏层数目； </li>
<li>训练更长时间；</li>
<li>寻找合适的网络架构，使用更大的NN结构；</li>
</ul>
</li>
<li><p>过拟合 ： </p>
<ul>
<li>使用更多的数据；</li>
<li>正则化（ regularization）； </li>
<li>寻找合适的网络结构；</li>
</ul>
</li>
</ul>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>利用正则化来解决High variance 的问题，正则化是在 Cost function 中加入一项正则化项，惩罚模型的复杂度，这里我们简单的介绍一下正则化的概念</p>
<h3 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h3><p>损失函数基础上加上权重参数的绝对值 $L=E_{in}+\lambda{\sum_j} \left|w_j\right|$；</p>
<h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p>损失函数基础上加上权重参数的平方和 $L=E_{in}+\lambda{\sum_j} w^2_j$；</p>
<p>需要说明的是：$L_1$ 相比于 $L_2$ 会更容易获得稀疏解。</p>
<h1 id="卷积神经网络经典模型LeNet-5"><a href="#卷积神经网络经典模型LeNet-5" class="headerlink" title="卷积神经网络经典模型LeNet-5"></a>卷积神经网络经典模型LeNet-5</h1><p>卷积神经网路的开山之作，麻雀虽小，但五脏俱全，卷积层、pooling层、全连接层，这些都是现代CNN网络的基本组件 - 用卷积提取空间特征； </p>
<ul>
<li>由空间平均得到子样本； </li>
<li>用 tanh 或 sigmoid 得到非线性； </li>
<li>用 multi-layer neural network（MLP）作为最终分类器；</li>
<li>层层之间用稀疏的连接矩阵，以避免大的计算成本。 <img src="https://handbook.pytorch.wiki/chapter2/lenet5.jpg" alt="img"></li>
</ul>
<p>输入：图像Size为32*32。</p>
<p>输出：10个类别，分别为0-9数字的概率</p>
<ol>
<li>C1层是一个卷积层，有6个卷积核（提取6种局部特征），核大小为5 * 5</li>
<li>S2层是pooling层，下采样（区域:2 * 2 ）降低网络训练参数及模型的过拟合程度。</li>
<li>C3层是第二个卷积层，使用16个卷积核，核大小:5 * 5 提取特征</li>
<li>S4层也是一个pooling层，区域:2*2</li>
<li>C5层是最后一个卷积层，卷积核大小:5 * 5 卷积核种类:120</li>
<li>最后使用全连接层，将C5的120个特征进行分类，最后输出0-9的概率</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet5</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LeNet5, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 5x5 square convolution</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>) <span class="comment"># 这里论文上写的是conv,官方教程用了线性层</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># If the size is a square you can only specify a single number</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">num_flat_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = LeNet5()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"></span><br><span class="line"><span class="comment"># LeNet5(</span></span><br><span class="line"><span class="comment">#   (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#   (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=400, out_features=120, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=120, out_features=84, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc3): Linear(in_features=84, out_features=10, bias=True)</span></span><br><span class="line"><span class="comment"># )</span></span><br></pre></td></tr></table></figure>
<h1 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h1><p><a target="_blank" rel="noopener" href="http://www.feiguyunai.com/index.php/2019/06/13/python-ml-pytorch/">《Python深度学习基于PyTorch》 | Python技术交流与分享 (feiguyunai.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/ShusenTang/Dive-into-DL-PyTorch">ShusenTang/Dive-into-DL-PyTorch: 本项目将《动手学深度学习》(Dive into Deep Learning)原书中的MXNet实现改为PyTorch实现。 (github.com)</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://paradise-yang.github.io">Paradise</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://paradise-yang.github.io/2022/11/10/pytorch.html">https://paradise-yang.github.io/2022/11/10/pytorch.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/Pytorch/">Pytorch</a></div><div class="post_share"><div class="social-share" data-image="/image/pytorch.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2022/11/07/network.html"><img class="next-cover" src="/image/all.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">神经网络简介</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Paradise</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/paradise-yang"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/paradise-yang" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:ly_1207@mail.ustc.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Pytorch%E5%9F%BA%E7%A1%80%EF%BC%9ATensor%EF%BC%88%E5%BC%A0%E9%87%8F%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">Pytorch基础：Tensor（张量）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensor-%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%B1%BB%E5%9E%8B"><span class="toc-number">1.1.</span> <span class="toc-text">Tensor 的基本类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BE%E5%A4%87%E8%BD%AC%E6%8D%A2"><span class="toc-number">1.2.</span> <span class="toc-text">设备转换</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Autograd%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6%E6%95%B0%E5%80%BC"><span class="toc-number">2.</span> <span class="toc-text">Autograd计算梯度数值</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8A%A0%E8%BD%BD%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">3.</span> <span class="toc-text">数据的加载和预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Dataset"><span class="toc-number">3.1.</span> <span class="toc-text">Dataset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dataloader"><span class="toc-number">3.2.</span> <span class="toc-text">Dataloader</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torchvision-%E5%8C%85"><span class="toc-number">3.3.</span> <span class="toc-text">torchvision 包</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#torchvision-datasets"><span class="toc-number">3.3.1.</span> <span class="toc-text">torchvision.datasets</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torchvision-models"><span class="toc-number">3.3.2.</span> <span class="toc-text">torchvision.models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torchvision-transforms"><span class="toc-number">3.3.3.</span> <span class="toc-text">torchvision.transforms</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8C%85nn%E5%92%8C%E4%BC%98%E5%8C%96%E5%99%A8optimizer"><span class="toc-number">4.</span> <span class="toc-text">神经网络包nn和优化器optimizer</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E4%B8%80%E4%B8%AA%E7%BD%91%E7%BB%9C"><span class="toc-number">4.1.</span> <span class="toc-text">定义一个网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">4.2.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">4.3.</span> <span class="toc-text">优化器</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80"><span class="toc-number">5.</span> <span class="toc-text">深度学习基础</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">5.1.</span> <span class="toc-text">监督学习和无监督学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%EF%BC%88Linear-Regreesion%EF%BC%89"><span class="toc-number">5.2.</span> <span class="toc-text">线性回归 （Linear Regreesion）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-Loss-Function"><span class="toc-number">5.3.</span> <span class="toc-text">损失函数(Loss Function)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#nn-L1Loss"><span class="toc-number">5.3.1.</span> <span class="toc-text">nn.L1Loss:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#nn-NLLLoss"><span class="toc-number">5.3.2.</span> <span class="toc-text">nn.NLLLoss:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#n-MSELoss"><span class="toc-number">5.3.3.</span> <span class="toc-text">n.MSELoss:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#nn-CrossEntropyLoss"><span class="toc-number">5.3.4.</span> <span class="toc-text">nn.CrossEntropyLoss:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#nn-BCELoss"><span class="toc-number">5.3.5.</span> <span class="toc-text">nn.BCELoss:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">5.4.</span> <span class="toc-text">梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Mini-batch%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">5.4.1.</span> <span class="toc-text">Mini-batch的梯度下降法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-optim-SGD"><span class="toc-number">5.4.2.</span> <span class="toc-text">torch.optim.SGD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-optim-RMSprop"><span class="toc-number">5.4.3.</span> <span class="toc-text">torch.optim.RMSprop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-optim-Adam"><span class="toc-number">5.4.4.</span> <span class="toc-text">torch.optim.Adam</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B9%E5%B7%AE-%E5%81%8F%E5%B7%AE"><span class="toc-number">5.5.</span> <span class="toc-text">方差&#x2F;偏差</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">5.6.</span> <span class="toc-text">正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#L1%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">5.6.1.</span> <span class="toc-text">L1正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#L2%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">5.6.2.</span> <span class="toc-text">L2正则化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8BLeNet-5"><span class="toc-number">6.</span> <span class="toc-text">卷积神经网络经典模型LeNet-5</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E9%93%BE%E6%8E%A5"><span class="toc-number">7.</span> <span class="toc-text">相关链接</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/11/10/pytorch.html" title="Pytorch基本教程"><img src="/image/pytorch.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Pytorch基本教程"/></a><div class="content"><a class="title" href="/2022/11/10/pytorch.html" title="Pytorch基本教程">Pytorch基本教程</a><time datetime="2022-11-10T02:00:00.000Z" title="Created 2022-11-10 10:00:00">2022-11-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/07/network.html" title="神经网络简介"><img src="/image/all.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="神经网络简介"/></a><div class="content"><a class="title" href="/2022/11/07/network.html" title="神经网络简介">神经网络简介</a><time datetime="2022-11-07T06:33:55.000Z" title="Created 2022-11-07 14:33:55">2022-11-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/07/script.html" title="脚本积累"><img src="/image/code.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="脚本积累"/></a><div class="content"><a class="title" href="/2022/11/07/script.html" title="脚本积累">脚本积累</a><time datetime="2022-11-07T01:00:00.000Z" title="Created 2022-11-07 09:00:00">2022-11-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/06/nerf.html" title="NeRF数据采集与处理"><img src="/image/lego.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="NeRF数据采集与处理"/></a><div class="content"><a class="title" href="/2022/11/06/nerf.html" title="NeRF数据采集与处理">NeRF数据采集与处理</a><time datetime="2022-11-06T08:50:20.000Z" title="Created 2022-11-06 16:50:20">2022-11-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/03/becoming-jane.html" title="《Becoming Jane》(成为简·奥斯汀)"><img src="/image/movie/becoming_jane_1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="《Becoming Jane》(成为简·奥斯汀)"/></a><div class="content"><a class="title" href="/2022/11/03/becoming-jane.html" title="《Becoming Jane》(成为简·奥斯汀)">《Becoming Jane》(成为简·奥斯汀)</a><time datetime="2022-11-02T18:04:00.000Z" title="Created 2022-11-03 02:04:00">2022-11-03</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/image/pytorch.png')"><div id="footer-wrap"><div class="copyright">&copy;2022 By Paradise</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>