<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>NeuS</title>
      <link href="/2022/12/05/paper/neus.html"/>
      <url>/2022/12/05/paper/neus.html</url>
      
        <content type="html"><![CDATA[<p>NeuS项目主页：<a href="https://lingjie0206.github.io/papers/NeuS/index.htm">NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction</a></p><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><ul><li>本文目的：从多视角2D图片，重建物体的3D表面。</li><li>传统的神经表面重建工作（DVR，IDR）要求前景 mask 做监督，并且容易陷入局部最小值，因此难以重建具有自遮挡或薄结构的物体；</li><li>对 NeRF 及其变体通过深度学习生成的高质量隐式表面没有足够的表面限制，难以提取高质量表面。</li></ul><h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul><li>提出了一种新颖的神经表面重建方法——NeuS（Neural surface Reconstruction）：将表面表示为符号距离函数（SDF： signed distance function）的零水平集（zero-level），通过新式的体渲染（volume rendering）方法来训练 SDF 表示；</li><li>相对于传统体渲染方法，提出一种在一阶估计中无偏的公式，达到更精确的表面重建甚至可以不用 mask 监督。</li></ul><h1 id="Input-amp-Output"><a href="#Input-amp-Output" class="headerlink" title="Input &amp; Output"></a>Input &amp; Output</h1><ul><li>Input： 多视角RGB图片、对应图片的相机位姿，Mask（可选）</li><li>Output： SDF</li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Scene-representation"><a href="#Scene-representation" class="headerlink" title="Scene representation"></a>Scene representation</h2><p>NeuS 是隐式表面重建，通过将待重建表面表达为 SDF 函数的零水平集，利用 MLP 和体渲染方法学习出这种神经 SDF 表示，具体场景表示建模如下： </p><ul><li><p>SDF： $f:\mathbf{x}\in\mathbb{R}^3\to signed\ distance\in\mathbb{R}$，$\mathbf{x}$ 为空间位置；</p><ul><li>默认待重建表面是水密的（密闭的）；</li><li>SDF 表示空间任一点到待重建表面的有向距离，当该点在物体外部时距离为正，在物体内部时距离为负，</li></ul></li><li><p>Logistic density distribution：$\phi_s(x)=s e^{-s x}/(1+e^{-s x})^2$；</p><ul><li>即 Sigmoid 函数 $\Phi_s(x)=1/(1+e^{-sx})$ 的导数，$\phi^\prime_s(x)=\Phi^\prime_s(x)$；</li><li>事实上 $\phi_s(x)$ 可以是任意以 $0$ 为中心的单峰密度分布，这里出于计算方便；</li><li>标准差为 $1/s$，也是一个可训练的参数，希望最终收敛 $1/s\sim 0$；</li></ul></li><li><p>待重建的表面被表示为 zero-level 集：$\mathcal{S}={\mathbf{x}\in\mathbb{R}^3|f(\mathbf{x})=0}$；</p></li><li>密度函数 S-density：$\sigma(\mathbf{x})=\phi_s(f(\mathbf{x}))$；</li><li>辐射场函数 $c:(\mathbf{x,v})\in\mathbb{R}^3\times\mathbb{S}^2\to\mathbb{R}^3$，$\mathbf{v}$ 为观察方向；<ul><li>对空间任一点 $\mathbf{x}$，在给定观察方向 $\mathbf{v}$ 下的颜色值；</li></ul></li></ul><p>期望的是训练结果，待重建的表面SDF接近于 $0$，其他地方为有向距离；并且训练收敛后 $1/s\sim 0$，S-density $\phi_s(f(\mathbf{x}))$ 在待重建的表面附近有显著的高值，其他位置趋近于 $0$（如下图）。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->    <td><center><img src="/image/neus/sigmod.png"                        width="216" height="135.6"/></center></td>   <!--<center>标签将图片居中-->    </tr></table><h2 id="Volume-Rendering"><a href="#Volume-Rendering" class="headerlink" title="Volume Rendering"></a>Volume Rendering</h2><p>为了学习（训练）神经SDF表示和辐射场的参数，作者建议采用体渲染的方法来渲染SDF表示中的图片，所以，对任一图片内给定像素，其基于相机内部光心原点 $\mathbf{o}$ 在观察方向 $\mathbf{v}$ 发出的一条光线 ${\mathbf{x}(t)=\mathbf{o}+t \mathbf{v}|t\geq 0}$，沿光线累积颜色：</p><script type="math/tex; mode=display">C(\mathbf{o,v})=\int_0^{+\infty}w(t)c(\mathbf{x}(t),\mathbf{v})d t</script><p>$w(t)$ 为点 $\mathbf{x}(t)$ 的权重，$C(\mathbf{o,v})$ 即为这个像素的输出颜色。</p><h2 id="Requirements-on-weight-function"><a href="#Requirements-on-weight-function" class="headerlink" title="Requirements on weight function"></a>Requirements on weight function</h2><p>可以说，从 2D 图片学习精确的 SDF 表示的关键就在于构建出一种输出的颜色值与 SDF 的合适的联系，也即，基于场景的 SDF $f$ 推导出一个合适的权重函数。对权重函数的要求如下：</p><ul><li><p><strong>无偏性（Unbiased）。</strong>在光线与表面交点处 $\mathbf{x}(t^\ast)$，也即：$f(\mathbf{x}(t^\ast))=0$ 时，$w(t)$ 达到局部最大值；</p><ul><li>保证光线与表面（SDF零水平集）交点对像素颜色贡献最大；</li></ul></li><li><p><strong>感知遮挡（Occlusion-aware）。</strong></p><ul><li><script type="math/tex; mode=display">f(t_0)=f(t_1),\quad t_0<t_1,\quad w(t_0)>0,w(t_1)>0 \Rightarrow w(t_0)>w(t_1)</script><p>即更靠近相机的点有更大的权重，也即对最终的输出颜色贡献更大；</p></li><li><p>确保当光线依次通过多个表面时，渲染过程将正确地使用离相机最近的表面的颜色来计算输出颜色。</p></li></ul></li></ul><p>在传统的体渲染中：$w(t)=T(t)\sigma(t),\quad T(t)=exp(-\int_0^t \sigma(u)d u)$ 为沿光线累积透射率（accumulated transmittance），$\sigma(t)=\phi_s(f(\mathbf{x}(t)))$ 为体密度（volume density）。——遮挡感知的，但是<strong>有偏的</strong>，会在重建表面引入固有错误（如下图，在表面交点处，权重函数并没有局部最大）。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->    <td><center><img src="/image/neus/fig2.png"                        width="361.8" height="266.1"/></center></td>   <!--<center>标签将图片居中-->    </tr></table><p><strong>NeuS的方案。</strong></p><script type="math/tex; mode=display">\begin{aligned}w(t)&=T(t)\rho(t),&\cdots(1)\\T(t)&=exp\left(-\int_0^t \rho(u)d u\right),&\cdots(2)\\\rho(t)&=\max\left(\frac{-\frac{d\Phi_s}{dt}(f(\mathbf{x}(t)))}{\Phi_s(f(\mathbf{x}(t)))},0\right).&\cdots(3)\end{aligned}</script><p>其中式子 (1) 为传统体渲染格式，保证了遮挡感知；式子 (3) 为不透明密度（opaque density）函数。这在 SDF 的一阶近似中是无偏的（具体推导参见论文即可，无偏的示例如上面图2b所示，多表面交点示例如下所示）。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->    <td><center><img src="/image/neus/fig3-neus.png"                        width="241.8" height="300.8"/></center></td>   <!--<center>标签将图片居中-->    </tr></table><h2 id="Discretization"><a href="#Discretization" class="headerlink" title="Discretization"></a>Discretization</h2><p>为了对不透明密度和权重函数的离散化形式，采用类似 NeRF 中的同样的估计方法。沿光线采样 $n$ 个点，从而沿此光线的像素处颜色为 ：</p><script type="math/tex; mode=display">\begin{aligned}\hat{C}&=\sum_{i=1}^n T_i\alpha_i c_i \\T_i&=\prod_{j=1}^{i-1}(1-\alpha_j) \\\alpha_i&=max\{\frac{\Phi_s(f(\mathbf{x}(t_i)))-\Phi_s(f(\mathbf{x}(t_{i+1})))}{\Phi_s(f(\mathbf{x}(t_i)))},0\}.\end{aligned}</script><p>有关 $\alpha_i$ 的详细推导参见原文的附录。</p><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>通过在每次迭代中，从一张图片内随机的选取一部分（batch）像素以及世界空间 $P={C_k,M_k,\mathbf{o}_k,\mathbf{v}_k}$ 内相应的光线来优化神经网络和上述标准差的倒数（$1/s$），其中 $C_k$ 为像素的真实颜色，$M_k\in{0,1}$ 为可选的 Mask 的值，即是否有 Mask。$n$ 为采样点数，$m$ 为 batch size。</p><ul><li><p>损失函数：</p><script type="math/tex; mode=display">\mathcal{L}=\mathcal{L}_{color}+\lambda\mathcal{L}_{reg}+\beta\mathcal{L}_{mask}</script><ul><li><p>光度（颜色）损失：</p><script type="math/tex; mode=display">\mathcal{L}_{color}=\frac{1}{m}\sum_k\left|\hat{C}_k-C_k\right|</script><ul><li>监督渲染颜色接近真实值颜色</li></ul></li><li><p>Eikonal loss： </p><script type="math/tex; mode=display">\mathcal{L}_{reg}=\frac{1}{nm}\sum_{i,k}\left(\parallel\nabla f(\hat{\mathbf{x}}_{k,i})\parallel_2-1\right)^2</script><ul><li><p>约束 MLP 学习出的 SDF 的导数满足导数模长等于 1 的性质；</p></li><li><p>简单证明：</p><p>假设上述 $f$ 是定义在 $\Omega\in\mathbb{R}^3$ 上的 SDF，</p><p>对 $\forall \mathbf{x}\notin\partial\Omega$，存在一个最近点 $\mathbf{y}\in\partial\Omega$，</p><p>因此 $\mathbf{v}=(\mathbf{y-x})/\parallel\mathbf{y-x}\parallel$ 是最速下降方向，也即与 $\nabla f(\mathbf{x})$ 同向，</p><p>又因 $f$ 定义，即恰好表示其到最近点的距离，</p><p>故沿 $\mathbf{v}$ 方向移动一单位，$f$ 也变化一单位，</p><p>即方向导数满足 $D_v f=\nabla f(\mathbf{x})\cdot\mathbf{v}=\parallel\nabla f(\mathbf{x})\parallel\parallel\mathbf{v}\parallel\cos\theta=1$，</p><p>故 $\parallel\nabla f\parallel=1$。</p></li></ul></li><li><p>可选的掩码损失： </p><script type="math/tex; mode=display">\mathcal{L}_{mask}=BCE(M_k,\hat{W}_k),\quad\hat{W}_k=\sum_{i=1}^nT_{k,i}\alpha_{k,i}</script><ul><li>$\hat{W}_k$ 沿相机光线的权重总和，$BCE$ 是二元交叉熵损失。</li></ul></li></ul></li><li><p>分层采样（Hierarchical sampling）：</p><ul><li>首先沿射线均匀采样 64 个点，然后迭代进行 $k=4$ 次的重要性采样。</li><li>只维护一个网络（不像NeRF中同时维护粗糙和精细两个网络）<ul><li>粗采样的概率基于固有标准差的 S-density $\phi_s(f(\mathbf{x}))$ 计算；</li><li>精细采样的概率则是基于学习到的 $s$ 的 $\phi_s(f(\mathbf{x}))$ 计算；</li><li>具体细节参见原论文附录以及渲染代码。</li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeRF</title>
      <link href="/2022/12/04/paper/nerf.html"/>
      <url>/2022/12/04/paper/nerf.html</url>
      
        <content type="html"><![CDATA[<p>NeRF项目主页：<a href="https://www.matthewtancik.com/nerf">NeRF: Neural Radiance Fields</a></p><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><ul><li>本文目的：新视角合成（new view synthesis），对输入的 2D RGB 多视角稀疏图片，合成其他视角下的高逼真图片。</li></ul><h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul><li>提出了一种可以应对复杂几何和材质的连续场景的表示方法 NeRF：参数化为 MLP 的神经辐射场；</li><li>基于经典体渲染技术的可微渲染过程，提出分层采样策略，充分利用 MLP 的表达能力以对网络进行优化；</li><li>使用位置编码（Position Encoding），将输入的 5D 坐标映射到更高维的空间，让 NeRF 可以表示高频场景内容。</li></ul><h1 id="Input-amp-Output"><a href="#Input-amp-Output" class="headerlink" title="Input &amp; Output"></a>Input &amp; Output</h1><ul><li><strong>Input：</strong> 多视角RGB图片、对应图片的相机位姿;</li><li><strong>Output：</strong> NeRF 场景表示</li></ul><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文以一种新颖的方式解决了长期存在的视图合成问题：将静态场景表示（拟合）为连续的 5D 函数，函数输出各个空间点 $\mathbf{x}=(x,y,z)$ 在各个方向 $\mathbf{v}=(\theta,\phi)$ 的辐射亮度（颜色）和密度；通过优化一个（无卷积层）深度全连接神经网络（MLP，多层感知器，multilayer perceptron）的参数，根据一个 5D 输入 $(x,y,z,\theta,\phi)$，最小化渲染图像与真实图像的误差损失，回归学习输出空间该点处的体密度和此方向下的 RGB 颜色。流程图 （Pipeline）如下：</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>                <img src="/image/nerf/pipline.png"                        width="729.45" height="412.65"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><h1 id="Neural-Radiance-Field-Scene-Representation"><a href="#Neural-Radiance-Field-Scene-Representation" class="headerlink" title="Neural Radiance Field Scene Representation"></a>Neural Radiance Field Scene Representation</h1><ul><li>NeRF 通过将一个连续的静态场景表示为一个 5D 的向量值函数：</li></ul><script type="math/tex; mode=display">F_\Theta:(\mathbf{x},\mathbf{v})\in\mathbb{R}^5\to(\mathbf{c},\sigma)</script><p>函数输入为：3D 空间位置 $\mathbf{x}=(x,y,z)\in\mathbb{R}^3$ 以及 2D 视角观察方向 $\mathbf{v}=(\theta,\phi)$（一般表示为单位笛卡尔向量，3D向量，但是模长为1）；函数输出为：辐射颜色 $\mathbf{c}=(r,g,b)$ 和空间体密度 $\sigma$。</p><ul><li><p>为了将输出的依赖关系约束为：预测的体密度 $\sigma$ 仅依赖于空间位置 $\mathbf{x}$，RGB 颜色 $\mathbf{c}$ 同时依赖于空间位置 $\mathbf{x}$ 和视角方向 $\mathbf{v}$，（这是合理的，因为空间中任一点的密度不会随观察角度的变化而发生改变，但是颜色则会依赖从那个观察方向观看），以此通过这种依赖关系约束网格学习到场景的多视角连续表示，网格的具体进程如下：</p><ul><li>首先使用 8 层的全连接层（ReLU 作为激活函数，每层有 256 个通道），输入 3D 空间坐标 $\mathbf{x}$，输出 体密度 $\sigma$ 和一个 256 维的特征向量 $\xi$；</li><li>其次将上述特征向量 $\xi$ 和视角方向 $\mathbf{v}$ 连接起来作为输入，输入到另一个全连接层（ReLU 作为激活函数，每层有 256 个通道），输出和方向相关的 RGB 颜色 $\mathbf{c}$。</li></ul><p>（后面的工作已证明这是两个独立的网络）</p></li></ul><h1 id="Volume-Rendering-with-Radiance-Fields"><a href="#Volume-Rendering-with-Radiance-Fields" class="headerlink" title="Volume Rendering with Radiance Fields"></a>Volume Rendering with Radiance Fields</h1><p>使用经典体渲染技术可以渲染出任意射线穿过场景的颜色。体积密度 $\sigma(\mathbf{x})$ 可以解释为：光线在位置 $\mathbf{x}$ 处终止于无穷小粒子的可微概率。在最近 $t_n$ 与最远 $t_f$ 边界的条件下，相机光线 $\mathbf{r}(t)=\mathbf{o}+t\mathbf{d}$ 的颜色 $C(\mathbf{r})$ 为：</p><script type="math/tex; mode=display">C(\mathbf{r}) = \int_{t_n}^{t_f} T(t)\sigma(\mathbf{r}(t))c(\mathbf{r}(t),\mathbf{d}) dt, \qquad where\quad T(t)=e^{-\int_{t_n}^{t}\sigma(\mathbf{r}(s))} ds</script><p>其中函数 $T(t)$ 表示沿着光线从 $t_n$ 到 $t$ 所累积的透明度（accumulated transmittance），即光线从 $t_n$ 到 $t$ 未击中任何粒子的概率。视图渲染即需估计积分 $C(\mathbf{r})$，即光线通过每个像素所累积的颜色。</p><p>通过数值离散积分估计上述累积颜色：将 $[t_n,t_f]$ 区间 $N$ 等分，对每个区间段随机均匀采样，故第 $i$ 个采样点为</p><script type="math/tex; mode=display">t_i \sim \mathcal{U} [t_n+\frac{i-1}{N}(t_f-t_n),t_n+\frac{i}{N}(t_f-t_n)]</script><p>此时积分的离散形式为：</p><script type="math/tex; mode=display">C(\mathbf{r}) = \sum_{i=1}^{N} T_i \cdot (1-e^{-\sigma_i \delta_i}) \mathbf{c}_i,\quad where\ T_i=exp(-\sum_{j=1}^{i-1}\sigma_j\delta_j), \quad \delta_i=t_{i+1}-t_i</script><h1 id="Optimizing-a-Neural-Radiance-Field"><a href="#Optimizing-a-Neural-Radiance-Field" class="headerlink" title="Optimizing a Neural Radiance Field"></a>Optimizing a Neural Radiance Field</h1><p>如果只是采用上述这种策略来拟合场景，对于一些复杂的场景效果并不理想，一方面很难得到较高分辨率的结果，无法恢复场景中的高频细节，另一方面不能高效利用每条光线的采样点。所以论文提出了两个进一步的优化技巧：位置编码（Position Encoding）、分层采样（Hierarchical Sampling）。</p><h2 id="Position-Encoding"><a href="#Position-Encoding" class="headerlink" title="Position Encoding"></a>Position Encoding</h2><p>Rahaman 等人证明了深度网络倾向于学习到低频函数；使用高频函数把输入映射到更高维的空间中，再传递到神经网络，可以更好地拟合具有高频变化的数据。</p><p>因此为了拟合场景中的高频细节，应用到 NeRF 中，重建 $F<em>\Theta$ 为两个函数的复合形式：$F</em>\Theta=F<em>\Theta^\prime\circ\gamma$。其中 $F</em>\Theta^\prime$ 是普通的MLP，编码函数 $\gamma:\mathbb{R}\to\mathbb{R}^{2L}$ 如下：</p><script type="math/tex; mode=display">\gamma(p)=(\sin(2^0\pi p),\cos(2^0\pi p),\cdots,\sin(2^{L-1}\pi p),\cos(2^{L-1}\pi p))</script><p>该函数分别作用于空间位置 $\mathbf{x}$ 以及视角方向 $\mathbf{v}$ 的每个分量。（该论文中空间位置和视角方向分别为 $L=10,4$）</p><h2 id="Hierarchical-volume-sampling"><a href="#Hierarchical-volume-sampling" class="headerlink" title="Hierarchical volume sampling"></a>Hierarchical volume sampling</h2><p>前面提出的传统沿光线离散采样 $N$ 个点对于评估 NeRF 网络是不充分的：空白空间和遮挡区域对于渲染图片是没有贡献的，但是仍然被重复的采样评估。通过会早期体渲染工作的借鉴，论文提出了分层采样的策略来提升渲染效率：根据所期望的渲染效果，来按比例地分配采样点。所以并不是使用单个网络来表示场景，而是考虑同时优化两个网络：粗糙网络和精细网络。</p><ul><li><p>首先使用分层抽样，采样第一个集合，包含 $N_c$ 个位置点，用上述 $t_i$ 与 $C(\mathbf{r})$ 方程计算粗糙网络，并根据粗糙网络的输出，沿着光线生成更明智的采样点（更偏向与体积相关的部分）。在上述粗网格下，重写颜色公式：</p><script type="math/tex; mode=display">C_c(\mathbf{r}) = \sum_{i=1}^{N_c} \omega_i \mathbf{c}_i,\qquad \omega_i=T_i(1-e^{-\sigma_i \delta_i})</script><p>再归一化权重</p><script type="math/tex; mode=display">\hat{\omega_i}=\frac{\omega_i}{\sum_{i=1}^{N_c} \omega_i},</script><p>这样就可以把上式颜色公式看作是沿着光线的分段连续概率密度函数(PDF)，可以粗略的估计此光线上物体的分布情况。</p></li><li><p>进一步，使用逆变换采样（inverse transform sampling），从上述分布采样出第二个、包含 $N_f$ 个位置点，并在第一个采样点集合和第二个采样点集合的并集上计算精细网络，最后利用上述 $N_c+N_f$ 个采样点计算最终的光线颜色 $C_f(\mathbf{r})$。</p></li></ul><p>注：这种方法，可以分配更多的样本点在包含场景内容的区域内。它解决了与重要性抽样（importance sampling）相同的目标，但论文使用采样值作为整个积分域的非均匀离散化，而不是将每个样本视为整个积分的独立概率估计。</p><h2 id="Implementation-details"><a href="#Implementation-details" class="headerlink" title="Implementation details"></a>Implementation details</h2><ul><li><p>训练损失函数：</p><script type="math/tex; mode=display">\mathcal{L}=\sum_{\mathbf{r}\in\mathcal{R}} [\parallel C_c(\mathbf{r}) -C(\mathbf{r})\parallel^2_2+ \parallel C_f(\mathbf{r}) -C(\mathbf{r})\parallel^2_2]</script><p>注意到上述同时也最小化了 $C_c(\mathbf{r})$ 的损失，以便粗网络的权重分布可用于在细网络中分配样本。</p></li><li><p>论文中 $N_c=64$，$N_f=128$；</p></li><li><p>使用 Adam optimizer（优化器），学习率（learning rate）从 $5\times 10^{-4}$ 指数下降到 $5\times 10^{-5}$，其他优化器参数 $\beta_1=0.9,\beta_2=0.999,\epsilon=10^{-7}$。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeRF </tag>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>体渲染相关知识</title>
      <link href="/2022/11/17/volume.html"/>
      <url>/2022/11/17/volume.html</url>
      
        <content type="html"><![CDATA[<h1 id="体渲染相对于表面渲染优势"><a href="#体渲染相对于表面渲染优势" class="headerlink" title="体渲染相对于表面渲染优势"></a>体渲染相对于表面渲染优势</h1><p>表面渲染和体渲染技术上最核心的区别在于：<strong>光线是否会在物体内部发生作用</strong>。体渲染会考虑光线在物体内部的传播，表面渲染的光线仅仅在物体表面发生作用。这导致的核心应用区别是：表面渲染只适合于渲染漫反射材质、金属材质、塑料材质、镜面材质等。但对于玉石、烟雾等半透明材质，只有可微体渲染才能支持。同时，体渲染的渲染代价要高于表面渲染。</p><h1 id="体素"><a href="#体素" class="headerlink" title="体素"></a>体素</h1><p>每一个体素都代表着空间中的一个单位，尽管可以把体素想象成一个很小的立方体，但更应该把每个体素认为是从连续的三维信号中对一个无穷小的点采样得到的样本。我们关注的体渲染一般可以理解为：刻画空间中每个体素的密度与颜色值。</p><h1 id="体渲染积分"><a href="#体渲染积分" class="headerlink" title="体渲染积分"></a>体渲染积分</h1><p>最基础的、也是我们最常用的体渲染算法是 <code>Ray-Casting</code> 算法，这是一种逐像素的直接体渲染算法：<strong>对要渲染的图片中的每一个像素，从眼睛发出一束光线，传过像素中心，进入体内部，然后对沿着射线遇到的体密度进行光学属性的积分（数值）。</strong>注意到，这种通用的描述假定了体和映射到体上的光学属性是连续的。在实际中，体素是离散的，并且积分的计算结果是数值近似的。</p><ul><li>对于一条光线 $\mathbf{x}(t)=\mathbf{o}+t\mathbf{v}\in\mathbb{R}^3$，$\mathbf{o}$ 为光线的原点，$\mathbf{v}$ 为光线的方向，利用距离 $t$ 进行参数化；</li><li>对沿光线的某个位置对应的标量，记为 $s(\mathbf{x}(t))\in\mathbb{R}$，常用的比如体密度、深度；</li><li>光线吸收系数 $k(t)=k(s(\mathbf{x}(t)))$；</li><li>辐射颜色 $c(t)=c(s(\mathbf{x}(t)))$；</li></ul><p>常用的光照模型认为粒子会发光，也会遮挡（吸收）到来的光，但是没有散射或间接光。所以此时体渲染方程即对吸收系数 $k(t)$、辐射颜色 $c(t)$ 沿着光线进行积分，即得某位置上单个粒子到达渲染图片的颜色为：</p><script type="math/tex; mode=display">c^\prime(t)=c(t)\cdot e^{-\int_0^tk(y)dy}.</script><p>当考虑到这条射线上所有可能位置 $t$ 发出的辐射总和，即有最终渲染：</p><script type="math/tex; mode=display">C^\prime=\int_0^\infty c(t)\cdot e^{-\int_0^tk(y)dy}dt.</script><p>离散化上述积分：</p><script type="math/tex; mode=display">C^\prime=\sum_i c(t_i) e^{-\sum_j k(j\cdot\Delta t)\Delta t}.</script><p>实际上，体渲染积分是通过以从前到后或从后到前的 <code>Alpha混合（合成）</code> 的方式来近似求解的。（下述记号是以均匀采样标写的，所以 $\delta<em>i=t</em>{i+1}-t_i=\Delta t$）</p><ul><li>累积透射度 $T(t)=e^{-\int_0^t k(y)dy}$，离散情形 $T_i=e^{-\sum_j k(j\cdot\Delta t)\Delta t}$；</li><li>累积不透明度 $A(t)=1-T(t)$，离散情形 $A_i=1-T_i$；</li><li>辐射颜色离散 $c_i=c(t_i)\Delta t$；</li></ul><p>此时最终辐射总和计算为：</p><script type="math/tex; mode=display">C^\prime=\sum_i c_i T_i.</script><p>上述这个公式能通过以从后到前或从前到后的顺序的 <code>Alpha混合（合成）</code> 来迭代计算。</p><h1 id="Alpha合成"><a href="#Alpha合成" class="headerlink" title="Alpha合成"></a>Alpha合成</h1><p>上述那个公式能通过以从后到前使用 $n-1$ 到 $0$ 的 $i$ 来迭代计算：</p><script type="math/tex; mode=display">C^\prime_i=c_i+(1-A_i)\cdot C^\prime_{i+1}.</script><p>新值 $C<em>i^\prime$ 可以由当前位置 $i$ 的颜色 $C_i$ 和不透明度 $A_i$ 以及前一个位置 $i+1$ 的复合颜色 $C</em>{i+1}^\prime$ 来算出。起始条件是 $C_n^\prime=0$。</p><ul><li>注意到，在所有混合方程中，我们使用带有不透明度权重的颜色，也就是所说的 <code>associated colors</code>。带有不透明度权重的颜色是已经提前乘好他们相关的不透明度的颜色。这是一种十分便利的符号，在用于插值时尤其重要。可以证明，对颜色和不透明度分别进行插值后会造成失真，但对不透明度加权的颜色进行插值就会得到正确的结果。</li></ul><p>下面是可选的从前到后顺序的迭代公式，$i$ 从 $1$ 递增至 $n$：</p><script type="math/tex; mode=display">C^\prime_i=c_{i-1}+(1-A_{i-1})\cdot C^\prime_{i} \\A^\prime_i=A_{i-1}^\prime+(1-A_{i-1}^\prime)\cdot A^\prime_{i}.</script><p>新值 $C<em>i^\prime,A_i^\prime$ 可以由当前位置 $i$ 的颜色 $C_i$ 和不透明度 $A_i$ 以及前一个位置 $i-1$ 的复合颜色 $C</em>{i-1}^\prime$ 和不透明度 $A_{i-1}^\prime$ 来算出。起始条件是 $C_0^\prime=0,A^\prime_0=0$。</p><ul><li>注意到从前到后复合需要追踪 <code>alpha</code> 的值，然而从后到前组合不需要。在硬件实现中，使用从前到后的复合的话，意味着终点 <code>alpha</code> 必须被帧缓冲所支持（比如一个 <code>alpha</code> 的值必须存在帧缓冲中，然后它必须能在混合操作中用作乘数）。然而，由于从前到后复合的主要优点是一种叫做提前射线终止的常见优化方法，这种方法会在沿射线累计的 <code>alpha</code> 到达 $0$ 时，将过程立刻终止。这在硬件上是难以执行的，使用 GPU 的体渲染通常使用从后到前的复合。</li></ul><h1 id="NeRF中的体渲染"><a href="#NeRF中的体渲染" class="headerlink" title="NeRF中的体渲染"></a>NeRF中的体渲染</h1><p>（综合 NeRF 原文和其他论文中的理解）</p><p>我们考虑恢复体素的颜色与密度：</p><ul><li>体密度 $\sigma(\mathbf{x}(t))$；</li><li>累积透射度 $T(t)=e^{-\int_0^t\sigma(\mathbf{x}(y))dy}$；</li><li>累积不透明度 $A(t)=1-T(t)$，将 $A(t)$ 看作是一个累积分布函数；</li><li>概率密度函数（PDF）$\tau(t)=\frac{d T(t)}{d t}=\sigma(\mathbf{x}(t))T(t)$；</li></ul><p>从而最终合成颜色为：</p><script type="math/tex; mode=display">C(\mathbf{o,v})=\int_0^{+\infty} c(\mathbf{x}(t),\mathbf{v})\cdot\sigma(\mathbf{x}(t))T(t) dt.</script>]]></content>
      
      
      <categories>
          
          <category> Basic Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeRF </tag>
            
            <tag> Basic Knowledge </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeRF数据采集与处理</title>
      <link href="/2022/11/16/nerf.html"/>
      <url>/2022/11/16/nerf.html</url>
      
        <content type="html"><![CDATA[<h1 id="拍摄要求"><a href="#拍摄要求" class="headerlink" title="拍摄要求"></a>拍摄要求</h1><ul><li>拍摄尽可能同一时间段拍完，避免巨变的光照条件变化、曝光失常、焦点失焦等问题；</li><li>可以选择拍摄图片，但需保证不同图片的相机焦距一定，避免手机滤镜、美颜等磨去纹理等；<strong>建议</strong>拍摄视频，可以保证相邻图片直接连续性；</li><li>小物件，可以物品为球心，在半球式多视角拍摄图片；对于较大物品，无法做到半球式拍摄顶部，可以柱面或部分球面的形式拍摄，但重建效果对于未采集区域必然是不好的；</li></ul><h1 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h1><p>（此处我提供了一些 <code>python</code> 脚本处理，例如抽帧、降分辨率等，具体请见：脚本积累）</p><ul><li>拍摄为视频时：保持镜头的连续性，以一定间隔进行抽帧（保证相邻图片特征匹配良好），并剔除失焦模糊、曝光失常等无效图片；</li><li>必要时可以将图片同比例的缩小；</li><li>（可选）如果只想单独重建物体，可以进行图像分割，将想要重建的物品抠图出来（PS、<a href="https://github.com/hkchengrex/MiVOS/tree/MiVOS-STCN">MiVOS-STCN</a>等方法）；</li></ul><h1 id="相机内外参"><a href="#相机内外参" class="headerlink" title="相机内外参"></a>相机内外参</h1><p>我们利用 <a href="(http://colmap.github.io/">colmap</a>) 图形界面获取相机内外参：</p><ul><li><p>最好这里保证如下文件结构：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+ --- scene               <span class="comment"># 待合成场景</span></span><br><span class="line">|     + --- image        <span class="comment"># 图片文件夹</span></span><br><span class="line">|     + --- mask         <span class="comment"># 如果进行了语义分割，mask文件夹</span></span><br><span class="line">|     + --- database.db  <span class="comment"># 从此开始,下述文件为位姿获取时生成</span></span><br><span class="line">|     + --- sparse       <span class="comment"># 相机位姿的二进制文件</span></span><br><span class="line">|     + --- text         <span class="comment"># 相机外参的.txt格式文件 </span></span><br><span class="line">|     + --- project.ini</span><br></pre></td></tr></table></figure></li><li><p>菜单栏 <code>File-&gt;New project</code>，第一栏创建 <code>database.db</code> 二进制进程文件，第二栏添加 <code>image</code> 图片文件夹路径；</p></li><li><p>菜单栏 <code>Processing-&gt;Feature extraction</code>，将 <code>camera model</code> 调整为 <code>PINHOLE</code>，选中下面的 <code>Shared for all images</code>（这也就是为什么我们要求不要改变内参，避免不必要的麻烦）。（如果有 <code>mask</code>，可以导入对应文件夹路径；如果有多个 GPU，可以在下面选择 <code>gpu_index</code> 参数）最后点击 <code>Extract</code> 提取特征；</p></li><li><p>菜单栏 <code>Processing-&gt;Feature matching-&gt;Exhaustive-&gt;Run</code>；</p></li><li><p>菜单栏 <code>Reconstruction-&gt;Automatic reconstruction</code>，<code>Workspace</code> 导入 <code>scene</code> 文件夹，<code>Image folder</code> 导入<code>image</code> 文件夹，选中 <code>Shared intrinsics</code>，<strong>取消</strong> <code>Dense model</code>，然后 <code>Run</code>；</p></li><li><p>直观的，你可以导出看看相机内外参，<code>File-&gt;Export model as text</code>；</p></li><li><p>关闭前会让你保存一个 <code>project.ini</code> 文件。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> NeRF </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>脚本积累</title>
      <link href="/2022/11/15/script.html"/>
      <url>/2022/11/15/script.html</url>
      
        <content type="html"><![CDATA[<p><strong>此博客用来存放、并简单介绍相关数据处理等脚本。</strong></p><h2 id="视频抽帧"><a href="#视频抽帧" class="headerlink" title="视频抽帧"></a>视频抽帧</h2><p><a href="/code/video2images.py">video2images.py</a>：以一定间隔从视频内提取帧；</p><ul><li><p>在第6行修改视频格式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> input_path.endswith(<span class="string">&#x27;.MP4&#x27;</span>):</span><br></pre></td></tr></table></figure></li><li><p>在第19行修改图片大小（尽量保持原比例缩放）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">frame_n = cv2.resize(frame, (<span class="number">1920</span>, <span class="number">1080</span>)) <span class="comment"># (W,H)</span></span><br></pre></td></tr></table></figure></li><li><p>第28、29、30分别为视频输入路径、图片储存文件夹、间隔帧数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input_path = <span class="string">&quot;/path/to/your/video&quot;</span>     <span class="comment"># 视频输入路径</span></span><br><span class="line">save_path = <span class="string">&quot;/path/to/your/image/&quot;</span>     <span class="comment"># 提取图片输出路径</span></span><br><span class="line">frame_interval = <span class="number">10</span>                    <span class="comment"># 抽帧间隔数</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="图片批量重命名"><a href="#图片批量重命名" class="headerlink" title="图片批量重命名"></a>图片批量重命名</h2><p><a href="/code/rename.py">rename.py</a>：将文件夹家内以一定顺序重命名；</p><ul><li><p>第4、5、6行分别为原始图片文件夹、重命名后文件夹、重命名后第一个图片序号</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">path = <span class="string">&#x27;/path/to/your/image/&#x27;</span>          <span class="comment"># 文件夹路径</span></span><br><span class="line">out_path = <span class="string">&#x27;/path/to/your/new/image/&#x27;</span>  <span class="comment"># 新文件夹路径</span></span><br><span class="line">start = <span class="number">0</span>                              <span class="comment"># 重命名后第一个图片序号</span></span><br></pre></td></tr></table></figure></li><li><p>第9行为命名原则</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">name = <span class="string">&quot;%05d&quot;</span> % start                  <span class="comment">#5位数 不足前面补零</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="依据mask图像分割"><a href="#依据mask图像分割" class="headerlink" title="依据mask图像分割"></a>依据mask图像分割</h2><p><a href="/code/extract.py">extract.py</a>：这里给出的是依据 <a href="https://github.com/hkchengrex/MiVOS/tree/MiVOS-STCN">MiVOS-STCN</a> 算法分割给出的（红色）mask进行图像分割，输出白色mask、分割后的图片；<strong>需要保持mask与原始图片名字一致！</strong></p><ul><li><p>只需修改第21—24行相关路径即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">original_path = <span class="string">&#x27;/path/to/ori/image/&#x27;</span> <span class="comment"># 原始图片路径</span></span><br><span class="line">ori_mask_path = <span class="string">&#x27;/path/to/ori/mask/&#x27;</span>  <span class="comment"># 原始红色mask路径</span></span><br><span class="line">images_path   = <span class="string">&#x27;/path/to/new/image/&#x27;</span> <span class="comment"># 分割后图片路径</span></span><br><span class="line">masks_path    = <span class="string">&#x27;/path/to/new/image/&#x27;</span> <span class="comment"># 修改颜色后mask路径</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="调整图像分辨率"><a href="#调整图像分辨率" class="headerlink" title="调整图像分辨率"></a>调整图像分辨率</h2><p><a href="/code/lowresolution.py">lowresolution.py</a>：降低图像分辨率</p><ul><li><p>修改5、6行路径</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sourceDir = os.path.join(curDir, <span class="string">&#x27;/path/to/ori/image/&#x27;</span>)</span><br><span class="line">resultDir = os.path.join(curDir, <span class="string">&#x27;/path/to/new/image/&#x27;</span>)</span><br></pre></td></tr></table></figure></li><li><p>第13行修改到指定分辨率（尽量保持原始比例）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pic_n = cv2.resize(pic, (<span class="number">960</span>, <span class="number">540</span>)) <span class="comment"># (W,H)</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="修改图片通道数"><a href="#修改图片通道数" class="headerlink" title="修改图片通道数"></a>修改图片通道数</h2><p><a href="/code/rgb2rgba.py">rgb2rgba.py</a>：有些项目需要<code>RGB</code>通道，有些则需要<code>RGBA</code>通道</p><ul><li><p>修改4、5行路径</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">path      = <span class="string">&quot;/path/to/ori/image/&quot;</span>     <span class="comment"># 原始路径</span></span><br><span class="line">save_path = <span class="string">&#x27;/path/to/new/image/&#x27;</span>     <span class="comment"># 保存路径</span></span><br></pre></td></tr></table></figure></li><li><p>第13行可以修改转换后通道数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pimg = img.convert(<span class="string">&quot;RGB&quot;</span>)  <span class="comment"># 4通道转化为rgb三通道</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="提取colmap匹配的有效图片"><a href="#提取colmap匹配的有效图片" class="headerlink" title="提取colmap匹配的有效图片"></a>提取colmap匹配的有效图片</h2><p><a href="/code/valid_imgs_from_imgstxt.py">valid_imgs_from_imgstxt.py</a>：colmap获取位姿可能因为某些原因造成部分图片无法匹配，这对有些项目影响很大！造成代码一些数组序号出问题、甚至可能无法运行，这里建议提取有效图片</p><ul><li><p>修改第25—27行相关路径即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">file_name   = <span class="string">&quot;/path/to/colmap/text/images.txt&quot;</span> <span class="comment"># images.txt 文件</span></span><br><span class="line">input_path  = <span class="string">&quot;/path/to/ori/image/&quot;</span>             <span class="comment"># 原始图片路径</span></span><br><span class="line">output_path = <span class="string">&quot;/path/to/valid/image/&quot;</span>           <span class="comment"># 提取后有效图片路径</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="图像转GIF"><a href="#图像转GIF" class="headerlink" title="图像转GIF"></a>图像转GIF</h2><p><a href="/code/image2gif.py">image2gif.py</a>：将一定顺序的图片序列转换为GIF动图</p><ul><li><p>修改13—15行相关路径</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">img_dir  = <span class="string">&#x27;/path/to/image&#x27;</span>  <span class="comment"># 图片路径</span></span><br><span class="line">duration = <span class="number">0.05</span>              <span class="comment"># 图片间隔,每秒20帧,即1/20</span></span><br><span class="line">gif_name = img_dir+<span class="string">&#x27;.gif&#x27;</span>    <span class="comment"># 输出gif名字</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="图像转视频"><a href="#图像转视频" class="headerlink" title="图像转视频"></a>图像转视频</h2><p><a href="/code/image2video.py">image2video.py</a>：将一定顺序的图片序列转换为视频</p><ul><li><p>修改16行帧率、17行视频存储路径及名字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fps = <span class="number">30</span></span><br><span class="line">file_path = <span class="string">r&quot;/pathto/video/name.mp4&quot;</span> </span><br></pre></td></tr></table></figure></li><li><p>第30行图片路径及分辨率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">picvideo(<span class="string">r&#x27;/path/to/image/&#x27;</span>, (<span class="number">1920</span>, <span class="number">1080</span>)) <span class="comment"># (W,H)</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="相机位姿添加噪声"><a href="#相机位姿添加噪声" class="headerlink" title="相机位姿添加噪声"></a>相机位姿添加噪声</h2><p><a href="/code/add_noise_in_txt.py">add_noise_in_txt.py</a>：直接给colamp跑出的 <code>image.txt</code> 添加高斯噪声</p><ul><li><p>修改第9、10行噪声方差</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sigma_r = <span class="number">5.0</span> <span class="comment"># 旋转矩阵的噪声方差</span></span><br><span class="line">sigma_t = <span class="number">0.1</span> <span class="comment"># 位移向量的噪声方差</span></span><br></pre></td></tr></table></figure></li><li><p>修改第49、50行相关路径</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">file_name = <span class="string">&quot;/path/to/colmap/text/images.txt&quot;</span> <span class="comment"># 原始colmap的image.tx</span></span><br><span class="line">output_name = <span class="string">&quot;/path/to/new/images.txt&quot;</span>       <span class="comment"># 添加噪声后的image.txt路径</span></span><br></pre></td></tr></table></figure></li></ul><p><a href="/code/add_noise_in_json.py">add_noise_in_json.py</a>：给 <code>tansform.json</code> 添加高斯噪声</p><ul><li><p>修改第41、42行噪声方差</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sigma_r = <span class="number">5.0</span> <span class="comment"># 旋转矩阵的噪声方差</span></span><br><span class="line">sigma_t = <span class="number">0.1</span> <span class="comment"># 位移向量的噪声方差</span></span><br></pre></td></tr></table></figure></li><li><p>修改第98、99行相关路径</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">file_name = <span class="string">&quot;/path/to/transform.json&quot;</span>         <span class="comment"># 原始位姿</span></span><br><span class="line">output_name = <span class="string">&quot;/path/to/new/transform.json&quot;</span>   <span class="comment"># 添加噪声后</span></span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Code </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Script </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch基本教程</title>
      <link href="/2022/11/14/pytorch.html"/>
      <url>/2022/11/14/pytorch.html</url>
      
        <content type="html"><![CDATA[<p><strong>此博客以机器学习中的一些简单模型为基础，学习Pytorch的基本架构，不做过多深入每个函数的研究与探讨，这部分只需要面向具体的项目代码即可！文章最后面会附上一些相关链接用来参考学习。</strong></p><h1 id="Pytorch基础：Tensor（张量）"><a href="#Pytorch基础：Tensor（张量）" class="headerlink" title="Pytorch基础：Tensor（张量）"></a>Pytorch基础：Tensor（张量）</h1><p>对于张量（Tensor），就可以理解为多为矩阵，也只是一种特别的存储方式而已，当然也可以表示一个元素的张量（或，标量）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.tensor([<span class="number">3.1433223</span>]) </span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br><span class="line">tensor.size()</span><br><span class="line">tensor.item()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">3.1433</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>])</span><br><span class="line"><span class="number">3.143322229385376</span></span><br></pre></td></tr></table></figure><h2 id="Tensor-的基本类型"><a href="#Tensor-的基本类型" class="headerlink" title="Tensor 的基本类型"></a>Tensor 的基本类型</h2><p>Tensor的基本数据类型有五种： </p><ul><li>32位浮点型：torch.FloatTensor。 (默认) </li><li>64位整型：torch.LongTensor。</li><li>32位整型：torch.IntTensor。</li><li>16位整型：torch.ShortTensor。</li><li>64位浮点型：torch.DoubleTensor。</li></ul><p>除以上数字类型外，还有 byte和chart型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">3.1426</span>], dtype=torch.float16)</span><br></pre></td></tr></table></figure><h2 id="设备转换"><a href="#设备转换" class="headerlink" title="设备转换"></a>设备转换</h2><p>相对于 <code>Numpy</code> 中多维矩阵的表示 <code>ndarray</code> 只能在 CPU 上运行，<code>Tensor</code> 可以在 GPU 上运行。所以有时候需要统一好每个 Tensor 所在设备。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用torch.cuda.is_available()来确定是否有cuda设备</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(device)</span><br><span class="line"><span class="comment">#将tensor传送到设备</span></span><br><span class="line">gpu_b=cpu_b.to(device)</span><br><span class="line">gpu_b.<span class="built_in">type</span>()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cuda</span><br><span class="line"><span class="string">&#x27;torch.cuda.FloatTensor&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="Autograd计算梯度数值"><a href="#Autograd计算梯度数值" class="headerlink" title="Autograd计算梯度数值"></a>Autograd计算梯度数值</h1><p>在创建张量时，可以通过设置 <code>requires_grad=True</code>来告诉 Pytorch 对该张量进行自助求导，Pytorch 会记录该张量的每一步操作历史并自动计算梯度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">x</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[0.0403, 0.5633, 0.2561, 0.4064, 0.9596],</span></span><br><span class="line"><span class="comment">#         [0.6928, 0.1832, 0.5380, 0.6386, 0.8710],</span></span><br><span class="line"><span class="comment">#         [0.5332, 0.8216, 0.8139, 0.1925, 0.4993],</span></span><br><span class="line"><span class="comment">#         [0.2650, 0.6230, 0.5945, 0.3230, 0.0752],</span></span><br><span class="line"><span class="comment">#         [0.0919, 0.4770, 0.4622, 0.6185, 0.2761]], requires_grad=True)</span></span><br><span class="line"></span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">z=x**<span class="number">2</span>+y**<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[3.3891e-01, 4.9468e-01, 8.0797e-02, 2.5656e-01, 2.9529e-01],</span></span><br><span class="line"><span class="comment">#        [7.1946e-01, 1.6977e-02, 1.7965e-01, 3.2656e-01, 1.7665e-01],</span></span><br><span class="line"><span class="comment">#        [3.1353e-01, 2.2096e-01, 1.2251e+00, 5.5087e-01, 5.9572e-02],</span></span><br><span class="line"><span class="comment">#        [1.3015e+00, 3.8029e-01, 1.1103e+00, 4.0392e-01, 2.2055e-01],</span></span><br><span class="line"><span class="comment">#        [8.8726e-02, 6.9701e-01, 8.0164e-01, 9.7221e-01, 4.2239e-04]],</span></span><br><span class="line"><span class="comment">#       grad_fn=&lt;AddBackward0&gt;)</span></span><br></pre></td></tr></table></figure><p>在张量进行操作后，<code>grad_fn</code> 已经被赋予了一个新的函数，这个函数引用了一个创建了这个 Tensor 类的 Function 对象。 Tensor 和 Function 互相连接生成了一个非循环图，它记录并且编码了完整的计算历史。每个张量都有一个 <code>`.grad_fn</code> 属性，如果这个张量是用户手动创建的那么这个张量的 <code>grad_fn</code> 是 <code>None</code>。</p><p>当计算完成后调用 <code>.backward()</code> 方法自动计算梯度并且将计算结果保存到 <code>grad</code> 属性中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">z.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad,y.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.]]) </span></span><br><span class="line"><span class="comment"># tensor([[1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们的返回值不是一个标量，所以需要输入一个大小相同的张量作为参数，</span></span><br><span class="line"><span class="comment"># 这里我们用ones_like函数根据x生成一个张量</span></span><br><span class="line">z.backward(torch.ones_like(x))</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[0.2087, 1.3554, 0.5560, 1.0009, 0.9931],</span></span><br><span class="line"><span class="comment">#        [1.2655, 0.1223, 0.8008, 1.1127, 0.7261],</span></span><br><span class="line"><span class="comment">#        [1.1052, 0.2579, 1.8006, 0.1544, 0.3646],</span></span><br><span class="line"><span class="comment">#        [1.8855, 1.2296, 1.9061, 0.9313, 0.0648],</span></span><br><span class="line"><span class="comment">#        [0.5952, 1.6190, 0.8430, 1.9213, 0.0322]])</span></span><br></pre></td></tr></table></figure><p>我们可以使用 <code>with torch.no_grad()</code> 上下文管理器临时禁止对已设置 <code>requires_grad=True</code> 的张量进行自动求导。这个方法在测试集计算准确率的时候会经常用到，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="built_in">print</span>((x +y*<span class="number">2</span>).requires_grad)</span><br><span class="line">   </span><br><span class="line"><span class="comment"># False</span></span><br></pre></td></tr></table></figure><p>使用 <code>.no_grad()</code> 进行嵌套后，代码不会跟踪历史记录，也就是说保存的这部分记录会减少内存的使用量并且会加快少许的运算速度。</p><h1 id="数据的加载和预处理"><a href="#数据的加载和预处理" class="headerlink" title="数据的加载和预处理"></a>数据的加载和预处理</h1><p>PyTorch通过 <code>torch.utils.data</code> 对一般常用的数据加载进行了封装，可以很容易地实现多线程数据预读和批量加载。 并且 <code>torchvision</code> 已经预先实现了常用图像数据集，包括前面使用过的CIFAR-10，ImageNet、COCO、MNIST、LSUN等数据集，可通过 <code>torchvision.datasets</code> 方便的调用。这里不具体介绍。</p><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>Dataset 是一个抽象类，为了能够方便的读取，需要将要使用的数据包装为 Dataset 类。 自定义的 Dataset 需要继承它并且实现两个成员方法： </p><pre><code>- `__getitem__()` 该方法定义用索引(`0` 到 `len(self)`)获取一条数据或一个样本-  `__len__()` 该方法返回数据集的总长度</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#引用</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义一个数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BulldozerDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 数据集演示 &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, csv_file</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;实现初始化方法，在初始化的时候将数据读载入&quot;&quot;&quot;</span></span><br><span class="line">        self.df=pd.read_csv(csv_file)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        返回df的长度</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.df)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        根据 idx 返回一行数据</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> self.df.iloc[idx].SalePrice</span><br><span class="line"><span class="comment"># ------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 实例化一个对象访问它</span></span><br><span class="line">ds_demo= BulldozerDataset(<span class="string">&#x27;median_benchmark.csv&#x27;</span>)</span><br><span class="line"><span class="comment"># 实现了 __len__ 方法所以可以直接使用len获取数据总数</span></span><br><span class="line"><span class="built_in">len</span>(ds_demo)</span><br><span class="line"><span class="comment"># 用索引可以直接访问对应的数据，对应 __getitem__ 方法</span></span><br><span class="line">ds_demo[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>自定义的数据集已经创建好了，下面我们使用官方提供的数据载入器，读取数据。</p><h2 id="Dataloader"><a href="#Dataloader" class="headerlink" title="Dataloader"></a>Dataloader</h2><p>DataLoader 为我们提供了对 Dataset 的读取操作，常用参数有：</p><ul><li>batch_size(每个batch的大小)、 </li><li>shuffle(是否进行shuffle操作)、 </li><li>num_workers(加载数据的时候使用几个子进程)。</li></ul><p>下面做一个简单的操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">dl = torch.utils.data.DataLoader(ds_demo, batch_size=<span class="number">10</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataLoader返回的是一个可迭代对象，我们可以使用迭代器分次获取数据</span></span><br><span class="line">idata=<span class="built_in">iter</span>(dl)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(idata))</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000.], dtype=torch.float64)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 常见的用法是使用for循环对其进行遍历</span></span><br><span class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(dl):</span><br><span class="line">    <span class="built_in">print</span>(i,data)</span><br></pre></td></tr></table></figure><p>我们已经可以通过 Dataset 定义数据集，并使用 Datalorder 载入和遍历数据集，除了这些以外， PyTorch 还提供能 torcvision 的计算机视觉扩展包。</p><h2 id="torchvision-包"><a href="#torchvision-包" class="headerlink" title="torchvision 包"></a>torchvision 包</h2><p>torchvision 是PyTorch中专门用来处理图像的库。</p><h3 id="torchvision-datasets"><a href="#torchvision-datasets" class="headerlink" title="torchvision.datasets"></a>torchvision.datasets</h3><p>torchvision.datasets 可以理解为PyTorch团队自定义的dataset，这些dataset帮我们提前处理好了很多的图片数据集，我们拿来就可以直接使用： - MNIST - COCO - Captions - Detection - LSUN - ImageFolder - Imagenet-12 - CIFAR - STL10 - SVHN - PhotoTour 我们可以直接使用，示例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> datasets</span><br><span class="line">trainset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, <span class="comment"># 表示 MNIST 数据的加载的目录</span></span><br><span class="line">                          train=<span class="literal">True</span>,    <span class="comment"># 表示是否加载数据库的训练集，false的时候加载测试集</span></span><br><span class="line">                          download=<span class="literal">True</span>, <span class="comment"># 表示是否自动下载 MNIST 数据集</span></span><br><span class="line">                          transform=<span class="literal">None</span>)<span class="comment"># 表示是否需要对数据进行预处理，none为不进行预处理</span></span><br></pre></td></tr></table></figure><h3 id="torchvision-models"><a href="#torchvision-models" class="headerlink" title="torchvision.models"></a>torchvision.models</h3><p>torchvision 不仅提供了常用图片数据集，还提供了训练好的模型，可以加载之后，直接使用，或者在进行迁移学习 <code>torchvision.models</code> 模块的 子模块中包含以下模型结构。 - AlexNet - VGG - ResNet - SqueezeNet - DenseNet</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#我们直接可以使用训练好的模型，当然这个与datasets相同，都是需要从服务器下载的</span></span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line">resnet18 = models.resnet18(pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="torchvision-transforms"><a href="#torchvision-transforms" class="headerlink" title="torchvision.transforms"></a>torchvision.transforms</h3><p>transforms 模块提供了一般的图像转换操作类，用作数据处理和数据增强</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="keyword">as</span> transforms</span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.RandomCrop(<span class="number">32</span>, padding=<span class="number">4</span>), <span class="comment">#先四周填充0，在把图像随机裁剪成32*32</span></span><br><span class="line">    transforms.RandomHorizontalFlip(),    <span class="comment">#图像一半的概率翻转，一半的概率不翻转</span></span><br><span class="line">    transforms.RandomRotation((-<span class="number">45</span>,<span class="number">45</span>)),  <span class="comment">#随机旋转</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>), (<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>)), <span class="comment">#R,G,B每层的归一化用到的均值和方差</span></span><br><span class="line">])</span><br></pre></td></tr></table></figure><p>肯定有人会问：(0.485, 0.456, 0.406), (0.2023, 0.1994, 0.2010) 这几个数字是什么意思？官方的这个帖子有详细的说明: <a href="https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/21">https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/21</a> 这些都是根据ImageNet训练的归一化参数，可以直接使用，我们认为这个是固定值就可以。</p><h1 id="神经网络包nn和优化器optimizer"><a href="#神经网络包nn和优化器optimizer" class="headerlink" title="神经网络包nn和优化器optimizer"></a>神经网络包nn和优化器optimizer</h1><p><code>torch.nn</code> 是专门为神经网络设计的模块化接口。<code>nn</code> 构建于 <code>Autograd</code> 之上，可用来定义和运行神经网络。 这里我们主要介绍几个一些常用的类。除了nn别名以外，我们还引用了nn.functional，这个包中包含了神经网络中使用的一些常用函数，这些函数的特点是，不具有可学习的参数(如ReLU，pool，DropOut等)，这些函数可以放在构造函数中，也可以不放，但是这里建议不放。一般情况下我们会<strong>将nn.functional 设置为大写的F</strong>，这样缩写方便调用。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 首先要引入相关的包</span><br><span class="line">import torch</span><br><span class="line"># 引入torch.nn并指定别名</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br></pre></td></tr></table></figure><h2 id="定义一个网络"><a href="#定义一个网络" class="headerlink" title="定义一个网络"></a>定义一个网络</h2><p>PyTorch 中已经为我们准备好了现成的网络模型，只要继承 <code>nn.Module</code>，并实现它的 <code>forward</code> 方法，PyTorch 会根据 <code>autograd</code>，自动实现 <code>backward</code> 函数，在 <code>forward</code> 函数中可使用任何 tensor 支持的函数，还可以使用 if、for 循环、print、log 等 Python 语法，写法和标准的 Python 写法一致。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># nn.Module子类的函数必须在构造函数中执行父类的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 卷积层 &#x27;1&#x27;表示输入图片为单通道， &#x27;6&#x27;表示输出通道数，&#x27;3&#x27;表示卷积核为3*3</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">3</span>) </span><br><span class="line">        <span class="comment">#线性层，输入1350个特征，输出10个特征</span></span><br><span class="line">        self.fc1   = nn.Linear(<span class="number">1350</span>, <span class="number">10</span>)  <span class="comment">#这里的1350是如何计算的呢？这就要看后面的forward函数</span></span><br><span class="line">    <span class="comment">#正向传播 </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): </span><br><span class="line">        <span class="built_in">print</span>(x.size()) <span class="comment"># 结果：[1, 1, 32, 32]</span></span><br><span class="line">        <span class="comment"># 卷积 -&gt; 激活 -&gt; 池化 </span></span><br><span class="line">        x = self.conv1(x) <span class="comment">#根据卷积的尺寸计算公式，计算结果是30，具体计算公式后面第二章第四节 卷积神经网络 有详细介绍。</span></span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        <span class="built_in">print</span>(x.size()) <span class="comment"># 结果：[1, 6, 30, 30]</span></span><br><span class="line">        x = F.max_pool2d(x, (<span class="number">2</span>, <span class="number">2</span>)) <span class="comment">#我们使用池化层，计算结果是15</span></span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        <span class="built_in">print</span>(x.size()) <span class="comment"># 结果：[1, 6, 15, 15]</span></span><br><span class="line">        <span class="comment"># reshape，‘-1’表示自适应</span></span><br><span class="line">        <span class="comment">#这里做的就是压扁的操作 就是把后面的[1, 6, 15, 15]压扁，变为 [1, 1350]</span></span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], -<span class="number">1</span>) </span><br><span class="line">        <span class="built_in">print</span>(x.size()) <span class="comment"># 这里就是fc1层的的输入1350 </span></span><br><span class="line">        x = self.fc1(x)        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Net(</span></span><br><span class="line"><span class="comment">#   (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=1350, out_features=10, bias=True)</span></span><br><span class="line"><span class="comment"># )</span></span><br></pre></td></tr></table></figure><p>网络的可学习参数通过 <code>net.parameters()</code> 返回</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> parameters <span class="keyword">in</span> net.parameters():</span><br><span class="line">    <span class="built_in">print</span>(parameters)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># ----------输出----------</span></span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[[[ <span class="number">0.2745</span>,  <span class="number">0.2594</span>,  <span class="number">0.0171</span>],</span><br><span class="line">          [ <span class="number">0.0429</span>,  <span class="number">0.3013</span>, -<span class="number">0.0208</span>],</span><br><span class="line">          [ <span class="number">0.1459</span>, -<span class="number">0.3223</span>,  <span class="number">0.1797</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ <span class="number">0.1847</span>,  <span class="number">0.0227</span>, -<span class="number">0.1919</span>],</span><br><span class="line">          [-<span class="number">0.0210</span>, -<span class="number">0.1336</span>, -<span class="number">0.2176</span>],</span><br><span class="line">          [-<span class="number">0.2164</span>, -<span class="number">0.1244</span>, -<span class="number">0.2428</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ <span class="number">0.1042</span>, -<span class="number">0.0055</span>, -<span class="number">0.2171</span>],</span><br><span class="line">          [ <span class="number">0.3306</span>, -<span class="number">0.2808</span>,  <span class="number">0.2058</span>],</span><br><span class="line">          [ <span class="number">0.2492</span>,  <span class="number">0.2971</span>,  <span class="number">0.2277</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ <span class="number">0.2134</span>, -<span class="number">0.0644</span>, -<span class="number">0.3044</span>],</span><br><span class="line">          [ <span class="number">0.0040</span>,  <span class="number">0.0828</span>, -<span class="number">0.2093</span>],</span><br><span class="line">          [ <span class="number">0.0204</span>,  <span class="number">0.1065</span>,  <span class="number">0.1168</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ <span class="number">0.1651</span>, -<span class="number">0.2244</span>,  <span class="number">0.3072</span>],</span><br><span class="line">          [-<span class="number">0.2301</span>,  <span class="number">0.2443</span>, -<span class="number">0.2340</span>],</span><br><span class="line">          [ <span class="number">0.0685</span>,  <span class="number">0.1026</span>,  <span class="number">0.1754</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ <span class="number">0.1691</span>, -<span class="number">0.0790</span>,  <span class="number">0.2617</span>],</span><br><span class="line">          [ <span class="number">0.1956</span>,  <span class="number">0.1477</span>,  <span class="number">0.0877</span>],</span><br><span class="line">          [ <span class="number">0.0538</span>, -<span class="number">0.3091</span>,  <span class="number">0.2030</span>]]]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([ <span class="number">0.2355</span>,  <span class="number">0.2949</span>, -<span class="number">0.1283</span>, -<span class="number">0.0848</span>,  <span class="number">0.2027</span>, -<span class="number">0.3331</span>],</span><br><span class="line">       requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ <span class="number">2.0555e-02</span>, -<span class="number">2.1445e-02</span>, -<span class="number">1.7981e-02</span>,  ..., -<span class="number">2.3864e-02</span>,</span><br><span class="line">          <span class="number">8.5149e-03</span>, -<span class="number">6.2071e-04</span>],</span><br><span class="line">        [-<span class="number">1.1755e-02</span>,  <span class="number">1.0010e-02</span>,  <span class="number">2.1978e-02</span>,  ...,  <span class="number">1.8433e-02</span>,</span><br><span class="line">          <span class="number">7.1362e-03</span>, -<span class="number">4.0951e-03</span>],</span><br><span class="line">        [ <span class="number">1.6187e-02</span>,  <span class="number">2.1623e-02</span>,  <span class="number">1.1840e-02</span>,  ...,  <span class="number">5.7059e-03</span>,</span><br><span class="line">         -<span class="number">2.7165e-02</span>,  <span class="number">1.3463e-03</span>],</span><br><span class="line">        ...,</span><br><span class="line">        [-<span class="number">3.2552e-03</span>,  <span class="number">1.7277e-02</span>, -<span class="number">1.4907e-02</span>,  ...,  <span class="number">7.4232e-03</span>,</span><br><span class="line">         -<span class="number">2.7188e-02</span>, -<span class="number">4.6431e-03</span>],</span><br><span class="line">        [-<span class="number">1.9786e-02</span>, -<span class="number">3.7382e-03</span>,  <span class="number">1.2259e-02</span>,  ...,  <span class="number">3.2471e-03</span>,</span><br><span class="line">         -<span class="number">1.2375e-02</span>, -<span class="number">1.6372e-02</span>],</span><br><span class="line">        [-<span class="number">8.2350e-03</span>,  <span class="number">4.1301e-03</span>, -<span class="number">1.9192e-03</span>,  ..., -<span class="number">2.3119e-05</span>,</span><br><span class="line">          <span class="number">2.0167e-03</span>,  <span class="number">1.9528e-02</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([ <span class="number">0.0162</span>, -<span class="number">0.0146</span>, -<span class="number">0.0218</span>,  <span class="number">0.0212</span>, -<span class="number">0.0119</span>, -<span class="number">0.0142</span>, -<span class="number">0.0079</span>,  <span class="number">0.0171</span>,</span><br><span class="line">         <span class="number">0.0205</span>,  <span class="number">0.0164</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><code>net.named_parameters</code> 可同时返回可学习的参数及名称</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name,parameters <span class="keyword">in</span> net.named_parameters():</span><br><span class="line"></span><br><span class="line"><span class="comment"># conv1.weight : torch.Size([6, 1, 3, 3])</span></span><br><span class="line"><span class="comment"># conv1.bias : torch.Size([6])</span></span><br><span class="line"><span class="comment"># fc1.weight : torch.Size([10, 1350])</span></span><br><span class="line"><span class="comment"># fc1.bias : torch.Size([10])</span></span><br></pre></td></tr></table></figure><p>forward函数的输入和输出都是Tensor</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>) <span class="comment"># 这里的对应前面fforward的输入是32</span></span><br><span class="line">out = net(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">input</span>.size()</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.Size([1, 1, 32, 32])</span></span><br><span class="line"></span><br><span class="line">out.size()</span><br><span class="line"></span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">6</span>, <span class="number">30</span>, <span class="number">30</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">6</span>, <span class="number">15</span>, <span class="number">15</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">1350</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>在反向传播前，先要将所有参数的梯度清零</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad() </span><br><span class="line">out.backward(torch.ones(<span class="number">1</span>,<span class="number">10</span>)) <span class="comment"># 反向传播的实现是PyTorch自动实现的，我们只要调用这个函数即可</span></span><br></pre></td></tr></table></figure><p><strong>注意</strong>：<code>torch.nn</code> 只支持 <code>mini-batches</code>，不支持一次只输入一个样本，即一次必须是一个batch。也就是说，就算我们输入一个样本，也会对样本进行分批，所以，所有的输入都会增加一个维度，我们对比下刚才的 <code>input</code>，<code>nn</code> 中定义为3维，但是我们人工创建时多增加了一个维度，变为了4维，最前面的1即为batch-size。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>在nn中PyTorch还预制了常用的损失函数，下面我们用MSELoss用来计算均方误差：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">y = torch.arange(<span class="number">0</span>,<span class="number">10</span>).view(<span class="number">1</span>,<span class="number">10</span>).<span class="built_in">float</span>()</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">loss = criterion(out, y)</span><br><span class="line"><span class="comment">#loss是个scalar，我们可以直接用item获取到他的python类型的数值</span></span><br><span class="line"><span class="built_in">print</span>(loss.item()) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 28.92203712463379</span></span><br></pre></td></tr></table></figure><h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><p>在反向传播计算完所有参数的梯度后，还需要使用优化方法来更新网络的权重和参数，例如随机梯度下降法(SGD)的更新策略如下：</p><p><code>weight = weight - learning_rate * gradient</code></p><p>在 <code>torch.optim</code> 中实现大多数的优化方法，例如 RMSProp、Adam、SGD等，下面我们使用SGD做个简单的样例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim</span><br><span class="line"></span><br><span class="line">out = net(<span class="built_in">input</span>) <span class="comment"># 这里调用的时候会打印出我们在forword函数中打印的x的大小</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">loss = criterion(out, y)</span><br><span class="line"><span class="comment">#新建一个优化器，SGD只需要要调整的参数和学习率</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr = <span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># 先梯度清零(与net.zero_grad()效果一样)</span></span><br><span class="line">optimizer.zero_grad() </span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment">#更新参数</span></span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><p>这样，神经网络的数据的一个完整的传播就已经通过PyTorch实现了。</p><h1 id="深度学习基础"><a href="#深度学习基础" class="headerlink" title="深度学习基础"></a>深度学习基础</h1><h2 id="监督学习和无监督学习"><a href="#监督学习和无监督学习" class="headerlink" title="监督学习和无监督学习"></a>监督学习和无监督学习</h2><p>监督学习、无监督学习、半监督学习、强化学习是我们日常接触到的常见的四个机器学习方法：</p><ul><li>监督学习：通过已有的训练样本（即已知数据以及其对应的输出）去训练得到一个最优模型（这个模型属于某个函数的集合，最优则表示在某个评价准则下是最佳的），再利用这个模型将所有的输入映射为相应的输出。</li><li>无监督学习：它与监督学习的不同之处，在于我们事先没有任何训练样本，而需要直接对数据进行建模。</li><li>半监督学习 ：在训练阶段结合了大量未标记的数据和少量标签数据。与使用所有标签数据的模型相比，使用训练集的训练模型在训练时可以更为准确。</li><li>强化学习：我们设定一个回报函数（reward function），通过这个函数来确认否越来越接近目标，类似我们训练宠物，如果做对了就给他奖励，做错了就给予惩罚，最后来达到我们的训练目的。</li></ul><p>这里我们只着重介绍监督学习，因为我们后面的绝大部们课程都是使用的监督学习的方法，在训练和验证时输入的数据既包含输入x，又包含x对应的输出y，即学习数据已经事先给出了正确答案。</p><h2 id="线性回归-（Linear-Regreesion）"><a href="#线性回归-（Linear-Regreesion）" class="headerlink" title="线性回归 （Linear Regreesion）"></a>线性回归 （Linear Regreesion）</h2><p>这里不解释具体原理，直接看写法，体会训练过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意，这里我们使用了一个新库叫 seaborn 如果报错找不到包的话请使用pip install seaborn 来进行安装</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Linear, Module, MSELoss</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> SGD</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面我生成一些随机的点，来作为我们的训练数据，回归：y = 5*x + 7</span></span><br><span class="line">x = np.random.rand(<span class="number">256</span>)</span><br><span class="line">noise = np.random.randn(<span class="number">256</span>) / <span class="number">4</span></span><br><span class="line">y = x * <span class="number">5</span> + <span class="number">7</span> + noise</span><br><span class="line">df = pd.DataFrame()</span><br><span class="line">df[<span class="string">&#x27;x&#x27;</span>] = x</span><br><span class="line">df[<span class="string">&#x27;y&#x27;</span>] = y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们随机生成了一些点，下面将使用PyTorch建立一个线性的模型来对其进行拟合，这就是所说的训练的过程，由于只有一层线性模型，所以我们就直接使用了:</span></span><br><span class="line">model=Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 其中参数(1,1)代表输入输出的特征(feature)数量都是1</span></span><br><span class="line"><span class="comment"># Linear 模型的表达式是y=wx+b，其中w代表权重，b代表偏置</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数我们使用均方损失函数</span></span><br><span class="line">criterion = MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器我们选择最常见的优化方法 SGD，就是每一次迭代计算 mini-batch 的梯度，然后对参数进行更新，学习率 0.01</span></span><br><span class="line">optim = SGD(model.parameters(), lr = <span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练3000次</span></span><br><span class="line">epochs = <span class="number">3000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备训练数据: x_train, y_train 的形状是(256, 1)， </span></span><br><span class="line"><span class="comment"># 代表mini-batch大小为256，feature为1. astype(&#x27;float32&#x27;) 是为了下一步可以直接转换为 torch.float</span></span><br><span class="line">x_train = x.reshape(-<span class="number">1</span>, <span class="number">1</span>).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">y_train = y.reshape(-<span class="number">1</span>, <span class="number">1</span>).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 整理输入和输出的数据，这里输入和输出一定要是torch的Tensor类型</span></span><br><span class="line">    inputs = torch.from_numpy(x_train)</span><br><span class="line">    labels = torch.from_numpy(y_train)</span><br><span class="line">    <span class="comment">#使用模型进行预测</span></span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    <span class="comment">#梯度置0，否则会累加</span></span><br><span class="line">    optim.zero_grad()</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 使用优化器默认方法优化</span></span><br><span class="line">    optim.step()</span><br><span class="line">    <span class="keyword">if</span> (i%<span class="number">100</span>==<span class="number">0</span>):</span><br><span class="line">        <span class="comment">#每 100次打印一下损失函数，看看效果</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch &#123;&#125;, loss &#123;:1.4f&#125;&#x27;</span>.<span class="built_in">format</span>(i,loss.data.item()))</span><br></pre></td></tr></table></figure><p>训练完成了，看一下训练的成果是多少。用 <code>model.parameters()</code> 提取模型参数。 $w$， $b$ 是我们所需要训练的模型参数，我们期望的数据 $w=5$，$b=7$ 可以做一下对比</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[w, b] = model.parameters()</span><br><span class="line"><span class="built_in">print</span> (w.item(),b.item())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.994358062744141 7.0252156257629395</span></span><br></pre></td></tr></table></figure><h2 id="损失函数-Loss-Function"><a href="#损失函数-Loss-Function" class="headerlink" title="损失函数(Loss Function)"></a>损失函数(Loss Function)</h2><p>损失函数（loss function）是用来估量模型的预测值(我们例子中的output)与真实值（例子中的y_train）的不一致程度，它是一个非负实值函数，损失函数越小，模型的鲁棒性就越好。 我们训练模型的过程，就是通过不断的迭代计算，使用梯度下降的优化算法，使得损失函数越来越小。损失函数越小就表示算法达到意义上的最优。</p><p>这里有一个重点：因为PyTorch是使用mini-batch来进行计算的，所以损失函数的计算出来的结果已经对mini-batch取了平均。</p><p>常见（PyTorch内置）的损失函数有以下几个：</p><h3 id="nn-L1Loss"><a href="#nn-L1Loss" class="headerlink" title="nn.L1Loss:"></a>nn.L1Loss:</h3><p>输入x和目标y之间差的绝对值，要求 x 和 y 的维度要一样（可以是向量或者矩阵），得到的 loss 维度也是对应一样的</p><script type="math/tex; mode=display">loss(x,y)=1/n\sum|x_i-y_i|</script><h3 id="nn-NLLLoss"><a href="#nn-NLLLoss" class="headerlink" title="nn.NLLLoss:"></a>nn.NLLLoss:</h3><p>用于多分类的负对数似然损失函数$loss(x, class) = -x[class]$；</p><p>NLLLoss中如果传递了weights参数，会对损失进行加权，公式就变成了</p><script type="math/tex; mode=display">loss(x, class) = -weights[class] * x[class]</script><h3 id="n-MSELoss"><a href="#n-MSELoss" class="headerlink" title="n.MSELoss:"></a>n.MSELoss:</h3><p>均方损失函数 ，输入x和目标y之间均方差</p><script type="math/tex; mode=display">loss(x,y)=1/n\sum(x_i-y_i)^2</script><h3 id="nn-CrossEntropyLoss"><a href="#nn-CrossEntropyLoss" class="headerlink" title="nn.CrossEntropyLoss:"></a>nn.CrossEntropyLoss:</h3><p>多分类用的交叉熵损失函数，LogSoftMax 和 NLLLoss 集成到一个类中，会调用 nn.NLLLoss 函数，我们可以理解为 CrossEntropyLoss()=log_softmax() + NLLLoss()</p><script type="math/tex; mode=display"> \begin{aligned} loss(x, class) &= -\text{log}\frac{exp(x[class])}{\sum_j exp(x[j]))}\ &= -x[class] + log(\sum_j exp(x[j])) \end{aligned}</script><p>因为使用了NLLLoss，所以也可以传入weight参数，这时loss的计算公式变为：</p><script type="math/tex; mode=display">loss(x, class) = weights[class] * (-x[class] + log(\sum_j exp(x[j])))</script><p>所以一般多分类的情况会使用这个损失函数；</p><h3 id="nn-BCELoss"><a href="#nn-BCELoss" class="headerlink" title="nn.BCELoss:"></a>nn.BCELoss:</h3><p>计算 x 与 y 之间的二进制交叉熵。$loss(o,t)=-\frac{1}{n}\sum_i(t[i] <em>log(o[i])+(1-t[i])</em> log(1-o[i]))$ 与NLLLoss类似，也可以添加权重参数：</p><script type="math/tex; mode=display">loss(o,t)=-\frac{1}{n}\sum_iweights[i] *(t[i]* log(o[i])+(1-t[i])* log(1-o[i]))</script><p>用的时候需要在该层前面加上 Sigmoid 函数。</p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>在介绍损失函数的时候我们已经说了，梯度下降是一个使损失函数越来越小的优化算法，在无求解机器学习算法的模型参数，即约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一。所以梯度下降是我们目前所说的机器学习的核心，了解了它的含义，也就了解了机器学习算法的含义。</p><h3 id="Mini-batch的梯度下降法"><a href="#Mini-batch的梯度下降法" class="headerlink" title="Mini-batch的梯度下降法"></a>Mini-batch的梯度下降法</h3><p>对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候处理速度会很慢，而且也不可能一次的载入到内存或者显存中，所以我们会把大数据集分成小数据集，一部分一部分的训练，这个训练子集即称为Mini-batch。 在PyTorch中就是使用这种方法进行的训练，可以看看上一章中关于dataloader的介绍里面的batch_size就是我们一个Mini-batch的大小。</p><p>为了介绍的更简洁，使用 吴恩达老师的 <a href="https://www.deeplearning.ai/deep-learning-specialization/">deeplearning.ai</a> 课程板书。</p><p>对于普通的梯度下降法，一个epoch只能进行一次梯度下降；而对于Mini-batch梯度下降法，一个epoch可以进行Mini-batch的个数次梯度下降。 <img src="https://handbook.pytorch.wiki/chapter2/2.png" alt="img"> 普通的batch梯度下降法和Mini-batch梯度下降法代价函数的变化趋势，如下图所示： <img src="https://handbook.pytorch.wiki/chapter2/3.png" alt="img"></p><ul><li><p>如果训练样本的大小比较小时，能够一次性的读取到内存中，那我们就不需要使用Mini-batch，</p></li><li><p>如果训练样本的大小比较大时，一次读入不到内存或者现存中，那我们必须要使用 Mini-batch来分批的计算 </p></li><li>Mini-batch size的计算规则如下，在内存允许的最大情况下使用2的N次方个size <img src="https://handbook.pytorch.wiki/chapter2/4.png" alt="img"></li></ul><p><code>torch.optim</code>是一个实现了各种优化算法的库。大部分常用优化算法都有实现，我们直接调用即可。</p><h3 id="torch-optim-SGD"><a href="#torch-optim-SGD" class="headerlink" title="torch.optim.SGD"></a>torch.optim.SGD</h3><p>随机梯度下降算法，带有动量（momentum）的算法作为一个可选参数可以进行设置，样例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#lr参数为学习率，对于SGD来说一般选择0.1 0.01.0.001，如何设置会在后面实战的章节中详细说明</span></span><br><span class="line"><span class="comment">##如果设置了momentum，就是带有动量的SGD，可以不设置</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><h3 id="torch-optim-RMSprop"><a href="#torch-optim-RMSprop" class="headerlink" title="torch.optim.RMSprop"></a>torch.optim.RMSprop</h3><p>除了以上的带有动量Momentum梯度下降法外，RMSprop（root mean square prop）也是一种可以加快梯度下降的算法，利用RMSprop算法，可以减小某些维度梯度更新波动较大的情况，使其梯度下降的速度变得更快</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#我们的课程基本不会使用到RMSprop所以这里只给一个实例</span></span><br><span class="line">optimizer = torch.optim.RMSprop(model.parameters(), lr=<span class="number">0.01</span>, alpha=<span class="number">0.99</span>)</span><br></pre></td></tr></table></figure><h3 id="torch-optim-Adam"><a href="#torch-optim-Adam" class="headerlink" title="torch.optim.Adam"></a>torch.optim.Adam</h3><p>Adam 优化算法的基本思想就是将 Momentum 和 RMSprop 结合起来形成的一种适用于不同深度学习结构的优化算法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里的lr，betas，还有eps都是用默认值即可，所以Adam是一个使用起来最简单的优化方法</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>, betas=(<span class="number">0.9</span>, <span class="number">0.999</span>), eps=<span class="number">1e-08</span>)</span><br></pre></td></tr></table></figure><h2 id="方差-偏差"><a href="#方差-偏差" class="headerlink" title="方差/偏差"></a>方差/偏差</h2><ul><li>偏差度量了学习算法的期望预测与真实结果的偏离程序，即刻画了学习算法本身的拟合能力</li><li>方差度量了同样大小的训练集的变动所导致的学习性能的变化，即模型的泛化能力 <img src="https://handbook.pytorch.wiki/chapter2/5.png" alt="img"></li></ul><p>从图中我们可以看出 - 高偏差（high bias）的情况，一般称为欠拟合（underfitting），即我们的模型并没有很好的去适配现有的数据，拟合度不够。 - 高方差（high variance）的情况一般称作过拟合（overfitting），即模型对于训练数据拟合度太高了，失去了泛化的能力。</p><p>如何解决这两种情况呢？</p><ul><li><p>欠拟合： </p><ul><li>增加网络结构，如增加隐藏层数目； </li><li>训练更长时间；</li><li>寻找合适的网络架构，使用更大的NN结构；</li></ul></li><li><p>过拟合 ： </p><ul><li>使用更多的数据；</li><li>正则化（ regularization）； </li><li>寻找合适的网络结构；</li></ul></li></ul><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>利用正则化来解决High variance 的问题，正则化是在 Cost function 中加入一项正则化项，惩罚模型的复杂度，这里我们简单的介绍一下正则化的概念</p><h3 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h3><p>损失函数基础上加上权重参数的绝对值 $L=E_{in}+\lambda{\sum_j} \left|w_j\right|$；</p><h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p>损失函数基础上加上权重参数的平方和 $L=E_{in}+\lambda{\sum_j} w^2_j$；</p><p>需要说明的是：$L_1$ 相比于 $L_2$ 会更容易获得稀疏解。</p><h1 id="卷积神经网络经典模型LeNet-5"><a href="#卷积神经网络经典模型LeNet-5" class="headerlink" title="卷积神经网络经典模型LeNet-5"></a>卷积神经网络经典模型LeNet-5</h1><p>卷积神经网路的开山之作，麻雀虽小，但五脏俱全，卷积层、pooling层、全连接层，这些都是现代CNN网络的基本组件 - 用卷积提取空间特征； </p><ul><li>由空间平均得到子样本； </li><li>用 tanh 或 sigmoid 得到非线性； </li><li>用 multi-layer neural network（MLP）作为最终分类器；</li><li>层层之间用稀疏的连接矩阵，以避免大的计算成本。 <img src="https://handbook.pytorch.wiki/chapter2/lenet5.jpg" alt="img"></li></ul><p>输入：图像Size为32*32。</p><p>输出：10个类别，分别为0-9数字的概率</p><ol><li>C1层是一个卷积层，有6个卷积核（提取6种局部特征），核大小为5 * 5</li><li>S2层是pooling层，下采样（区域:2 * 2 ）降低网络训练参数及模型的过拟合程度。</li><li>C3层是第二个卷积层，使用16个卷积核，核大小:5 * 5 提取特征</li><li>S4层也是一个pooling层，区域:2*2</li><li>C5层是最后一个卷积层，卷积核大小:5 * 5 卷积核种类:120</li><li>最后使用全连接层，将C5的120个特征进行分类，最后输出0-9的概率</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet5</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LeNet5, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 5x5 square convolution</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>) <span class="comment"># 这里论文上写的是conv,官方教程用了线性层</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># If the size is a square you can only specify a single number</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">num_flat_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = LeNet5()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"></span><br><span class="line"><span class="comment"># LeNet5(</span></span><br><span class="line"><span class="comment">#   (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#   (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=400, out_features=120, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=120, out_features=84, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc3): Linear(in_features=84, out_features=10, bias=True)</span></span><br><span class="line"><span class="comment"># )</span></span><br></pre></td></tr></table></figure><h1 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h1><p><a href="http://www.feiguyunai.com/index.php/2019/06/13/python-ml-pytorch/">《Python深度学习基于PyTorch》 | Python技术交流与分享 (feiguyunai.com)</a></p><p><a href="https://github.com/ShusenTang/Dive-into-DL-PyTorch">ShusenTang/Dive-into-DL-PyTorch: 本项目将《动手学深度学习》(Dive into Deep Learning)原书中的MXNet实现改为PyTorch实现。 (github.com)</a></p>]]></content>
      
      
      <categories>
          
          <category> Code </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络简介</title>
      <link href="/2022/11/13/network.html"/>
      <url>/2022/11/13/network.html</url>
      
        <content type="html"><![CDATA[<p><strong>说明</strong>：本文以机器学习中最基本的分类网络为背景，介绍神经网络的基本结构；基本只需要知道这些基本组成便可以着手具体的项目代码，而一些特殊的网络组成，只需遇到时自行搜索学习即可。</p><p>传统<strong>k-近邻（KNN）算</strong>法：</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/k-neighbor.png"                        width="178" height="146.5"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><ul><li><p><strong>问：</strong>绿色是属于方块还是三角？——取决于 k。</p></li><li><p><strong>算法流程：</strong></p><ul><li>（1）计算已知类别数据集中的点与当前点的距离，并依次排序</li><li>（2）选取与当前点距离最小的 k 个点，并确定类别；</li><li>（3）返回前 k 个点出现频率最高的类别作为预测。</li></ul></li></ul><h2 id="设计网络需要考虑"><a href="#设计网络需要考虑" class="headerlink" title="设计网络需要考虑"></a>设计网络需要考虑</h2><ul><li>数据的预处理与初始化如何更高效？</li><li>网络结构：隐藏层层数、每层神经元个数？</li><li>损失函数？正则化项？</li><li>优化策略？</li></ul><h2 id="（全连接）神经网路"><a href="#（全连接）神经网路" class="headerlink" title="（全连接）神经网路"></a>（全连接）神经网路</h2><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/all.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><h3 id="数据预处理和参数初始化"><a href="#数据预处理和参数初始化" class="headerlink" title="数据预处理和参数初始化"></a>数据预处理和参数初始化</h3><ul><li>一般对输入数据进行预处理或归一化（例如映射到 $[0,1]$ 范围内），再作为网络输入；</li><li>对网络参数初始化时进行随机初始化（尽量浮动小一些，类似初始学习率也取很小）。</li></ul><h3 id="线性函数（得分函数）"><a href="#线性函数（得分函数）" class="headerlink" title="线性函数（得分函数）"></a>线性函数（得分函数）</h3><p>​        假设分类有10个类别，权重和偏移分别为 $W\in\mathbb{R}^{10\times d},b\in\mathbb{R}^{10},x\in\mathbb{R}^d$，得分函数即为</p><script type="math/tex; mode=display">f(x,W)=W x+b:input \to score,</script><p>相当于每个类别、每个像素点对应不同参数权重。</p><p>​        事实上，这个权重 $W$、偏移 $b$ 是上面网络中层与层之间连线部分计算的线性部分：从某一层输出，经过线性变换之后作为下一层的输入。</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>​        <strong>目的</strong>：如果只是一直的线性变换，那只是简单的线性拟合，无法应对非线性数据；</p><p>​        <strong>方法</strong>：经过上述线性变换后，在传输到下一组神经元之前，进行非线性变换，也即<strong>激活</strong>。事实上这个激活函数就是上面每层网络中的圆圈部分。下面是两个最常用的两种激活函数：</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/sigmod.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>​        用于监督与反向传播，衡量计算当前得分的权重的好坏，例如这里 ：</p><script type="math/tex; mode=display">L_i=\sum_{j\neq y_i}\max(0,s_j-s_{y_i}+1)+\lambda R(W)</script><p>后面 $\lambda R(W)$ 为正则项，是为了削减过拟合。</p><h3 id="softmax-分类器"><a href="#softmax-分类器" class="headerlink" title="$softmax$ 分类器"></a>$softmax$ 分类器</h3><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/softmax.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p>从而计算损失，如图中取的是交叉熵，因为概率越靠近1，损失越小。</p><h3 id="反向传播（可微）"><a href="#反向传播（可微）" class="headerlink" title="反向传播（可微）"></a>反向传播（可微）</h3><p>​        链式法则逐层计算偏导，从网络的输出反向进行到网络的输入，以进行后续的优化，例如<strong>梯度下降法</strong>：</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/grad.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><h3 id="Drop-out"><a href="#Drop-out" class="headerlink" title="Drop-out"></a>Drop-out</h3><p>​        为了削弱过拟合部分，随机在某些训练步内失活一定的神经元：</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/drop-out.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/cnn.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr>    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/cnn-all.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>​        <strong>目的</strong>：增加关联性，考虑到像素与周围像素是有关联性的；</p><p>​        <strong>策略</strong>：利用卷积块作为权重参数对指定大小的像素块做加权组合；注意，每个颜色通道是单独做卷积。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/cnn-1.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/cnn-11.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/cnn-12.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p><strong>边缘填充</strong>，是因为边界在做卷积时，边缘只计算一次，但是内部可能计算多次，为了平衡重要性，在边缘添加一圈0。</p><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>​        <strong>效果</strong>：卷积一定层后，可能发生维度爆炸，利用池化层降低维度。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/cnn-2.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/cnn-21.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><h3 id="特征图变化"><a href="#特征图变化" class="headerlink" title="特征图变化"></a>特征图变化</h3><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/cnn-3.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><h3 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h3><p>​        可以看到，到第二个卷积层后，一个像素块已经<strong>感受</strong>到了周围像素块的关联性。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/feelfield.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/feelfield2.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p>（可跳过）下面的经典网络只是学习时候遇见，罗列在此。</p><h2 id="经典网络"><a href="#经典网络" class="headerlink" title="经典网络"></a>经典网络</h2><h3 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h3><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/vgg.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><h3 id="Resnet-残差网络"><a href="#Resnet-残差网络" class="headerlink" title="Resnet 残差网络"></a>Resnet 残差网络</h3><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/renet.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><h3 id="递归神经网络"><a href="#递归神经网络" class="headerlink" title="递归神经网络"></a>递归神经网络</h3><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/rnn.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/rnn2.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><ul><li><p>保留上一步时间的特征，和下一时间特征一起输入到下一层；</p></li><li><p>一般只选择最后一层输出结果。</p></li></ul><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/lstm.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/lstm2.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/lstm3.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/lstm4.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/lstm5.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table>]]></content>
      
      
      <categories>
          
          <category> Basic Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Basic Knowledge </tag>
            
            <tag> Network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Anaconda基本教程</title>
      <link href="/2022/11/12/conda.html"/>
      <url>/2022/11/12/conda.html</url>
      
        <content type="html"><![CDATA[<p>先简单说一下：<code>Anaconda</code> 是对 <code>Python</code> 环境进行管理的一个工具，我们知道每个 Python 环境中需要一个解释器和一个包集合，但是随着不同项目需要的解释器以及各种包的版本不同，需要多个环境管理，这时就需要 <code>Anaconda</code> 的存在，将多个 Python 环境分隔开管理。</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="Windows安装"><a href="#Windows安装" class="headerlink" title="Windows安装"></a>Windows安装</h2><ol><li><p><a href="https://www.anaconda.com/">Anaconda官网</a> 下载，除以下两步外，其他一切默认即可：</p><ul><li>有一步为 <code>Install for</code>，选择第二项 <code>All Users</code>;</li><li>有一步是 <code>Advanced Options</code>，选择第二项（第一项写着 Not Recommended），我们要自己添加环境变量，不然后面有问题。</li><li>（当然你可以修改安装路径）</li></ul></li><li><p>配置环境变量：<code>此电脑-&gt;属性-&gt;高级系统设置-&gt;环境变量-&gt;系统变量-&gt;Path-&gt;新建</code>，将如下几条文件（当然要是你自己的路径）添加进去：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">E:\Anaconda3</span><br><span class="line">E:\Anaconda3\Scripts</span><br><span class="line">E:\Anaconda3\Library\bin</span><br><span class="line">E:\Anaconda\Library\mingw-w64\bin</span><br><span class="line">E:\Anaconda\Library\usr\bin</span><br></pre></td></tr></table></figure></li><li><p>测试是否安装成功，快捷键 <code>Ctrl+R</code> 输入 <code>cmd</code>，再输入 <code>conda --version</code>，输出版本正常就成功。</p></li></ol><h2 id="Linux安装"><a href="#Linux安装" class="headerlink" title="Linux安装"></a>Linux安装</h2><ol><li><p><a href="https://www.anaconda.com/">Anaconda官网</a> 下载，如果嫌慢，这里提供 <a href="https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/">清华大学开源软件镜像站</a>，下载后进入相应目录即可；当然你也可以用 <code>wget</code> 命令行下载（下面这个2021年的可用）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-x86_64.sh</span><br></pre></td></tr></table></figure></li><li><p><code>bash</code> 运行安装包，比如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash Anaconda3-2021.11-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><p>一路直接回车或者 <code>&quot;yes&quot;</code> 即可，到达 <code>Do you wish the installer to initialize Anaconda3 by running conda init ?</code>，一定选择 <strong>no</strong>！</p></li><li><p>添加环境变量</p><ul><li><p>可以用 <code>vim ~/.bashrc</code> 编辑</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH=&quot;/root/anaconda3/bin:$PATH” #添加你自己的路径即可</span><br></pre></td></tr></table></figure><p>也可以直接用文本编辑器修改，方便一些！</p></li><li><p><code>source</code> 一下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure></li></ul></li><li><p>检查是否安装成功 <code>conda --version</code>。</p></li></ol><h1 id="换源"><a href="#换源" class="headerlink" title="换源"></a>换源</h1><p>解释一下：如果你不换源，那每次安装各种包的时候将非常的慢！（毕竟国内要科学上网）</p><ul><li><p><code>Windows</code> 下路径是 <code>C:\Users\your_name\.condarc</code>，用记事本打开这个文件，添加下面的镜像源（清华源）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/peterjc123/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/</span><br><span class="line">  - defaults</span><br><span class="line">show_channel_urls: true</span><br></pre></td></tr></table></figure><p>我只添加了清华源，不过下面附上其他源：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#中国科大镜像源</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/pkgs/main/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/Anaconda/pkgs/free/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/Anaconda/cloud/msys2/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/Anaconda/cloud/bioconda/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/Anaconda/cloud/menpo/  </span><br></pre></td></tr></table></figure></li><li><p><code>Linux</code> 下路径 <code>~/.condarc</code>，你可以用 <code>vim ~/.condarc</code> 进行编辑，当然最简单的就是用文本编辑器打开，然后添加上述源即可。</p></li><li><p>当然你也可以通过命令行逐一添加，太麻烦了。。。（以其中一个为例）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/Anaconda/pkgs/main/</span><br></pre></td></tr></table></figure></li></ul><h1 id="创建虚拟环境"><a href="#创建虚拟环境" class="headerlink" title="创建虚拟环境"></a>创建虚拟环境</h1><p>如果你没创建任何环境的话，运行下述命令，会显示你已经有了 <code>base</code> 环境，但我们一般在这个环境下安装什么，而是为具体的项目定制具体的环境：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda info --envs</span><br></pre></td></tr></table></figure><p>当然，事不绝对，你要是在 <code>base</code> 环境下安装什么包也是没问题的，没人拦着你。</p><p>下面命令行进行虚拟环境创建：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 创建虚拟环境</span><br><span class="line"># xxx为python解释器版本,一般都是3.7/8/9,出于稳定考虑，不要太新，不然某些包版本不兼容</span><br><span class="line">conda create -n your_env_name python=xxx</span><br><span class="line"></span><br><span class="line"># 安装完成后，进入环境</span><br><span class="line"># 不过建议首次激活用 source activate your_env_name</span><br><span class="line">conda activate your_env_name</span><br><span class="line"></span><br><span class="line"># 如果是windows,有可能就是 activate your_env_name</span><br><span class="line"># Linux下如果不是在默认base环境下，可能需要 source activate your_env_name 才能进入环境</span><br></pre></td></tr></table></figure><h1 id="基本常用命令"><a href="#基本常用命令" class="headerlink" title="基本常用命令"></a>基本常用命令</h1><h2 id="查看已经安装的包"><a href="#查看已经安装的包" class="headerlink" title="查看已经安装的包"></a>查看已经安装的包</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure><h2 id="安装指定包"><a href="#安装指定包" class="headerlink" title="安装指定包"></a>安装指定包</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conda install package_name</span><br><span class="line"></span><br><span class="line"># 当然可以指定包的版本，例如</span><br><span class="line">conda install scrapy==1.3</span><br><span class="line"></span><br><span class="line"># 在conda指定的某个环境中安装包</span><br><span class="line">conda install -n your_env_name package_name</span><br></pre></td></tr></table></figure><p>这里说一下，上述 <code>conda install</code> 如果不成功，可能是由于相应包由 <code>pip</code> 管理</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install package_name</span><br></pre></td></tr></table></figure><h2 id="查看当前有哪些环境"><a href="#查看当前有哪些环境" class="headerlink" title="查看当前有哪些环境"></a>查看当前有哪些环境</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 任选其一皆可</span><br><span class="line">conda env list</span><br><span class="line">conda info -e</span><br><span class="line">conda info --envs</span><br></pre></td></tr></table></figure><h2 id="删除某一环境的某一包"><a href="#删除某一环境的某一包" class="headerlink" title="删除某一环境的某一包"></a>删除某一环境的某一包</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">onda remove --name your_env_name package_name</span><br></pre></td></tr></table></figure><h2 id="删除某一环境"><a href="#删除某一环境" class="headerlink" title="删除某一环境"></a>删除某一环境</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda remove -n your_env_name --all</span><br></pre></td></tr></table></figure><h2 id="检查更新conda"><a href="#检查更新conda" class="headerlink" title="检查更新conda"></a>检查更新conda</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 检查更新当前conda</span><br><span class="line">conda update conda</span><br><span class="line"></span><br><span class="line"># 更新anaconda</span><br><span class="line">conda update anaconda</span><br></pre></td></tr></table></figure><h2 id="更新包"><a href="#更新包" class="headerlink" title="更新包"></a>更新包</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 更新所有库，轻易不要用这个，不然不就丧失了conda多版本管理的意义</span><br><span class="line">conda update --all</span><br><span class="line"></span><br><span class="line"># 更新指定包</span><br><span class="line">conda update package_name</span><br><span class="line"></span><br><span class="line"># pip更新</span><br><span class="line">pip install --upgrade package_name</span><br><span class="line"></span><br><span class="line"># 也可以更新python</span><br><span class="line">conda update python</span><br></pre></td></tr></table></figure><h2 id="导出环境"><a href="#导出环境" class="headerlink" title="导出环境"></a>导出环境</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 导出当前环境的包信息，当然路径就在你所在的目录</span><br><span class="line">conda env export &gt; environment.yaml</span><br></pre></td></tr></table></figure><h2 id="用配置文件创建环境"><a href="#用配置文件创建环境" class="headerlink" title="用配置文件创建环境"></a>用配置文件创建环境</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda env create -f environment.yaml</span><br></pre></td></tr></table></figure><h2 id="安装requirements-txt依赖"><a href="#安装requirements-txt依赖" class="headerlink" title="安装requirements.txt依赖"></a>安装requirements.txt依赖</h2><p>解释：一般 <code>requirements.txt</code> 是项目作者将环境提取出来，用于他人复刻</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt # 当然要求你运行这个命令行所在目录有这个文件</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Code </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Anaconda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git教程</title>
      <link href="/2022/11/11/git.html"/>
      <url>/2022/11/11/git.html</url>
      
        <content type="html"><![CDATA[<h1 id="Git安装"><a href="#Git安装" class="headerlink" title="Git安装"></a>Git安装</h1><p>先注册 <code>Github</code> 主页，在学习使用之前，注意：<strong>如果你涉及git只是简单的保存代码且文件大小&lt;25M，github的图形界面完全可以搞定！没必要从头一点一点学命令行。</strong></p><p>然后官网下载<a href="https://git-scm.com/">Git (git-scm.com)</a>，一切默认选项，不要乱改！</p><h1 id="Git最基本使用"><a href="#Git最基本使用" class="headerlink" title="Git最基本使用"></a>Git最基本使用</h1><h2 id="配置并生成密钥"><a href="#配置并生成密钥" class="headerlink" title="配置并生成密钥"></a>配置并生成密钥</h2><p>（这是为了你能 <code>clone</code> 以及推送远程仓库做准备）</p><p>（下述命令行都是在指定文件夹下用 <code>Git Bash</code> 输入并运行）</p><ol><li><p>用如下命令检查一下用户名和邮箱是否配置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global  --list</span><br></pre></td></tr></table></figure></li><li><p>配置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global  user.name 用户名</span><br><span class="line">git config --global user.email 邮箱</span><br></pre></td></tr></table></figure></li><li><p>生成秘钥：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C 邮箱</span><br></pre></td></tr></table></figure><p>执行命令后需要进行3次或4次确认：</p><ul><li>(1) 确认秘钥的保存路径（如果不需要改路径则直接回车）；</li><li>(2) 如果上一步置顶的保存路径下已经有秘钥文件，则需要确认是否覆盖（如果之前的秘钥不再需要则直接回车覆盖，如需要则手动拷贝到其他目录后再覆盖）；</li><li>(3) 创建密码（如果不需要密码则直接回车）；</li><li>(4) 确认密码；</li><li>在指定的保存路径（一般是 <code>C:/users/用户名/.ssh</code>）下会生成2个名为 <code>id_rsa</code> 和 <code>id_rsa.pub</code> 的文件；</li></ul></li><li><p>网页打开 Github，进入 <code>Settings -&gt; SSH and GPG keys -&gt; New SSH key</code>；</p></li><li><p>然后用文本工具（记事本之类的）打开之前生成的 <code>id_rsa.pub</code> 文件，把内容拷贝到 <code>key</code> 下面的输入框，并为这个 <code>key</code> 定义一个名称（通常用来区分不同主机），然后保存即可。</p></li></ol><h2 id="创建本地仓库"><a href="#创建本地仓库" class="headerlink" title="创建本地仓库"></a>创建本地仓库</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir sth  #创建文件夹,sth即为文件夹名称,建议英文</span><br><span class="line">cd sth     #进入文件夹</span><br><span class="line">git init   #初始化为git仓库,此时文件夹下会创建.ssh隐藏文件夹</span><br></pre></td></tr></table></figure><p>可以将指定文件添加到这个仓库内并记录每次更改</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git add any_file #将指定文件添加到Git仓库管理,any_file即为文件名字,例如 temp.txt</span><br><span class="line">git commit -m &quot;description&quot; #-m后面&quot;&quot;内的描述是本次提交的说明(英文),用这个简要说明帮你记录每次提交的原因和目的</span><br></pre></td></tr></table></figure><p>也可以多次提交并记录说明</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git add any_file_1.txt</span><br><span class="line">git add any_file_2.txt</span><br><span class="line">git commit -m &quot;description&quot;</span><br></pre></td></tr></table></figure><p>或者本文件夹下内的文件全部提交</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br></pre></td></tr></table></figure><p>当然你也可以一直用下面这个命令查看仓库状态：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git status</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：如果你只是把文件放在 <code>sth</code> 文件夹下，<code>Git</code> 仓库不知道你放进去了，就没办法进行版本管理（回退等），只有你进行提交之后才能进行记录与管理，只有提交的文件才能被推送到指定远程仓库。</p><h2 id="添加远程仓库"><a href="#添加远程仓库" class="headerlink" title="添加远程仓库"></a>添加远程仓库</h2><ol><li><p>在自己的 Github 网页版建一个远程仓库：点击 <code>New repository</code>，起好名字之后，你可以选择该仓库是否公开 <code>Public</code> 或私有 <code>Private</code>，也可以选择是否为这个仓库添加一个说明问价 <code>Add a README file</code>，当然也可以什么都不点击就直接 <code>Create repository</code>；</p></li><li><p>在你想要连接的本地仓库内（文件夹下），添加这个仓库远程地址：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin git@github.com:your_github_name/repository_name.git </span><br><span class="line"># 或者下面这种添加也可以</span><br><span class="line">git remote add origin https://github.com/your_github_name/repository_name</span><br></pre></td></tr></table></figure><p>可以调用命令行看看当前文件夹或仓库的远程库信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git remote -v</span><br><span class="line"># 输出可能如下，或没链接远程仓库</span><br><span class="line">origin git@github.com:xxxxxxx.git (fetch)</span><br></pre></td></tr></table></figure><p>也可以删除远程仓库（比如添加的时候写错地址等），根据名字删除 <code>origin</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote rm origin</span><br></pre></td></tr></table></figure></li><li><p>仍在本地仓库内（文件夹下），推送到远程仓库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">git add .                    #如果你之前已经添加，可以不进行此步</span><br><span class="line">git commit -m &quot;description&quot;</span><br><span class="line"></span><br><span class="line">#由于你新建的远程仓库是空的，所以有-u这个参数</span><br><span class="line">git push -u origin main      #推送到main分支，日后你可以创建多个分支管理</span><br><span class="line"></span><br><span class="line">#日后就可以取消-u参数来进行推送</span><br><span class="line">git push origin main</span><br></pre></td></tr></table></figure></li></ol><h2 id="拉取远程仓库"><a href="#拉取远程仓库" class="headerlink" title="拉取远程仓库"></a>拉取远程仓库</h2><ul><li><p>一般的，你可以直接运行下述命令，将连接的远程仓库直接拉取下来：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin main</span><br></pre></td></tr></table></figure><p><strong>但是注意</strong>：这会直接覆盖掉你本地仓库，当然你可以新建一个本地空仓库，链接到远程这个仓库，在拉取也行！</p></li><li><p>可以换一种方法：先将本地代码放到暂存区，拉取之后再将暂存区代码放回本地</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git stash save &quot;description&quot;  #放到暂缓区</span><br><span class="line">git pull                      #拉取</span><br><span class="line">git stash pop                 #将暂缓区放回本地</span><br></pre></td></tr></table></figure><ul><li><strong>但是</strong>：事实上这是为了解决这个问题存在的：甲乙两个程序员正在合作开发一款程序，甲晚上10点push了自己编辑的 test.txt 文档到GitHub；乙11点也想要 push 自己编辑过后的 test.txt 文档到 GitHub，因为乙本地仓库不是最新，乙push时会报错，乙如果使用 git pull 拉取最新版本，则自己修改的东就会丢失！</li><li>所以解决思路是：先把自己修改好的代码存放在缓存里，等最新代码拉取下来以后再恢复缓存里的自己修改的代码，乙再push，前提是两人不要修改相同的地方，可能会产生冲突！</li></ul></li></ul><h2 id="拉取远程仓库指定单个文件夹"><a href="#拉取远程仓库指定单个文件夹" class="headerlink" title="拉取远程仓库指定单个文件夹"></a>拉取远程仓库指定单个文件夹</h2><p>现在有一个 <code>test</code> 仓库<a href="https://github.com/mygithub/test，你要">https://github.com/mygithub/test，你要</a> <code>gitclone</code> 里面的 <code>tt</code> 子目录。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git config core.sparsecheckout true                     #设置允许克隆子目录</span><br><span class="line">echo &#x27;tt*&#x27; &gt;&gt; .git/info/sparse-checkout                 #设置要克隆的仓库的子目录路径 !空格别漏!</span><br><span class="line">git remote add origin git@github.com:mygithub/test.git  #这里换成你要克隆的项目和库，就是将你想拉取的远程仓库添加</span><br><span class="line">git pull origin master                                  #下载</span><br></pre></td></tr></table></figure><h2 id="克隆仓库"><a href="#克隆仓库" class="headerlink" title="克隆仓库"></a>克隆仓库</h2><p>除了上述必须添加远程仓库的克隆方法，最简单的可哦那个仓库方法应该是</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/mygithub/test.git #其实候直接复制仓库链接没有.git也可以</span><br></pre></td></tr></table></figure><h2 id="指定分支上克隆代码"><a href="#指定分支上克隆代码" class="headerlink" title="指定分支上克隆代码"></a>指定分支上克隆代码</h2><p><code>git clone 地址</code> 默认从 <code>main/master</code> 分支上克隆，如下展示从指定分支上克隆代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone -b 分支名称 地址</span><br></pre></td></tr></table></figure><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/git_1.png"                        width="542" height="309"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p>示例：<code>git clone -b v2.8.1 https://git.oschina.net/oschina/android-app.git</code></p><h2 id="子模块下载or更新"><a href="#子模块下载or更新" class="headerlink" title="子模块下载or更新"></a>子模块下载or更新</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone --recursive 地址</span><br></pre></td></tr></table></figure><p>下面这个命令是在已经克隆下这个库后，开始下载子模块；当然如果你不确定子模块是否都已经下载完全了，也可以执行一遍下面这个命令确认一下。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git submodule update --init --recursive</span><br></pre></td></tr></table></figure><p>更新子模块</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git submodule sync --recursive</span><br><span class="line">git submodule update --init --recursive</span><br></pre></td></tr></table></figure><h1 id="上传大文件（-gt-100M）"><a href="#上传大文件（-gt-100M）" class="headerlink" title="上传大文件（&gt;100M）"></a>上传大文件（&gt;100M）</h1><p>话说在前头，<strong>真不建议用 Github 储存这么大的文件！很费事！三思！</strong></p><ol><li><p>安装 <a href="https://git-lfs.github.com/">Git Large File Storage(LFS)</a> 工具，官网直接下载安装即可；</p></li><li><p>安装后把里面的 <code>git-lfs.exe</code> 放到你要上传的项目/仓库文件夹（最好是新建的文件夹仓库）；</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure></li><li><p><strong>如果你是在push完后被提示需要用LFS文件的话，还需要回退版本，这步非常重要</strong>；如果你还没有push过大文件，可以忽略这个步骤。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git log</span><br><span class="line"># 查看版本，选择你最后成功push的一次版本</span><br><span class="line">git reset commit号</span><br></pre></td></tr></table></figure></li><li><p>对于本地 git 仓库下，安装 <code>lfs</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git lfs install</span><br></pre></td></tr></table></figure></li><li><p>上传文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">git lfs track *        #追踪要上传的大文件，*表示路径下的所有文件</span><br><span class="line">git add .gitattributes #添加先上传的属性文件(要先上传属性文件，不然有可能失败)</span><br><span class="line">git commit -m &quot;pre&quot;    #添加属性文件上传的说明</span><br><span class="line">git remote add origin https://github.com/name/repo.git #建立本地和Github仓库的链接</span><br><span class="line">git push origin master #上传属性文件</span><br><span class="line">git add *              #添加要上传的大文件，*表示路径下的所有文件</span><br><span class="line">git commit -m &quot;Git LFS commit&quot; #添加大文件上传的说明</span><br><span class="line">git push origin master         #上传大文件</span><br></pre></td></tr></table></figure></li></ol><h1 id="常见问题及方法"><a href="#常见问题及方法" class="headerlink" title="常见问题及方法"></a>常见问题及方法</h1><h2 id="error-10054"><a href="#error-10054" class="headerlink" title="error 10054"></a>error 10054</h2><p><code>fatal: unable to access ‘https://github.com/…’: OpenSSL SSL_read: Connection was reset, errno 10054</code>产生原因：一般是这是因为服务器的SSL证书没有经过第三方机构的签署，所以才报错。参考网上解决办法：解除ssl验证后，再次git即可</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global http.sslVerify false</span><br></pre></td></tr></table></figure><h2 id="推送或克隆超时（设置、取消代理）"><a href="#推送或克隆超时（设置、取消代理）" class="headerlink" title="推送或克隆超时（设置、取消代理）"></a>推送或克隆超时（设置、取消代理）</h2><p>设置代理：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global https.proxy</span><br></pre></td></tr></table></figure><p>取消代理：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global --unset https.proxy</span><br></pre></td></tr></table></figure><p>网上有人建议，先设置代理，再取消代理，然后再克隆。</p><h2 id="GIT推送错误error-RPC-failed；-curl-92-HTTP-2-stream-0-was-not-closed-cleanly-CANCEL-err-8"><a href="#GIT推送错误error-RPC-failed；-curl-92-HTTP-2-stream-0-was-not-closed-cleanly-CANCEL-err-8" class="headerlink" title="GIT推送错误error: RPC failed； curl 92 HTTP/2 stream 0 was not closed cleanly: CANCEL (err 8)"></a>GIT推送错误error: RPC failed； curl 92 HTTP/2 stream 0 was not closed cleanly: CANCEL (err 8)</h2><p>类似状况如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Counting objects: 100% (25515/25515), done.</span><br><span class="line">Delta compression using up to 4 threads</span><br><span class="line">Compressing objects: 100% (18794/18794), done.</span><br><span class="line">error: RPC failed; curl 92 HTTP/2 stream 0 was not closed cleanly: CANCEL (err 8)</span><br><span class="line">fatal: the remote end hung up unexpectedlyiB | 19.00 KiB/s</span><br><span class="line">Writing objects: 100% (25503/25503), 28.46 MiB | 298.00 KiB/s, done.</span><br><span class="line">Total 25503 (delta 5409), reused 25463 (delta 5393), pack-reused 0</span><br><span class="line">fatal: the remote end hung up unexpectedly</span><br><span class="line">Everything up-to-date</span><br></pre></td></tr></table></figure><p>错误原因：error:RPC failed; curl 92 HTTP/2 stream 0 was not closed cleanly: CANCEL (err 8)<br>错误：RPC失败;HTTP/2 stream 0没有完全关闭:CANCEL (err 8)</p><p>解决方法：<br>方法一：将git 远程地址改为自己的ssh地址</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote set-url origin git@github.com:github用户名/仓库名.git</span><br></pre></td></tr></table></figure><p>方法二：增加git缓冲区大小</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global http.postBuffer 524288000</span><br></pre></td></tr></table></figure><h2 id="无法推送到某分支"><a href="#无法推送到某分支" class="headerlink" title="无法推送到某分支"></a>无法推送到某分支</h2><p>问题</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git push -u origin main</span><br><span class="line">error: src refspec main does not match any</span><br><span class="line">error: failed to push some refs to &#x27;https:</span><br></pre></td></tr></table></figure><p>原因是没发现main分支,切换到main分支,再次测试即可</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout main</span><br><span class="line">Switched to branch &#x27;main&#x27;</span><br><span class="line">Your branch is up to date with &#x27;origin/main&#x27;</span><br></pre></td></tr></table></figure><p>或者可以用git branch查看有哪些分支</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 查看本地分支</span><br><span class="line">$ git branch</span><br><span class="line">* main</span><br><span class="line">  master</span><br><span class="line"></span><br><span class="line"># 查看远程分支</span><br><span class="line">$ git branch -r</span><br><span class="line">  origin/main</span><br><span class="line">  origin/master</span><br><span class="line">  </span><br><span class="line"># 查看所有分支,包括本地和远程</span><br><span class="line">$ git branch -a</span><br><span class="line">* main</span><br><span class="line">  master</span><br><span class="line">  remotes/origin/main</span><br><span class="line">  remotes/origin/master</span><br></pre></td></tr></table></figure><p>总结：无法推送到某分支,先确认是否有该分支</p><h2 id="查看仓库信息"><a href="#查看仓库信息" class="headerlink" title="查看仓库信息"></a>查看仓库信息</h2><p>config 配置有system级别 global（用户级别） 和local（当前仓库）三个 设置先从system-&gt;global-&gt;local 底层配置会覆盖顶层配置，分别使用 <code>--system/global/local</code> 可以定位到配置文件：</p><p>查看系统config：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --system --list</span><br></pre></td></tr></table></figure><p>查看当前用户（global）配置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global --list</span><br></pre></td></tr></table></figure><p>查看当前仓库配置信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --local --list</span><br></pre></td></tr></table></figure><h2 id="查看、修改用户名和邮箱"><a href="#查看、修改用户名和邮箱" class="headerlink" title="查看、修改用户名和邮箱"></a>查看、修改用户名和邮箱</h2><p>查看用户名和邮箱地址：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config user.name</span><br><span class="line">git config user.email</span><br></pre></td></tr></table></figure><p>修改用户名和邮箱地址：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;username&quot;</span><br><span class="line">git config --global user.email &quot;email&quot;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Code </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Jagten》（狩猎）</title>
      <link href="/2022/11/05/movie/jagten.html"/>
      <url>/2022/11/05/movie/jagten.html</url>
      
        <content type="html"><![CDATA[<h1 id="Paradise的电影乐园-剧情"><a href="#Paradise的电影乐园-剧情" class="headerlink" title="#Paradise的电影乐园# 剧情"></a>#Paradise的电影乐园# 剧情</h1><p>导演：Thomas Vinterberg<br>主演：Mads Mikkelsen<br>上映时间：2012年05月20日</p><p><strong>“</strong><br><strong>看着我！</strong><br><strong>看着我的眼睛，</strong><br><strong>看看我的眼睛里有什么？</strong><br><strong>你看见了什么？</strong><br><strong>什么都没有，</strong><br><strong>一无所有。</strong><br><strong>”</strong></p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/movie/hunt_6.jpg"                        width="432" height="617.2"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p>这是一部，说是寒冷彻骨都不为过的丹麦电影。</p><p>就像每年都要步入的寒冬一样，会在仍处于颤栗中酝酿出近乎绝望的压抑与狂躁，毫不留情且毫无防备的在人性与社会道德上划出深入刻骨的一刀，而且是每年都要划上一般。</p><p>私以为，这更是一部当今互联网人基本上都应该好好看的电影。</p><p>初看这部电影的人，可能处在对其中的孩子“谎话连篇”的愤懑之中，但这绝不是电影的全部。孩子是特殊的弱势群体，在生理和心理都不成熟的阶段，尤其是原生家庭的不作为与养育“只养无育”之下，错误的表达所谓的情感与见解，能一味的归咎于成年人中的“恶”吗？恐怖最大的恶是将其从天使逼入魔鬼的人。</p><p>最可恶的并不是孩子，而是<strong>那群虚伪的，带有偏见的，先入为主的，自封为卫道士的傲慢的大人们。</strong></p><p>卢卡斯因为几句儿童的呓语便毁灭了本归是向好的人生，毁灭过程中最大的破坏力并不是直接由孩子带来的，而是由所谓打着“正义感”旗号的群体带来的，而是由社会群体的排斥与异化所带来的。人是群居动物，这种孤立于排斥的隔阂异化，所带来的伤害无疑是巨大到不可估量的，足以毁灭一个人的所有社会属性了，更何况，这种排斥和针对性的有意异化是冤屈的、是无法辩驳言明的。</p><p><strong>眼见，都并非为实，更何况道听途说？</strong></p><p><strong>在蒙蔽了双眼下驱使出来的“正义感”，是盲目的，是狗屎，是吃人的恶！</strong></p><p><strong>一种被扭曲的思想一旦植入大脑，错，就会无止境的继续下去。</strong>人往往是最不理智的高智慧生物，很可笑的矛盾点，最不理智与智慧。大部分人一旦相信自己是正义的，便会自动带上了有色眼镜，挥起了正义的鞭笞，认为所看到的一切都是支持自己想法的证据，与之相悖的线索全部被刻意忽略或者曲解，践行正义之道、执行正义审判，殊不知挥下的鞭笞是犹如杀人的死神镰刀。不过这就是事实，盲目的正义者们不会关心真相究竟是什么。因为抢站在道德的制高点批判是很容易的一件事，而自己本身的言行会对他人造成怎样的影响却很难预料，甚至不去顾及。人们害怕偏见，害怕排斥，害怕异化，但更多的时候是亲自促成了偏见，亲自执刀而行。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/movie/hunt_4.png"                        width="432" height="540"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p>这部影片的张力是相当足够的，从开头小镇的和睦祥和到群起的“正义”排斥，导演就是在不温不火的叙事和理智暖色的镜头之中酝酿了所有的排斥，即使是最温暖午后阳光，背后掩藏的都是开枪的“正义之士”。这种张力就是建立在这种排斥和冤屈的对立上，然后精准到位的把这种恶毒的人际对立和人物内心的崩塌完美的展现了出来。作为观影的我们来说，带入故事之后，那种慢慢酝酿而来的恶意和仇恨强烈到近乎可以演化为暴戾之气，不过，那种积郁已久的愤怒迟迟无法爆炸，最后沦为彻骨的寒意。没办法，这就是现实。毫不掩饰的说，第一次观影的我，在众多积怨之下，为男主默泪。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/movie/hunt_5.jpg"                        width="648" height="324"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p>影片最让人印象深刻的便是在教堂那一幕。卢卡斯非常讽刺来到教堂这个众人乞求救赎的地方，或许在这里所有人顶着上帝普世之爱的光环，是唯一能让他这个“罪人”和一社区“善人”平静共处一室的地方，孩子们用天使般的声音开始唱圣诞颂歌时，这种恶与纯真的巨大反差让男主角的理智处于崩溃边缘，但是终究理智还是没有崩溃。但是当卢卡斯望向身后曾是挚友的人，指控自己的一家低头私语时，他崩溃了，即使事实是挚友一家私语并不是谈论卢卡斯，但是那个动作如烈火灼烧之痛，他崩溃了，愤怒的起身打了挚友几拳，绝望的拉着挚友怒吼到：“看着我！看着我的眼睛！看看我的眼睛里有什么？你看见了什么？什么都没有了，一无所有。”对，没有了，被扼杀完了已经，一无所剩。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/movie/hunt_3.png"                        width="432" height="288"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p>《狩猎》这个片名也是有一定意味存在的，在最后一幕，卢卡斯就是社会群体森林中那只被追猎的麋鹿，毫无防备，甚至心怀感激的袒露在黑暗森林之中，无数躲在暗中的猎人可以中伤他，而最后那惊人心魄的一枪，足够彰显自封社会傲慢卫道士最大的恶意了，引用这部电影最具知名度的影评：</p><p><strong>“</strong></p><p><strong>枪响了。</strong></p><p><strong>你看到是谁开的枪吗？</strong></p><p><strong>我看不清，</strong></p><p><strong>他站在道德制高点上，他在阳光下。</strong></p><p><strong>”</strong></p><p><strong>他们看似伸张“正义”，却不以真凭实据为基础；</strong></p><p><strong>他们看似维持“公正”，却不以实事求是为准则；</strong></p><p><strong>他们看似通情达理，却在种种行为举止中透露出令人作呕的气息。</strong></p><p><strong>这是最让人心寒的一部分，</strong></p><p><strong>这是最让人愤怒的一部分，</strong></p><p><strong>这是最让人无奈的一部分，</strong></p><p><strong>这同样也是最尖酸讽刺的一部分。</strong></p><p>最后的题外话，不会放在这个平台说的，这种莫须有的道德审判，从古至今已经不少了。红如阮玲玉要自杀，杰克逊也要沉沦。而到了网络时代，这样的案例简直数不胜数：70码的时候，所有人都相信那个受审的不是吴斌是个替身，哪怕警方拿出所有证据；湖北29岁最年轻市长，周森锋必然上面有人；方舟子只要坚持不懈地骂“韩骗子”他就是个抄袭犯代笔作家，出版手稿也无法自证。反正无休止的怀疑、质疑、有罪推定，成本由被怀疑对象承担，收益都是自己的。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/movie/hunt_2.jpg"                        width="637.8" height="425.2"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table>]]></content>
      
      
      <categories>
          
          <category> Movie </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paradise的电影乐园 </tag>
            
            <tag> Paradise的电影乐园 剧情 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Metropolis》(大都会)</title>
      <link href="/2022/11/04/movie/metropolis.html"/>
      <url>/2022/11/04/movie/metropolis.html</url>
      
        <content type="html"><![CDATA[<h1 id="Paradise的电影乐园-科幻"><a href="#Paradise的电影乐园-科幻" class="headerlink" title="#Paradise的电影乐园# 科幻"></a>#Paradise的电影乐园# 科幻</h1><p>导演：Friedrich Christian Anton Lang<br>主演：Alfred Abel，Brigitte Helm，Gustav Fröhlich<br>上映时间：1927年01月10日</p><p>  </p><p><strong>“</strong></p><p><strong>Sinnspruch：</strong></p><p><strong>MITILER ZWISCHEN HIRN UND HÄNDEN MUSS DAS HERZ SEIN!</strong></p><p><strong>题记：大脑和双手之间的协调者必定是心！</strong></p><p><strong>”</strong></p><p><strong>“</strong></p><p><strong>„Tausende werden morgen in Wut und Versweiflung fragen:</strong><br><strong>Joh Fredersen,</strong><br><strong>wo ist mein Sohn-!</strong></p><p><strong>明天，上千人会在愤怒和绝望中向你质问：</strong></p><p><strong>约·弗雷德森，我的孩子在哪——！</strong></p><p><strong>”</strong></p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/movie/metropolis_2.jpg"                        width="395.82" height="291.6"/>            </center>        </td>   <!--<center>标签将图片居中-->        <td>            <center>            <img src="/image/movie/metropolis_3.jpg"                        width="395.82" height="291.6"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p>作为德国电影默片时代的三大扛鼎巨作之一（另外两部：《浮士德》1926年，《柏林：大城市的交响乐》1927年）  </p><p>科幻巨制、表现主义电影《大都会》是默片时代皇冠上璀璨的明珠，是联合国教科文组织编撰的《世界的记忆》中收录的第一部电影。  </p><p>《大都会》在科幻影史上的地位一言以蔽之：<strong>后世任何一部有关想象未来城市的电影，都很难说自己的作品走出了它的影子，可以说后世的电影人不断在弗里茨·郎打造的大都会框架下创造属于自己的未来之城。</strong>例如，《第五元素》里在摩天大厦之间穿针引线的城市交通轨道；《银翼杀手》里永远在冰冷的夜里不见白日的城市；另外，在如今我们最常用的视频通话发明之前，不少科幻电影也已经将《大都会》中的可视电话当成了科技设定的标配；而星球大战系列的机器人C-3PO和MV里带金属胸罩的麦当娜造型雷同，那就是因为二者都借鉴了罗特旺的人造机器人形象。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/movie/metropolis_4.gif"                        width="432" height="324"/>            </center>        </td>   <!--<center>标签将图片居中-->        <td>            <center>            <img src="/image/movie/metropolis_5.gif"                        width="432" height="324"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p>提到这部电影不得不提的就是，体现导演弗里茨·郎最初创作意志的原版《大都会》，已然成为了一个失落的传说。在最初发行公司审查原片时，认为故事复杂、片长过长，动手剪辑的大约四分之一的胶片；即使后来德国因为票房惨淡，参照美国上映版本重剪辑，也是不完整且丧失众多内容的版本；直到上世纪末，人们一直以为收入联合国世界文化遗产的2001年茂瑙基金会版本，就是当今现存胶片的最极致版本了，然而在2008年，阿根廷的一些电影爱好者发现了布宜诺斯艾利斯带你应博物馆馆藏的版本长达两个小时，震惊众人，基金会着手修复增添了许多原先版本没有的精彩画面，但也不可避免地在修复版本胶片上留存了许多灰尘和刮痕，甚至一些场景因为孙惠燕汇总，不得不以字幕的表现形式串联起整个故事情节（比如弗雷德森和罗特旺的打斗戏），但总归是沧海遗珠般珍贵与庆幸！</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/movie/metropolis_6.jpg"                        width="420" height="314.3"/>            </center>        </td>   <!--<center>标签将图片居中-->        <td>            <center>            <img src="/image/movie/metropolis_8.jpg"                        width="449.6" height="314.3"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p>与空中花园那样和谐美好的乌托邦（Utopia）正相反，反乌托邦讲的是在未来社会中，滥用与过载的技术如何奴役底层人民，并导致被压迫人民物质世界贫瘠、精神家园沦丧。  </p><p>导演弗里茨·郎设定的反乌托邦背景是一座于2000年的机器大都市，这个大都市是由以约·弗雷德森为首的资本家所统治，以此城市被认为地分为了两个世界：居于地面之上鳞次栉比地摩天大厦里、每日只有灯红酒绿、娱人娱己的资本家，作为城市中的“头脑”的象征，指挥着城市运转的机器系统；而屈居于地下永无天日、麻木无果、终日劳作维持机器运转的工人们，是城市中愤懑与彷徨“手”，直到过劳死的那一刻仍不停的辛劳。那“协调者——心”，弗雷德，起初在隔绝一切人间疾苦的”永恒花园“里与众人浪荡嬉戏、不谙世事，在一次工人圣女玛利亚误入此地并与其偶遇之后，一路追随她深入到工人所在地下城之内，体会到了工人们的水深火热之后，向其父亲控诉无果，决定充当”协调者“改变现状。而弗雷德森也由此发现了圣女玛利亚集结工人们演讲布道，决定让科学家罗特旺将半成品机器人制作成圣女玛利亚的模样，以此蛊惑工人暴动，以便镇压；但罗特旺出于对费雷德森的私愤与仇视，表面假装同意，实则指挥机器人煽动整个工人阶层毁灭整个大都会的存在。工人们在被恶意挑起的盛怒之下，摧毁了大都会赖以生存的核心中央机器，致使洪水的侵袭。  </p><p>在展示科技对人的桎梏以及随之而来的社会阶层大分化之后，《大都会》的重心不在于置疑和颠覆，而是调和。  </p><p>技术奇观巨人般的躯壳套着孩童般天真的灵魂，比比谁更幼稚，是轻易挫败魔鬼阴谋的小情小爱，还是初尝人间疾苦就能抹平统治者与被统治者之间鸿沟的公子哥？  </p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/movie/metropolis_9.gif"                        width="432" height="324"/>            </center>        </td>   <!--<center>标签将图片居中-->        <td>            <center>            <img src="/image/movie/metropolis_7.jpg"                        width="432.53" height="324"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p>这是一部”基督教寓言“式的科幻电影，其相关设定都可以一一对标《圣经》著作的内容：</p><p>资本家约·弗雷德森——大都会的主宰，耶和华</p><p>玛丽亚——圣母玛丽亚/先知</p><p>工头——先知/使徒</p><p>资本家的儿子弗雷茨——耶稣基督</p><p>巴别塔——天国的建立</p><p>发明家罗特旺——魔鬼撒旦</p><p>机器玛丽亚——敌基督</p><p>这种“基督教寓言”形式的科幻电影模式，也成为了科幻电影拍摄的一种可供借鉴的前例。从某种意义上来说，90年代拍摄的《骇客帝国》，正是这一表现手法的更精细运用。  </p><p>  </p><p>作为一战战败国，那是战后的颓败中迸发出的野蛮生命力，亦是在大发展中困于内在冲突的探讨；</p><p>科幻色彩下的宗教媒介，让你在光怪陆离的影像与行为中感受阶级矛盾的存在、激化、崩塌、协调；</p><p>纵使你是正义的一方，切勿堕入群体无意识的愚昧性，只是言语刺激下就沦为别人的枪口与子弹。</p>]]></content>
      
      
      <categories>
          
          <category> Movie </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paradise的电影乐园 </tag>
            
            <tag> Paradise的电影乐园 科幻 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Becoming Jane》(成为简·奥斯汀)</title>
      <link href="/2022/11/03/movie/becoming-jane.html"/>
      <url>/2022/11/03/movie/becoming-jane.html</url>
      
        <content type="html"><![CDATA[<h1 id="Paradise的电影乐园-传记-爱情"><a href="#Paradise的电影乐园-传记-爱情" class="headerlink" title="#Paradise的电影乐园# 传记 爱情"></a>#Paradise的电影乐园# 传记 爱情</h1><p>导演：Julian Jarrold<br>主演：Anne Hathaway，James McAvoy<br>上映时间：2007年03月09日</p><p><strong>“</strong><br><strong>Sometimes affection is a shy flower that takes time to blossom.</strong><br><strong>”</strong><br><strong>“</strong><br><strong>Affection is desirable,</strong><br><strong>Money is absolutely indispensable.</strong><br><strong>”</strong>  </p><p>她的心被扰乱了，只是一阵 夏日的狂风。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/movie/becoming_jane_3.jpg"                        width="511.2" height="556"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p>在那个英式典雅的时代，</p><p>一个安静的清晨，仿佛古典戏剧一般的开始，一切充满着田园牧歌与优雅的浪漫，</p><p>随着简的钢琴声的响起，群鸟惊飞，众人惊起，从一开始就奠定了简与保守英国乡村生活的格格不入。  </p><p>末了，已是作家的简偶然瞥见了前来观摩自己作品的汤姆，如今却已身为人父，</p><p>当女儿要求简为大家朗读作品时，汤姆严厉的呵斥：“简！” </p><p>错愕写满了简的脸上，</p><p>这是一个无法抵御的桥段，</p><p>本以为一切走向了终结，本以为终是尘埃落定，</p><p>谁料却是一直绵延交织在两人的余生——汤姆以简的名字为女儿命名，而简所写的故事也藏着他们的影子。</p><p>爱，带着观望般的珍重，永远不会厌倦和衰灭，融漾在淡淡的牵念和释然的遗憾之间，绵长一生，徘徊一生……   </p><p></p><p>世俗阻挡的爱情，古往今来的文人骚客写出了太多太多，或壮烈决绝，或凄美哀婉。</p><p>不是每个人都能如罗密欧那般欧式的决绝，如祝英台般中式的缠绵，</p><p>在此，更多的是一种生活的平静，无论是抗争还是妥协。   </p><p></p><p><strong>并不是只有激情迸发时弃世孤行的爱情才最真挚可贵，</strong></p><p><strong>若把世界逼堵到只容两个人立足，</strong></p><p><strong>结局也许早已违背当时的初衷。</strong></p><p><strong>若有天长地久的厮守，也必有终身不忘的情长。</strong></p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/movie/becoming_jane_2.jpg"                        width="840" height="560.7"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table>]]></content>
      
      
      <categories>
          
          <category> Movie </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paradise的电影乐园 爱情 </tag>
            
            <tag> Paradise的电影乐园 </tag>
            
            <tag> Paradise的电影乐园 传记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《L&#39;Argent》(钱)</title>
      <link href="/2022/11/02/movie/largent.html"/>
      <url>/2022/11/02/movie/largent.html</url>
      
        <content type="html"><![CDATA[<h1 id="Paradise的电影乐园-剧情"><a href="#Paradise的电影乐园-剧情" class="headerlink" title="#Paradise的电影乐园# 剧情"></a>#Paradise的电影乐园# 剧情</h1><p>导演：Robert Bresson<br>主演：Christian Patey<br>上映时间：1983年  </p><p><strong>“</strong><br><strong>当你意识到自己的存在和社会的荒谬，</strong><br><strong>而你别无选择时，</strong><br><strong>你会怎么办？</strong><br><strong>他们说</strong><br> <strong>‘ 服从吧，只要你别多管闲事；</strong><br><strong>等着吧，世界很快就会幸福起来。’</strong><br><strong>但我不想等到全世界都幸福，</strong><br><strong>相信我，伊文，那只会等到你变傻，</strong><br><strong>我想现在就得到幸福，用我自己的方法。</strong><br><strong>钱！</strong><br><strong>有形之神，无所不能。</strong><br><strong>”</strong></p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/movie/largent_3.jpg"                        width="691.2" height="388.8"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p>法国导演布列松的最后一部作品，一如既往的极简与冷峻，把一切可能引起情绪波动的镜头都省略，唯余平淡、留白。（困的时候真不能看布列松。）    </p><p>看布列松的电影，就好像在看一条三文鱼放在案板上切割，清晰带血，不留一丝余地。  </p><p>500法郎假钞辗转多人，最后经由一场伪证锤死无辜的伊文以牢狱之灾，但这不是一切的结束，而是冷冽悲剧的开始……虚无伴随左右，毁灭不是推波助澜，而是奔赴相拥的终点。  </p><p><strong>推荐理由：</strong></p><p>《电影手册》上法国导演排行榜上名列第一的导演，其最后一部作品将个人风格发挥到了极致。</p><p>“人物是属于导演理念的工具”，电影中人的眼神之间像隔了一层墙，你无法感受到人的呼吸和情绪的冷暖；</p><p>宛似“木偶人”的动作不会诱导无谓的煽情，不会让你有任何偏见与道德审视；</p><p>环境的先导性与延滞性，只提取结果的决定性瞬间，极尽的留白与省略之能事，画外音的自行填补，</p><p>这种冷峻且客观的极简镜头，只是为了展示最终酷烈且无望的主题。  </p><p>不得不提的就是电影的画面：极具老式的胶卷风格，搭配法式建筑与物品，搭配人物如模特的冷峻，这极简的镜头语言将恰到好处的绝佳构图发挥到了极致，随便抽帧便是一幅妥妥的油画或摄影佳作。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/movie/largent_2.jpg"                        width="635" height="384"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p>纯粹的善良是存在的，救赎的希望也给予了，但，来不及了。</p><p>他高举斧子：“钱在哪里？”，老妇人的沉默换来了伊文在疯狂中挥下了斧子，</p><p>其后，他踉踉跄跄的走出来，连粘满血的裤子都没有换下，径直的走进一家有警察的酒馆，在喝完酒保为其倒下的酒后，自首。</p><p>料峭，所有的一切都崩溃了，既然无法摆脱困境与煎熬，那我选择堕入虚无与毁灭的深渊，拥抱最后的惨淡。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/movie/largent_4.jpg"                        width="597" height="360"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p>钱之下，一切的无能为力、罪恶导向与价值虚无！  </p><p>金钱与权力结构以其固有的“神力”堵住了底层人的生存之路，</p><p>并于百态之下剥夺你仅剩的尊严，</p><p>再于极端情形之下助你踏上一切的罪恶之路，</p><p>就完了么？</p><p>没有，还要在制高点给予你最终的审视、批判你的原罪。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/movie/largent_5.jpg"                        width="480" height="270"/>            </center>        </td>   <!--<center>标签将图片居中-->        <td>            <center>            <img src="/image/movie/largent_6.jpg"                        width="480" height="270"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table>]]></content>
      
      
      <categories>
          
          <category> Movie </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paradise的电影乐园 </tag>
            
            <tag> Paradise的电影乐园 剧情 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《About Time》(时空恋旅人)</title>
      <link href="/2022/11/01/movie/about_time.html"/>
      <url>/2022/11/01/movie/about_time.html</url>
      
        <content type="html"><![CDATA[<h1 id="Paradise的电影乐园-爱情"><a href="#Paradise的电影乐园-爱情" class="headerlink" title="#Paradise的电影乐园#  爱情"></a>#Paradise的电影乐园#  爱情</h1><p>导演：Richard Curtis</p><p>主演：Domhnall Gleeson，Rachel McAdams</p><p>上映时间：2013年09月04日</p><p><strong>“</strong><br><strong>We’re all traveling through time together</strong><br><strong>every day of our lives.</strong><br><strong>All we can do is</strong><br><strong>do our best to relish this remarkable ride.</strong><br><strong>“</strong>   </p><p>披着科幻超能力外衣的人生爱情小电影。  </p><p>不顾一切要找回认识你的开始、尴尬到不知所措的同行之路、</p><p>不顾一切飞奔回家的普通求婚、每天分别向左向右的地铁站台、</p><p>一场狂风暴雨下荒唐而又甜蜜的婚礼，以及终究是生活的平凡。  </p><p>不要纠结时间悖论的逻辑Bug，去感受另一种生活的交错、平凡与融洽。  </p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/movie/about_time_3.gif"                        width="648" height="269.4"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p><strong>推荐理由：</strong></p><p>初看前半段你会以为是男主超能力的逆袭之路，</p><p>恰恰不然，拥有的能力是为了步入于我而言奢侈的爱情，</p><p>即使我有无限可以容错自身选择的能力，但是选择依然是你，为了遇见你、为了认识你、为了留住你，依然是你；</p><p>虽然我可以选择重新去过每一天，第一次用来经历与度过，第二次则是体验未注意到的小美好，</p><p>但是我终归有着的是当下与你的美好，就像已经经历过一次一样，享受我们这平凡又非凡的生命中每一天。  </p><p>镜头语言是文艺的，角色的内心随着镜头展现的淋漓尽致；另外，配乐是刚刚好的精妙。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/movie/about_time_2.jpg"                        width="648" height="648"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p>穿越的能力会有修改遗憾的可能，这或许是存在于另一维度或者世界的力量，是我们于这个平凡世界里所寄托的无限期冀。  </p><p>如果我们也有穿越时空的能力，每一次遗憾也就会变得有迹可循，离别后的再一次重逢也会很有意义。    </p><p>但是即使已经足够非凡的你我仍然是有着平凡的一生，</p><p>我们可以选择短暂的跳脱并游离于外世，</p><p>不过皆知那不是长久之计，所以尽早地着眼你我前方的美好与未知吧。</p>]]></content>
      
      
      <categories>
          
          <category> Movie </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paradise的电影乐园 爱情 </tag>
            
            <tag> Paradise的电影乐园 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
