<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>CUDA基础与自动求导教学文档</title>
      <link href="/2023/03/11/cuda.html"/>
      <url>/2023/03/11/cuda.html</url>
      
        <content type="html"><![CDATA[<h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h1><p>相比于一般的CPU代码，CUDA代码可以充分利用GPU中更多的运算核心。适合用于大规模的并行计算，提高运算效率。然而，CUDA代码的门槛比较高，代码比较复杂，并且不易debug。只有充分了解原理，才能减少失误，提高编程效率。</p><p>网上有许多很详细的CUDA编程教学。如果我哪里描述得不清楚，可以自行去网上搜搜看。如果有什么意见建议，也欢迎大家指出，我会尽快更新。</p><h1 id="2-CUDA基础知识"><a href="#2-CUDA基础知识" class="headerlink" title="2 CUDA基础知识"></a>2 CUDA基础知识</h1><h2 id="2-1-文件格式"><a href="#2-1-文件格式" class="headerlink" title="2.1 文件格式"></a>2.1 文件格式</h2><p><code>“.cu”</code> 文件中可以含有在GPU上调用/执行的代码，所以一般会用 <code>“.cu”</code> 代替 <code>“.cpp”</code>。而头文件依然可以沿用 <code>“.h”</code>，当然，使用 <code>“.cuh”</code> 作为在CPU上的函数的头文件在结构上更为合理。</p><h2 id="2-2-前缀"><a href="#2-2-前缀" class="headerlink" title="2.2 前缀"></a>2.2 前缀</h2><p>在CUDA中，<code>host</code> 和 <code>device</code> 是两个重要的概念，我们用 <code>host</code> 指代CPU及其内存，而用 <code>device</code> 指代GPU及其内存。因此，为了区分变量的位置，变量名一般会有前缀：host_，device_，也可以省略为 h_，d_。</p><ul><li>函数一般分为三种：<ul><li>在CPU上调用，在CPU上执行；</li><li>在CPU上调用，在GPU上执行；</li><li>在GPU上调用，在GPU上执行。</li></ul></li></ul><p>分别对应以下三种关键字：__host__，__global__，__device__。使用时声明在函数前面即可，甚至可以声明两种，比如： </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__host__ __device__ void func_1(float&amp; A);</span><br></pre></td></tr></table></figure><p>这样函数 func_1 既可以在CPU上调用执行，也可以在GPU上调用执行。</p><ul><li><p>__host__ 函数就是c++中最普通的函数，所以一般可以省略不写，只有在同时声明 __host__ 和 __device__ 的时候才有必要强调。</p></li><li><p>__global__ 函数必须为 void 类型，因为涉及到CPU与GPU之间的信息传输。调用的时候是在CPU上，比如说在 <code>&quot;main.cu&quot;</code> 里，但是函数内部是在GPU上的。</p></li><li>__device__ 函数一般调用的时候是在 __global__ 函数内部。</li></ul><h2 id="2-3-流程"><a href="#2-3-流程" class="headerlink" title="2.3 流程"></a>2.3 流程</h2><p>一般的代码流程为：</p><ol><li>分配CPU、GPU上的内存空间。</li><li>将变量、参数等从CPU拷贝到GPU分配好的空间上。</li><li>使用 __global__ 函数从CPU端调用GPU的空间进行并行运算。</li><li>将计算结果从GPU拷贝到CPU上并保存。</li><li>释放CPU、GPU的内存。</li></ol><p>这5步可以与C++中的类（class）结合，1、2步放在构造函数中，3、4步放在普通函数中，5步放在析构函数中。</p><h2 id="2-4-内存相关"><a href="#2-4-内存相关" class="headerlink" title="2.4 内存相关"></a>2.4 内存相关</h2><h3 id="2-4-1内存分配"><a href="#2-4-1内存分配" class="headerlink" title="2.4.1内存分配"></a>2.4.1内存分配</h3><p>对于CPU数组直接 <code>malloc</code> 就好。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">h_vertices = (float *)malloc(N * sizeof(float));</span><br></pre></td></tr></table></figure><p>或者使用锁页内存。好像说在拷贝时速度更快？具体请查询 nvidia文档。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaHostAlloc(&amp;h_vertices_2, N * sizeof(float), cudaHostAllocDefault);</span><br></pre></td></tr></table></figure><p>对GPU数组可以使用 <code>cudaMalloc</code> 函数。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaMalloc(&amp;d_vertices, N * sizeof(float));</span><br></pre></td></tr></table></figure><p>分配空间一般可以放在类的构造函数里，避免后续使用时因为忘记分配空间而报错。</p><p><strong>注：</strong>在分配前一定要先声明该变量。声明一般放在类的头文件中。如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">private:</span><br><span class="line">float *h_vertices, *d_vertices;</span><br></pre></td></tr></table></figure><h3 id="2-4-2-内存释放"><a href="#2-4-2-内存释放" class="headerlink" title="2.4.2 内存释放"></a>2.4.2 内存释放</h3><p>使用完毕后，针对上面的三种申请方式，分别使用以下释放方式就好。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">free(h_vertices);</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaFreeHost(h_vertices_2);</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaFree(d_vertices);</span><br></pre></td></tr></table></figure><p>内存释放可以放在类的析构函数中。</p><h3 id="2-4-3-内存拷贝"><a href="#2-4-3-内存拷贝" class="headerlink" title="2.4.3 内存拷贝"></a>2.4.3 内存拷贝</h3><p>分为四种类型：CPU到CPU，CPU到GPU，GPU到CPU，GPU到GPU。</p><p>分别对应四个指令：<code>udaMemcpyHostToHost</code>，<code>cudaMemcpyHostToDevice</code>，<code>cudaMemcpyDeviceToHost</code>，<code>cudaMemcpyDeviceToDevice</code>。</p><p>使用 <code>cudaMemcpy</code> 函数来拷贝，以第二种为例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaMemcpy(d_vertices, h_vertices, N * sizeof(float), cudaMemcpyHostToDevice);</span><br></pre></td></tr></table></figure><p>其中d_vertices、h_vertices都是已经分配好空间的，长度为N的float类型数组。d_、h_分别代表该数组位于GPU和CPU上（前文有提到）。</p><ul><li><p><code>cudaMemcpy</code> 函数只能在CPU中调用。</p></li><li><p><code>cudaMemcpy</code> 中四个参数依次是：拷贝目的地的起始地址、需要拷贝的数组的起始地址、拷贝长度（以字节为单位）、拷贝类型。</p></li><li><p><code>cudaMemcpy</code> 在拷贝前会执行同步函数 <code>cudaDeviceSynchronize();</code>，所以如果想并行执行拷贝命令，可以使用异步拷贝：</p></li><li><p><code>cudaMemcpyAsync</code>，并加入第五个参数：cuda流。这在后面会提到。例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaMemcpyAsync(d_vertices, h_vertices, N * sizeof(float), cudaMemcpyHostToDevice, stream[0]);</span><br></pre></td></tr></table></figure></li></ul><p><strong>注：</strong>前两个参数都是起始地址，所以对于CPU上 <code>float</code> 类型的常量，在输入 <code>cudaMemcpy</code> 函数时要使用 <code>&amp;</code>取地址。但 GPU 常量一般无法在 __host__ 函数中声明，所以要声明数组，再分配空间。例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">float h_eta = 1.49;</span><br><span class="line">float *d_eta;</span><br><span class="line">cudaMalloc(&amp;d_eta, sizeof(float));</span><br><span class="line">cudaMemcpy(d_eta, &amp;h_eta, sizeof(float), cudaMemcpyHostToDevice);</span><br><span class="line">//至此*d_eta = 1.49，并且在GPU上了。</span><br><span class="line">cudaFree(d_eta);</span><br></pre></td></tr></table></figure><h2 id="2-5-核函数"><a href="#2-5-核函数" class="headerlink" title="2.5 核函数"></a>2.5 核函数</h2><p>从流程中可以看到，__global__ 函数是 cuda 并行计算的核心，所以我们一般也称之为核函数（kernel\ function）。</p><p>核函数在调用时需要声明 $&lt;&lt;<blkPGrd,\ thrPBlk>&gt;&gt;$ 来指定cuda所需要的线程数量。例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">func_2 &lt;&lt;&lt;blkPGrd, thrPBlk&gt;&gt;&gt; (input, output, N);</span><br></pre></td></tr></table></figure><p>其中 func_2 是一个 __global__ 函数，blkPGrd 和 thrPBlk 都是 dim3 类型的变量，dim3 可以理解为无符号整型的三维向量 (x,y,z)。</p><p>在定义 blkPGrd 和 thrPBlk 时，系统会默认缺失值为1。这样会方便我们使用一维、二维向量。比如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dim3 blkPGrd(3,2);</span><br></pre></td></tr></table></figure><p>x 的值就是 0，1，2 中的一个，y 的值就是 0，1 中的一个。</p><p>一般来说一维就够用了，计算起来更方便，并且可以直接用 int 类型初始化 blkPGrd 和 thrPBlk。</p><p><strong>注：</strong>这里的 input、output 等参数，包括 N 要么用指针（*），要么传地址（\&amp;）。尽量不要直接(int N)传值，尤其是非标量的参数，因为传值会先把这个值复制到GPU上，再进行计算。影响效率不说，更增加了出错几率。所以，这些参数都要提前在GPU上分配好空间，因为核函数内部使用的参数是在GPU上的。</p><p>当执行到核函数内部，我们认为此时已经在GPU上了。首先需要计算出线程的序号，并忽略超出的部分。例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">__global__ void func_2  (float* input, float* output, const int&amp; N)</span><br><span class="line">&#123;</span><br><span class="line">//总序号 = 每个block含有的线程个数 * block序号 + 当前block下的线程号</span><br><span class="line">int index = blockDim.x * blockIdx.x + threadIdx.x; </span><br><span class="line">//超出需要计算的个数的这些线程直接返回</span><br><span class="line">//一定要注意！不然会报越界的错误</span><br><span class="line">if (index &gt;= N)</span><br><span class="line">&#123;</span><br><span class="line">return;</span><br><span class="line">&#125;</span><br><span class="line">//other computation</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>关于 2、3维的使用请参考网上的教程吧。处理矩阵、图片相关的问题会稍微方便一些。不过用一维 index/ 每行的个数 n 就可以得到行idx，用 index\%n 就可以得到列 idx，和二维 index 的 idx.x、idx.y 本质上是一样的。</p><h2 id="2-6-结构"><a href="#2-6-结构" class="headerlink" title="2.6 结构"></a>2.6 结构</h2><p>讲了这么多还不清楚blkPGrd和thrPBlk到底有什么意义啊？为什么要起这两个名字啊？这就要从GPU的结构说起了。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/cuda1.png"                        width="562" height="525"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p>图片为 CUDA结构示意图，其中 blkPGrd(3,2), thrPBlk(5,3);</p><p>图片节选自：<a href="https://blog.csdn.net/weixin_44966641/article/details/124448102">https://blog.csdn.net/weixin_44966641/article/details/124448102</a></p><ul><li><p>GPU的最小单元是线程（thread），一般的显卡中32个线程为一束（warp）。运行时，如果命令一致，这一束线程会同时执行命令（并行）；而如果命令不一致，这32个线程就会依次执行每种命令（串行）。这是GPU的物理结构导致的。所以如果核函数中的if分支很多，导致每一束的线程经常分在不同的if分支中，就会影响并行效率。不过准确性是一定可以保证的，如果不追求极致效率，if分支多一些也无妨。</p></li><li><p>多个线程构成一个线程块（block）。thrPBlk是 threadsPerBlock 的缩写（我自己瞎缩的），即每个线程块中的线程个数。通常为32（一线程束）的倍数。每种显卡的上限不同，3090最多好像是1024，有些老显卡只支持256。不过thrPBlk也不是越大越好。运算速度具体和什么有关俺也不太清楚。为了在不同服务器上能正常运行，我一般写成256，经过测试速度和512基本一样。</p></li></ul><ul><li><p>而多个线程块构成一个网格（grid）。blkPGrd是blocksPerGrid的缩写，即每个网格中的线程块个数。这一数目往往是根据你需要的线程总数决定的。比如你优化的网格有N个三角形，每个线程控制一个三角形，即总共需要至少N个线程，那么在一维情况下就是：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">blkPGrd = (N + thrPBlk - 1) / thrPBlk;</span><br></pre></td></tr></table></figure><p>公式可以留作思考题，如果你理解了GPU结构，这个公式是很容易推出来的。</p><p>==思考题2：==为什么不写成如下形式？想不出来可以来问我:)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">blkPGrd = (N - 1) / thrPBlk + 1;</span><br></pre></td></tr></table></figure></li><li><p>最后做个形象的类比：某小学校长需要拧 1200 个螺丝。学校每个年级 256 个学生，平均分为 8 个班，每班 32 人。则需要 (1200 + 256 - 1) / 256 = 5 个年级就够了，总共调用了 5 * 256 = 1280 个学生。这样就多出 80 个幸运儿可以摸鱼，也就是上面代码中超过 N 的部分直接 return。而 blkPGrd 如果取 4 及以下就拧不完螺丝，如果取 6 及以上就有更多的学生没螺丝拧。</p></li></ul><h2 id="2-7-CUDA流"><a href="#2-7-CUDA流" class="headerlink" title="2.7 CUDA流"></a>2.7 CUDA流</h2><p>某些时候，多个核函数是可以并行的。比如我们想对于input_a和input_b分别依次执行func_2和func_3，但a、b之间是独立的。直接写代码就会变成以下这样：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">func_2 &lt;&lt;&lt;blkPGrd, thrPBlk&gt;&gt;&gt; (input_a, output_a, N);</span><br><span class="line"></span><br><span class="line">func_3 &lt;&lt;&lt;blkPGrd, thrPBlk&gt;&gt;&gt; (input_a, output_a, N);</span><br><span class="line"></span><br><span class="line">func_2 &lt;&lt;&lt;blkPGrd, thrPBlk&gt;&gt;&gt; (input_b, output_b, N);</span><br><span class="line"></span><br><span class="line">func_3 &lt;&lt;&lt;blkPGrd, thrPBlk&gt;&gt;&gt; (input_b, output_b, N);</span><br></pre></td></tr></table></figure><p>但上面四条语句是依次执行的，效率不够高。此时就可以用到 cuda流。<strong>同一个流内是依次执行的，但不同流之间不会互相影响。</strong>这样就可以完美解决上面的问题。使用方法为：</p><ol><li><p>声明cuda流变量：以下二者均可，之后以第一种为例。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cudaStream_t stream[2];</span><br><span class="line"></span><br><span class="line">cudaStream_t stream0, stream1;</span><br></pre></td></tr></table></figure></li><li><p>创建流：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaStreamCreate(&amp;stream[k]);（k=0，1）</span><br></pre></td></tr></table></figure></li><li><p>修改上面四句话为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">func_2 &lt;&lt;&lt;blkPGrd, thrPBlk, 0, stream[0]&gt;&gt;&gt; (input_a, output_a, N);</span><br><span class="line"></span><br><span class="line">func_2 &lt;&lt;&lt;blkPGrd, thrPBlk, 0, stream[1]&gt;&gt;&gt; (input_b, output_b, N);</span><br><span class="line"></span><br><span class="line">func_3 &lt;&lt;&lt;blkPGrd, thrPBlk, 0, stream[0]&gt;&gt;&gt; (input_a, output_a, N);</span><br><span class="line"></span><br><span class="line">func_3 &lt;&lt;&lt;blkPGrd, thrPBlk, 0, stream[1]&gt;&gt;&gt; (input_b, output_b, N);</span><br><span class="line"></span><br><span class="line">cudaDeviceSynchronize();</span><br></pre></td></tr></table></figure><p>这一行为<strong>同步函数</strong>，作用是等待两个流全部执行完将运算同步方便后续运算。</p><p>在运行的时候，先开始执行第一行的核函数 func_2。紧接着就会读取第二行。由于第二行用的流 stream[1] 是空闲的，会同时执行第二行的核函数 func_2。紧接着读取第三行。直到第一行运行完毕，stream[0] 变成空闲的，第三行核函数 func_3 才会被执行。同理，直到第二行运行完毕，stream[1] 变成空闲的，第四行核函数 func_3 才会被执行。</p></li><li><p>销毁cuda流：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaStreamDestroy(stream[k]);（k=0，1）</span><br></pre></td></tr></table></figure></li></ol><ul><li><p><strong>注1：</strong>尖括号中第三个变量的含义是共享内存，一般用不到，写 0 就完事了。如果需要的话再参考网上教程吧。</p></li><li><p><strong>注2：</strong>当尖括号中只有 blkPGrd，thrPBlk 这两个变量时，调用默认cuda流。读取第二行时，由于默认流被占用，所以会等到第一行运行完默认流变成空闲的时候才会执行第二行。尖括号中要么写两个变量要么写四个，不能只写三个。</p></li><li><p><strong>注3：</strong>流的上限好像是1024？但是一般用不到那么多流，十几个就顶天了。并不是流越多执行就越快，如果GPU核心不够了，就算流是空闲的也不会执行该语句。</p></li></ul><h2 id="2-8-CUDA运算库的使用"><a href="#2-8-CUDA运算库的使用" class="headerlink" title="2.8 CUDA运算库的使用"></a>2.8 CUDA运算库的使用</h2><p>除了自己编写核函数之外，也可以用许多现成来调用GPU做计算。例如线性代数库cuBLAS，稀疏矩阵库cuSPARSE，规约计算库Thrust等等。具体的请参考<a href="https://docs.nvidia.com/cuda/">Cuda Toolkit官方文档</a> 这里着重介绍一下cuBLAS吧，毕竟对我们来说用得最多。</p><ol><li><p>声明cublashandle变量：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cublasHandle_t handle[2];</span><br></pre></td></tr></table></figure></li><li><p>创建handle：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cublasCreate(&amp;handle[k]);（k=0，1）</span><br></pre></td></tr></table></figure></li><li><p>将 handle 与 cuda流绑定（在 cudaStreamCreate 之后）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cublasSetStream(handle[k], stream[k]);（k=0，1）</span><br></pre></td></tr></table></figure></li><li><p>设置指针类型：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cublasSetPointerMode(handle[k], CUBLAS_POINTER_MODE_DEVICE);（k=0，1）</span><br></pre></td></tr></table></figure><p>默认类型为 <code>CUBLAS_POINTER_MODE_HOST</code>。用 host 上的变量来储存计算结果，包含了 “在GPU上计算结果” 和 “将结果拷贝回CPU” 两步。即相当于 <code>CUBLAS_POINTER_MODE_DEVICE + cudaMemcpyDeviceToHost</code> 。</p><p>由于拷贝之前会先同步，等待前面的代码全部执行完毕，所以不适用于多流并行。当然如果只使用一次的话可以去掉这一步使用默认指针类型。</p></li><li><p>以平方和运算为例，其他运算请参照官网文档。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cublasSdot(handle[0], 100, output_a, 1, output_a, 1, d_a);</span><br><span class="line">cublasSdot(handle[1], 100, output_b, 1, output_b, 1, d_b);</span><br><span class="line">// 紧接着使用异步拷贝</span><br><span class="line">cudaMemcpyAsync(h_a, d_a, sizeof(float), cudaMemcpyDeviceToHost, stream[0]);</span><br><span class="line">cudaMemcpyAsync(h_b, d_b, sizeof(float), cudaMemcpyDeviceToHost, stream[1]);</span><br><span class="line">// 多流并行计算结束，使用同步函数同步结果。</span><br><span class="line">cudaDeviceSynchronize();</span><br><span class="line">//输出计算结果</span><br><span class="line">printf(&quot;h_a = %f, h_b = %f \n&quot;, h_a, h_b);</span><br></pre></td></tr></table></figure></li><li><p>销毁handle：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cublasDestroy(handle[k]);（k=0，1）</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Basic Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Basic Knowledge </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>相机模型中坐标系问题</title>
      <link href="/2022/12/29/coordinate.html"/>
      <url>/2022/12/29/coordinate.html</url>
      
        <content type="html"><![CDATA[<p>在涉及到三维重建的相机模型中，真实三维世界中的某一点和其对应像素点之间是通过坐标系的转换得到的。这个过程具体涉及了四个坐标系：世界坐标系、相机坐标系、图像坐标系、像素坐标系。下述文章主要给出转换过程，以辅助理解坐标系的关机、相机光线的生成等问题。具体流程：</p><ul><li><strong>世界坐标系</strong>通过平移和旋转得到<strong>相机坐标系</strong>；</li><li><strong>相机坐标系</strong>通过成像模型原理得到<strong>图像坐标系</strong>；</li><li><strong>图像坐标系</strong>通过平移和缩放得到<strong>像素坐标系</strong>。</li></ul><p>整体示意图和<strong>从世界坐标系到像素坐标系的整体公式</strong>如下：</p><script type="math/tex; mode=display">z_c\left(\begin{array}{c}u \\ v \\ 1\end{array}\right)=\left(\begin{array}{ccc}\frac{1}{dx} & 0 & u_0 \\0 & \frac{1}{dy} & v_0 \\0 & 0 & 1 \end{array}\right)\left(\begin{array}{cccc}f & 0 & 0 & 0\\0 & f & 0 & 0\\0 & 0 & 1 & 0\end{array}\right)\left(\begin{array}{cc}R & T \\0 & c \\\end{array}\right)\left(\begin{array}{c}x_w \\ y_w \\ z_w \\1\end{array}\right)</script><p><img src="https://pic3.zhimg.com/80/v2-fc2d3cb6c20336174849bfb89f665262_720w.webp" alt="img|" style="zoom:80%;" /></p><h1 id="世界坐标系—-gt-相机坐标系"><a href="#世界坐标系—-gt-相机坐标系" class="headerlink" title="世界坐标系—&gt;相机坐标系"></a>世界坐标系—&gt;相机坐标系</h1><p><img src="https://pic3.zhimg.com/80/v2-9713c17b9725f602c1b5431783dbb316_720w.webp" alt="img|" style="zoom:75%;" /></p><script type="math/tex; mode=display">\left(\begin{array}{c}x_c \\ y_c \\ z_c \\1\end{array}\right)=\left(\begin{array}{cc}R & T \\0 & c \\\end{array}\right)_{4\times 4}\left(\begin{array}{c}x_w \\ y_w \\ z_w \\1\end{array}\right)</script><h1 id="相机坐标系—-gt-图像坐标系"><a href="#相机坐标系—-gt-图像坐标系" class="headerlink" title="相机坐标系—&gt;图像坐标系"></a>相机坐标系—&gt;图像坐标系</h1><p><img src="https://pic2.zhimg.com/80/v2-fc062f8ea80bec759403e46e601c58ed_720w.webp" alt="img"></p><p>由小孔成像原理，$f$ 为相机焦距：</p><script type="math/tex; mode=display">\begin{aligned}&\frac{x_c}{x}=\frac{y_c}{y}=\frac{z_c}{f} \\\Rightarrow & x=\frac{x_c}{z_c}f,\quad y=\frac{y_c}{z_c}f\end{aligned}</script><p>从而有：</p><script type="math/tex; mode=display">\left(\begin{array}{c}x \\ y \\ 1\end{array}\right)=\frac{1}{z_c}\left(\begin{array}{cccc}f & 0 & 0 & 0\\0 & f & 0 & 0\\0 & 0 & 1 & 0\end{array}\right)\left(\begin{array}{c}x_c \\ y_c \\ z_c \\1\end{array}\right)</script><h1 id="图像坐标系—-gt-图像坐标系"><a href="#图像坐标系—-gt-图像坐标系" class="headerlink" title="图像坐标系—&gt;图像坐标系"></a>图像坐标系—&gt;图像坐标系</h1><p>由于图像坐标系和像素坐标系处于同一平面，故两者之间的差异在于坐标原点的位置和单位。像素坐标系的原点在图像坐标系的左上角，同时像素坐标系的单位为像素。</p><p><img src="https://pic3.zhimg.com/80/v2-fc2d3cb6c20336174849bfb89f665262_720w.webp" alt="img|" style="zoom:80%;" /></p><p>故两个坐标系之间的变换满足：</p><script type="math/tex; mode=display">u=\frac{x}{dx}+u_0,\quad v=\frac{y}{dy}+v_0.</script><p>其中 $dx,dy$ 表示像素坐标系中每个像素点的宽和高，而图像坐标系的原点在像素坐标系中的横纵坐标分别为 $u_0,v_0$。从而有：</p><script type="math/tex; mode=display">\left(\begin{array}{c}u \\ v \\ 1\end{array}\right)=\left(\begin{array}{ccc}\frac{1}{dx} & 0 & u_0 \\0 & \frac{1}{dy} & v_0 \\0 & 0 & 1 \end{array}\right)\left(\begin{array}{c}x \\ y \\1\end{array}\right)</script>]]></content>
      
      
      <categories>
          
          <category> Basic Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Basic Knowledge </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Instant-ngp</title>
      <link href="/2022/12/10/paper/instant-ngp.html"/>
      <url>/2022/12/10/paper/instant-ngp.html</url>
      
        <content type="html"><![CDATA[<p>Instant-ngp 项目主页：<a href="https://nvlabs.github.io/instant-ngp/">Instant Neural Graphics Primitives with a Multiresolution Hash Encoding</a></p><p>CUDA 版本：<a href="https://github.com/NVlabs/instant-ngp">NVlabs/instant-ngp: Instant neural graphics primitives</a></p><p>Pytorch 版本：<a href="https://github.com/ashawkey/torch-ngp">ashawkey/torch-ngp: A pytorch CUDA extension implementation of instant-ngp (sdf and nerf)</a></p><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><ul><li>本文目的：NeRF 加速；</li><li>由 MLP 表示的 shape 和辐射场，依赖于 heuristics and structural modifications，会使训练过程更复杂，仅限制于具体的某一任务，GPU 训 练成本高昂；</li><li>现有的编码方式或所需内存较大、或计算成本更高，逼近效果较差；</li><li>密集的网络两种浪费：其一分配给空白区域的特征与表面附近的数量一样多，其二自然场景的光滑性促使多分辨率分解。</li></ul><h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul><li>通过一种通用的多分辨率 Hash 编码降低运算代价（减少浮点数和内存访问量），自适应、高效、独立<ul><li>不依赖于任何空间数据结构、训练期间的渐进式修剪或编码场景几何的先验知识</li><li>利用随机梯度进行优化</li><li>基于多分辨率结构允许网络消除 Hash 冲突</li></ul></li><li>整个系统利用 tiny-cuda-nn 框架的 fast fully-fused CUDA kernels，最大限度地减少带宽的浪费与计算操作，实现高效率的并行化</li></ul><h1 id="Multiresolution-Hash-Encoding"><a href="#Multiresolution-Hash-Encoding" class="headerlink" title="Multiresolution Hash Encoding"></a>Multiresolution Hash Encoding</h1><ul><li>全连接神经网络：$m(\mathbf{y},\Phi)$<ul><li>对输入的编码：$\mathbf{y}=ennc(\mathbf{x},\theta)$<ul><li>可训练编码参数：$\theta$</li></ul></li><li>可训练网络权重参数：$\Phi$</li></ul></li></ul><p>上述可训练参数被排列成 $L$ 个级别（level），每个级别包含多达 $T$ 个维度为 $F$ 的特征向量。其中这些超参数的较为典型值设定如下：</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>                <img src="/image/ingp/table1.png"                        width="599.8" height="207.9"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><h2 id="Hash-Encoding"><a href="#Hash-Encoding" class="headerlink" title="Hash Encoding"></a>Hash Encoding</h2><p>下图展示说明了多分辨率 Hash 编码的执行步骤：</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>                <img src="/image/ingp/fig3.png"                        width="904.8" height="423.36"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p>每个级别（上图中显示的红色和蓝色）都是独立的，并且在概念上是将<strong>特征向量存储在网格的顶点处</strong>，其分辨率被选择为介于最粗略和最精细网格分辨率之间的几何级数：</p><script type="math/tex; mode=display">\begin{aligned}&[N_{min},N_{max}] \\N_{l}&:=\lfloor N_{min}\cdot b^l \rfloor, \\b&:=exp\left(\frac{ln N_{max}-ln N_{min}}{L-1}\right).\end{aligned}</script><p>$N_{max}$ 的选择是为了匹配训练数据中最精细的细节。由于级别 $L$ 的数量很大，因此生长因子通常很小。论文中用例为 $b\in[1.38,2]$。</p><p><strong>首先考虑一个单独的级别（level）$l$</strong> 。输入坐标 $\mathbf{x}\in\mathbb{R}^d,(d=2,3)$，在向下和向上舍入之前，按这个级别下的网格分辨率缩放：</p><script type="math/tex; mode=display">\begin{aligned}\lfloor \mathbf{x}_l\rfloor&:=\lfloor \mathbf{x}\cdot N_l \rfloor,\\\lceil \mathbf{x}_l \rceil&:=\lceil \mathbf{x}\cdot N_l \rceil.\end{aligned}</script><p>$\lfloor \mathbf{x}_l\rfloor$ 和 $\lceil \mathbf{x}_l \rceil$ 在 $\mathbb{Z}^d$ 中横跨了一个具有 $2^d$ 个整数顶点的体素。我们将每个角（corner）映射到 level 中各自特征向量数组中的一个条目，该数组的固定大小最多为 $T$。对于粗略级别，一个密集网格需要少于 $T$ 个参数，即$(N_l)^d\leq T$，此映射为 $1:1$ 的。 在更精细的级别上，我们使用哈希函数 $h:\mathbb{Z}^d\to\mathbb{Z}_T$ 来索引数组，尽管没有显式的冲突处理，但是我们有效地可以将其视为哈希表。 相反，我们依靠基于梯度（gradient-based）优化以在数组中存储适当的稀疏细节，以及之后的神经网络 $m(\mathbf{y};\Phi)$ 来解决冲突问题。因此，可训练的编码参数 $\theta$ 的数量为$O(T)$，并以 $T\cdot L\cdot F$ 为界，在论文例子中，$T\cdot L\cdot F$ 始终为 $T\cdot 32$（见表 1）。</p><p>我们使用以下形式的哈希函数：</p><script type="math/tex; mode=display">h(\mathbf{x})=\left(\bigoplus_{i=1}^d x_i \pi_i\right) mod\ T.</script><p>其中 $\bigoplus$ 表示按位异或运算（the bit-wise XOR），$\pi_i$ 是唯一的、大素数。实际上，该公式对每个维度的线性同余（伪随机）置换的结果进行异或运算，从而消除了维度对哈希值的影响。值得注意的是，为了实现（伪）独立性，只需置换 $d$ 维中的 $d-1$ 个，因此我们选择 $\pi_1:=1,\pi_2=2654435761,\pi_3=805459861$ 以获得更好的缓存一致性。</p><p>最后，根据 $\mathbf{x}$ 在其超立方体内的相对位置，对每个角的特征向量进行 $d$ 线性插值，插值权重为 $\mathbf{w}_l:=\mathbf{x}_l-\lfloor\mathbf{x}_l\rfloor$。</p><p>回想一下，这个过程对于 $L$ 个 level 都是独立进行的。每个 level 的插值特征向量，以及辅助输入 $\xi\in\mathbb{R}^E$（如编码的视图方向和神经辐射缓存中的纹理），被连接起来产生 $\mathbf{y}\in\mathbb{R}^{L F+E}$，构成 MLP $m(\mathbf{y};\Phi)$ 的编码输入$enc(\mathbf{x};\theta)$。</p><p>所以，整个 <strong>Hash 编码流程</strong>如下：</p><ol><li>对于给定的输入坐标 $\mathbf{x}\in\mathbb{R}^d$，找到 $L$ 个分辨率级别下的周围体素，并通过 Hash 整数坐标为它们的 corner 分配索引;</li><li>对于得到的所有 corner 索引，从哈希表 $\theta_l$ 中查找相应的 $F$ 维特征向量；</li><li>根据 $\mathbf{x}$ 在对应第 $l$ 个体素内的相对位置对它们进行线性插值；</li><li>连接每个级别的结果以及辅助输入的 $\xi\in\mathbb{R}^{E}$，产生编码的 MLP 的输入 $\mathbf{y}\in\mathbb{R}^{L F+E}$；</li><li>MLP 评估。</li></ol><p>注：为了训练编码，损失梯度通过 MLP (5)、连接 (4)、线性插值 (3) 反向传播，然后在查找的特征向量中累积。</p><h2 id="Performance-vs-quality"><a href="#Performance-vs-quality" class="headerlink" title="Performance vs. quality"></a>Performance vs. quality</h2><p>哈希表大小 $T$ 的选择可以对性能、内存和质量之间进行权衡。$T$ 值越高，质量越高，性能越低。 内存占用在 $T$ 中是线性的，而质量和性能往往呈次线性增加。我们在图 4 中分析了 $T$ 的影响，其中报告了三个神经图形基元的各种 $T$ 值的测试误差与训练时间的关系。建议使用者调整 $T$ 以将编码调整为想要的性能特征。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>                <img src="/image/ingp/fig4.png"                        width="900" height="319"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p>超参数 $L$（层数）和 $F$（特征维数）也权衡了质量和性能，图 5 中分析了一个近似恒定数量的可训练编码参数 $\theta$。在此分析中，发现（$F=2,L=16$）在论文所有的应用程序中都是一个有利的 Pareto 最优值，因此我们在所有其他结果中使用这些值并将它们作为默认值推荐给从业者。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>                <img src="/image/ingp/fig5.png"                        width="900" height="293.83"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><h2 id="Implicit-hash-collision-resolution"><a href="#Implicit-hash-collision-resolution" class="headerlink" title="Implicit hash collision resolution"></a>Implicit hash collision resolution</h2><p>Hash 编码能够在存在哈希冲突的情况下仍然忠实地重建场景。这似乎违反直觉，但其成功的关键在于不同的分辨率层级具有相辅相成的不同优势。</p><ul><li>对于粗略的层级，Hash 编码是作为一个整体的编码，是单射的——也就是说，它们根本没有冲突。 但是，它们只能表示场景的低分辨率版本，因为其特征是从较宽间隔的点网格中线性插值的。</li><li>精细级别由于其精细的网格分辨率可以捕获细节（高频）特征，但会遭受许多冲突——即哈希函数将不同点映射到同一个表条目上。附近具有相等整数坐标 $\lfloor\mathbf{x}_l\rfloor$ 的输入不被视为冲突；当不同的整数坐标哈希映射到相同的索引时才视为发生冲突。幸运的是，这样的<strong>冲突是伪随机分散在空间中的，并且在统计上不太可能同时发生在单个给定点的每个层级中</strong>。</li></ul><p><strong>当训练样本以上述方式冲突时，它们相应的梯度会平均</strong>。考虑到此类样本对最终重建的重要性很少相等。例如，辐射场中可见表面上的一个点将对重建图像有很大的贡献（具有高可见性和高密度，这两项都会成倍地影响反向传播梯度的大小），将导致其表条目发生较大变化；而空白空间中恰好引用同一条目的点将具有更小的权重。结果，更重要样本点的梯度支配了冲突平均值，并且有问题的表中条目自然会以反映更高权重点的需求的方式进行优化；然后，不太重要的点将通过多分辨率层次结构中的其他级别对其最终输出进行校正。</p><p>哈希编码的多分辨率方面涵盖了从保证无碰撞的粗分辨率 $𝑁<em>{min}$ 到任务所需的最精细分辨率 $𝑁</em>{max}$ 的全部范围。因此，它保证包括所有可以进行有意义学习的尺度，而不管稀疏性如何。 几何缩放允许仅用 $\mathcal{O}(log(N<em>{max}/N</em>{min}))$ 个层级覆盖这些尺度，这允许为 $𝑁_{max}$ 选择一个保守的大值。</p><h2 id="Online-adaptivity"><a href="#Online-adaptivity" class="headerlink" title="Online adaptivity"></a>Online adaptivity</h2><p>注意，如果输入 $\mathbf{x}$ 的分布在训练期间随时间变化，例如，如果它们集中在一个小区域中，那么更精细的网格层级将经历更少的冲突，并且可以学习更准确的函数。换句话说，多分辨率哈希编码自动适应训练数据分布，继承了基于树的编码的优点，而无需维护可能导致训练期间离散跳跃的特定任务数据结构。其中一个应用程序，第 5.3 节中的神经辐射缓存（neural radiance caching），不断适应动画视点和 3D 内容，极大地受益于此功能。</p><h2 id="𝑑-linear-interpolation"><a href="#𝑑-linear-interpolation" class="headerlink" title="𝑑-linear interpolation"></a>𝑑-linear interpolation</h2><p>对查询的哈希表条目进行插值可确保编码 $enc(\mathbf{x};\theta)$ 以及通过链式法则与神经网络 $m(enc(\mathbf{x};\theta);\Phi)$ 的组合是连续的。如果没有插值，网络输出中将出现网格对齐的不连续性（grid-aligned discontinuities），这将导致不希望的块状外观。 人们可能需要更高阶的平滑度，例如在逼近偏微分方程时。 计算机图形学中的一个具体示例是有符号距离函数（SDF，signed distance functions），在这种情况下，梯度 $\partial m(enc(\mathbf{x};\theta);\Phi)/\partial \mathbf{x}$，即表面法线，在理想情况下也是连续的。对于这种情况，我们在附录 A 中提供了一种低成本的方法。</p><h1 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h1><p>为了演示多分辨率哈希编码的速度，论文在 CUDA 中实现，并将其与 tiny-cuda-nn 框架的 fast fully-fused 的 MLP 集成。 </p><p>$\mathbf{Performance\ considerations.}$ 为了优化推理和反向传播性能，以<strong>半精度</strong>（每个条目 2 个字节）存储哈希表条目。并且还维护了一个全精度参数的主副本（a master copy），以实现稳定的混合精度参数更新。</p><p>为了优化使用 GPU 的缓存，逐级评估哈希表：在处理一批输入位置时，我们安排计算查找所有输入的第一级多分辨率哈希编码，然后再查找所有输入的第二级，以此类推。因此，在任何给定时间，只有少量连续的哈希表必须驻留在缓存中，具体取决于 GPU 上可用的并行度。重要的是，这种计算结构自动充分利用了可用缓存和并行性，适用于各种哈希表大小 $T$。</p><p>在我们的硬件上，只要哈希表大小保持在 $T\leq2^{19}$ 以下，编码的性能就会大致保持不变。超过这个阈值，性能就会开始显着下降；请参见图 4。这可以通过我们的 NVIDIA RTX 3090 GPU 的 6 MB L2 cache 来解释，当 $2\cdot T\cdot F&gt;6\cdot 2^{20}$ 时，对于单个哈希表层级来说它就太小了（其中 $2$ 是半精度条目的大小）。</p><p>每次查找的最佳特征维度 $F$ 的数量取决于 GPU 架构。 一方面，在前面提到的流式处理方法中只有少数有利于缓存局部性；但另一方面，较大的 $F$ 通过允许 F-wide 向量加载指令有利于内存一致性。$F=2$ 为我们的 GPU 提供了最佳的成本质量权衡，我们在所有实验中都使用它； 见图 5。</p><p>$\mathbf{MLP\ architecture.}$ 在所有任务中，除了我们稍后将描述的 NeRF之外，我们使用具有两个隐藏层的 MLP，这些隐藏层的宽度为 64 个神经元和校正线性单元（ReLU，rectified linear unit）激活函数，以及一个线性输出层。对于 NeRF 和 SDF，最大分辨率 $N_{max}$ 设置为 $2048\times$ 场景大小，十亿像素图像宽度的一半，在辐射缓存（radiance caching）中设置为 $219$（大值以支持扩展场景中的近距离物体）。</p><p>$\mathbf{Initialization.}$ 我们根据 Glorot 和 Bengio 初始化神经网络权重，以提供合理的激活缩放比例及其在整个神经网络层中的梯度。 我们使用均匀分布 $\mathcal{U}(-10^{-4},10^{-4})$ 初始化哈希表条目以提供少量随机性，同时鼓励初始预测接近于零。 这种初始化在我们所有的任务中都运行良好。 我们还尝试了各种不同的分布，包括零初始化，所有这些都会导致初始收敛速度稍微差一些。 散列表似乎对初始化方案具有鲁棒性。</p><p>$\mathbf{Training.}$ 我们通过应用 Adam 优化器联合训练神经网络权重和哈希表条目，其中我们设置 $\beta_1=0.9$，$\beta_2=0.99$，$\epsilon=10^{-15}$。$\beta_1$ 和 $\beta_2$ 的选择只有很小的差别，但是当它们的梯度稀疏并且很弱时，$\epsilon=10^{-15}$ 的小值可以显著加速哈希表条目的收敛。为了防止长时间训练后出现发散，我们将弱 $L_2$ 正则化（因子 $10^{-6}$）应用于神经网络权重，而没有应用在哈希表条目上。</p><p>在拟合十亿像素图片或 NeRF 时，使用 $\mathcal{L}^2$ 损失。对于 SDF，使用平均绝对百分比误差（MAPE，mean absolute percentage error），$|prediction-target|/(|target|+0.01)$，对于神经辐射缓存，使用亮度相关的（luminance-relative） $\mathcal{L}^2$ 损失。</p><p>我们观察到最快的收敛速度下，符号距离函数的学习率为 $10^{−4}$，否则为 $10^{−2}$，神经辐射缓存的批处理大小为 $2^{14}$，否则为 $2^{18}$。</p><p>最后，对于梯度正好为 $0$ 的哈希表条目，我们跳过 Adam 步骤。这在梯度稀疏时节省了$\sim 10\%$的性能，这在 $T\gg BatchSize$ 时很常见。尽管这种启发式违反了 Adam 背后的一些假设，但我们观察到收敛性没有下降。</p><p>$\mathbf{Non-spatial\ input\ dimensions}\ \xi\in\mathbb{R}^E.$ 多分辨率哈希编码的目标是具有相对较低维度的空间坐标。我们所有的实验都是在 2D 或 3D 中进行的。 然而，在学习光场时，将辅助维度 $\xi\in\mathbb{R}^E$ 输入到神经网络中通常很有用，例如观察方向和材料参数。在这种情况下，可以使用已建立的技术对辅助维度进行编码，其成本不会随维度超线性增加；我们在神经辐射缓存中使用 one-blob 编码，在 NeRF 中使用球谐波基，类似于并发工作。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeRF </tag>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeuS</title>
      <link href="/2022/12/05/paper/neus.html"/>
      <url>/2022/12/05/paper/neus.html</url>
      
        <content type="html"><![CDATA[<p>NeuS项目主页：<a href="https://lingjie0206.github.io/papers/NeuS/index.htm">NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction</a></p><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><ul><li>本文目的：从多视角2D图片，重建物体的3D表面。</li><li>传统的神经表面重建工作（DVR，IDR）要求前景 mask 做监督，并且容易陷入局部最小值，因此难以重建具有自遮挡或薄结构的物体；</li><li>对 NeRF 及其变体通过深度学习生成的高质量隐式表面没有足够的表面限制，难以提取高质量表面。</li></ul><h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul><li>提出了一种新颖的神经表面重建方法——NeuS（Neural surface Reconstruction）：将表面表示为符号距离函数（SDF： signed distance function）的零水平集（zero-level），通过新式的体渲染（volume rendering）方法来训练 SDF 表示；</li><li>相对于传统体渲染方法，提出一种在一阶估计中无偏的公式，达到更精确的表面重建甚至可以不用 mask 监督。</li></ul><h1 id="Input-amp-Output"><a href="#Input-amp-Output" class="headerlink" title="Input &amp; Output"></a>Input &amp; Output</h1><ul><li>Input： 多视角RGB图片、对应图片的相机位姿，Mask（可选）</li><li>Output： SDF</li></ul><h1 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h1><h2 id="Scene-representation"><a href="#Scene-representation" class="headerlink" title="Scene representation"></a>Scene representation</h2><p>NeuS 是隐式表面重建，通过将待重建表面表达为 SDF 函数的零水平集，利用 MLP 和体渲染方法学习出这种神经 SDF 表示，具体场景表示建模如下： </p><ul><li><p>SDF： $f:\mathbf{x}\in\mathbb{R}^3\to signed\ distance\in\mathbb{R}$，$\mathbf{x}$ 为空间位置；</p><ul><li>默认待重建表面是水密的（密闭的）；</li><li>SDF 表示空间任一点到待重建表面的有向距离，当该点在物体外部时距离为正，在物体内部时距离为负，</li></ul></li><li><p>Logistic density distribution：$\phi_s(x)=s e^{-s x}/(1+e^{-s x})^2$；</p><ul><li>即 Sigmoid 函数 $\Phi_s(x)=1/(1+e^{-sx})$ 的导数，$\phi^\prime_s(x)=\Phi^\prime_s(x)$；</li><li>事实上 $\phi_s(x)$ 可以是任意以 $0$ 为中心的单峰密度分布，这里出于计算方便；</li><li>标准差为 $1/s$，也是一个可训练的参数，希望最终收敛 $1/s\sim 0$；</li></ul></li><li><p>待重建的表面被表示为 zero-level 集：$\mathcal{S}={\mathbf{x}\in\mathbb{R}^3|f(\mathbf{x})=0}$；</p></li><li>密度函数 S-density：$\sigma(\mathbf{x})=\phi_s(f(\mathbf{x}))$；</li><li>辐射场函数 $c:(\mathbf{x,v})\in\mathbb{R}^3\times\mathbb{S}^2\to\mathbb{R}^3$，$\mathbf{v}$ 为观察方向；<ul><li>对空间任一点 $\mathbf{x}$，在给定观察方向 $\mathbf{v}$ 下的颜色值；</li></ul></li></ul><p>期望的是训练结果，待重建的表面SDF接近于 $0$，其他地方为有向距离；并且训练收敛后 $1/s\sim 0$，S-density $\phi_s(f(\mathbf{x}))$ 在待重建的表面附近有显著的高值，其他位置趋近于 $0$（如下图）。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->    <td><center><img src="/image/neus/sigmod.png"                        width="216" height="135.6"/></center></td>   <!--<center>标签将图片居中-->    </tr></table><h2 id="Volume-Rendering"><a href="#Volume-Rendering" class="headerlink" title="Volume Rendering"></a>Volume Rendering</h2><p>为了学习（训练）神经SDF表示和辐射场的参数，作者建议采用体渲染的方法来渲染SDF表示中的图片，所以，对任一图片内给定像素，其基于相机内部光心原点 $\mathbf{o}$ 在观察方向 $\mathbf{v}$ 发出的一条光线 ${\mathbf{x}(t)=\mathbf{o}+t \mathbf{v}|t\geq 0}$，沿光线累积颜色：</p><script type="math/tex; mode=display">C(\mathbf{o,v})=\int_0^{+\infty}w(t)c(\mathbf{x}(t),\mathbf{v})d t</script><p>$w(t)$ 为点 $\mathbf{x}(t)$ 的权重，$C(\mathbf{o,v})$ 即为这个像素的输出颜色。</p><h2 id="Requirements-on-weight-function"><a href="#Requirements-on-weight-function" class="headerlink" title="Requirements on weight function"></a>Requirements on weight function</h2><p>可以说，从 2D 图片学习精确的 SDF 表示的关键就在于构建出一种输出的颜色值与 SDF 的合适的联系，也即，基于场景的 SDF $f$ 推导出一个合适的权重函数。对权重函数的要求如下：</p><ul><li><p><strong>无偏性（Unbiased）。</strong>在光线与表面交点处 $\mathbf{x}(t^\ast)$，也即：$f(\mathbf{x}(t^\ast))=0$ 时，$w(t)$ 达到局部最大值；</p><ul><li>保证光线与表面（SDF零水平集）交点对像素颜色贡献最大；</li></ul></li><li><p><strong>感知遮挡（Occlusion-aware）。</strong></p><ul><li><script type="math/tex; mode=display">f(t_0)=f(t_1),\quad t_0<t_1,\quad w(t_0)>0,w(t_1)>0 \Rightarrow w(t_0)>w(t_1)</script><p>即更靠近相机的点有更大的权重，也即对最终的输出颜色贡献更大；</p></li><li><p>确保当光线依次通过多个表面时，渲染过程将正确地使用离相机最近的表面的颜色来计算输出颜色。</p></li></ul></li></ul><p>在传统的体渲染中：$w(t)=T(t)\sigma(t),\quad T(t)=exp(-\int_0^t \sigma(u)d u)$ 为沿光线累积透射率（accumulated transmittance），$\sigma(t)=\phi_s(f(\mathbf{x}(t)))$ 为体密度（volume density）。——遮挡感知的，但是<strong>有偏的</strong>，会在重建表面引入固有错误（如下图，在表面交点处，权重函数并没有局部最大）。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->    <td><center><img src="/image/neus/fig2.png"                        width="361.8" height="266.1"/></center></td>   <!--<center>标签将图片居中-->    </tr></table>**NeuS的方案。**$$\begin{aligned}w(t)&=T(t)\rho(t),&\cdots(1)\\T(t)&=exp\left(-\int_0^t \rho(u)d u\right),&\cdots(2)\\\rho(t)&=\max\left(\frac{-\frac{d\Phi_s}{dt}(f(\mathbf{x}(t)))}{\Phi_s(f(\mathbf{x}(t)))},0\right).&\cdots(3)\end{aligned}$$其中式子 (1) 为传统体渲染格式，保证了遮挡感知；式子 (3) 为不透明密度（opaque density）函数。这在 SDF 的一阶近似中是无偏的（具体推导参见论文即可，无偏的示例如上面图2b所示，多表面交点示例如下所示）。<table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->    <td><center><img src="/image/neus/fig3-neus.png"                        width="241.8" height="300.8"/></center></td>   <!--<center>标签将图片居中-->    </tr></table><h2 id="Discretization"><a href="#Discretization" class="headerlink" title="Discretization"></a>Discretization</h2><p>为了对不透明密度和权重函数的离散化形式，采用类似 NeRF 中的同样的估计方法。沿光线采样 $n$ 个点，从而沿此光线的像素处颜色为 ：</p><script type="math/tex; mode=display">\begin{aligned}\hat{C}&=\sum_{i=1}^n T_i\alpha_i c_i \\T_i&=\prod_{j=1}^{i-1}(1-\alpha_j) \\\alpha_i&=max\{\frac{\Phi_s(f(\mathbf{x}(t_i)))-\Phi_s(f(\mathbf{x}(t_{i+1})))}{\Phi_s(f(\mathbf{x}(t_i)))},0\}.\end{aligned}</script><p>有关 $\alpha_i$ 的详细推导参见原文的附录。</p><h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><p>通过在每次迭代中，从一张图片内随机的选取一部分（batch）像素以及世界空间 $P={C_k,M_k,\mathbf{o}_k,\mathbf{v}_k}$ 内相应的光线来优化神经网络和上述标准差的倒数（$1/s$），其中 $C_k$ 为像素的真实颜色，$M_k\in{0,1}$ 为可选的 Mask 的值，即是否有 Mask。$n$ 为采样点数，$m$ 为 batch size。</p><ul><li><p>损失函数：</p><script type="math/tex; mode=display">\mathcal{L}=\mathcal{L}_{color}+\lambda\mathcal{L}_{reg}+\beta\mathcal{L}_{mask}</script><ul><li><p>光度（颜色）损失：</p><script type="math/tex; mode=display">\mathcal{L}_{color}=\frac{1}{m}\sum_k\left|\hat{C}_k-C_k\right|</script><ul><li>监督渲染颜色接近真实值颜色</li></ul></li><li><p>Eikonal loss： </p><script type="math/tex; mode=display">\mathcal{L}_{reg}=\frac{1}{nm}\sum_{i,k}\left(\parallel\nabla f(\hat{\mathbf{x}}_{k,i})\parallel_2-1\right)^2</script><ul><li><p>约束 MLP 学习出的 SDF 的导数满足导数模长等于 1 的性质；</p></li><li><p>简单证明：</p><p>假设上述 $f$ 是定义在 $\Omega\in\mathbb{R}^3$ 上的 SDF，</p><p>对 $\forall \mathbf{x}\notin\partial\Omega$，存在一个最近点 $\mathbf{y}\in\partial\Omega$，</p><p>因此 $\mathbf{v}=(\mathbf{y-x})/\parallel\mathbf{y-x}\parallel$ 是最速下降方向，也即与 $\nabla f(\mathbf{x})$ 同向，</p><p>又因 $f$ 定义，即恰好表示其到最近点的距离，</p><p>故沿 $\mathbf{v}$ 方向移动一单位，$f$ 也变化一单位，</p><p>即方向导数满足 $D_v f=\nabla f(\mathbf{x})\cdot\mathbf{v}=\parallel\nabla f(\mathbf{x})\parallel\parallel\mathbf{v}\parallel\cos\theta=1$，</p><p>故 $\parallel\nabla f\parallel=1$。</p></li></ul></li><li><p>可选的掩码损失： </p><script type="math/tex; mode=display">\mathcal{L}_{mask}=BCE(M_k,\hat{W}_k),\quad\hat{W}_k=\sum_{i=1}^nT_{k,i}\alpha_{k,i}</script><ul><li>$\hat{W}_k$ 沿相机光线的权重总和，$BCE$ 是二元交叉熵损失。</li></ul></li></ul></li><li><p>分层采样（Hierarchical sampling）：</p><ul><li>首先沿射线均匀采样 64 个点，然后迭代进行 $k=4$ 次的重要性采样。</li><li>只维护一个网络（不像NeRF中同时维护粗糙和精细两个网络）<ul><li>粗采样的概率基于固有标准差的 S-density $\phi_s(f(\mathbf{x}))$ 计算；</li><li>精细采样的概率则是基于学习到的 $s$ 的 $\phi_s(f(\mathbf{x}))$ 计算；</li><li>具体细节参见原论文附录以及渲染代码。</li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeRF</title>
      <link href="/2022/12/04/paper/nerf.html"/>
      <url>/2022/12/04/paper/nerf.html</url>
      
        <content type="html"><![CDATA[<p>NeRF项目主页：<a href="https://www.matthewtancik.com/nerf">NeRF: Neural Radiance Fields</a></p><h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><ul><li>本文目的：新视角合成（new view synthesis），对输入的 2D RGB 多视角稀疏图片，合成其他视角下的高逼真图片。</li></ul><h1 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h1><ul><li>提出了一种可以应对复杂几何和材质的连续场景的表示方法 NeRF：参数化为 MLP 的神经辐射场；</li><li>基于经典体渲染技术的可微渲染过程，提出分层采样策略，充分利用 MLP 的表达能力以对网络进行优化；</li><li>使用位置编码（Position Encoding），将输入的 5D 坐标映射到更高维的空间，让 NeRF 可以表示高频场景内容。</li></ul><h1 id="Input-amp-Output"><a href="#Input-amp-Output" class="headerlink" title="Input &amp; Output"></a>Input &amp; Output</h1><ul><li><strong>Input：</strong> 多视角RGB图片、对应图片的相机位姿;</li><li><strong>Output：</strong> NeRF 场景表示</li></ul><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>本文以一种新颖的方式解决了长期存在的视图合成问题：将静态场景表示（拟合）为连续的 5D 函数，函数输出各个空间点 $\mathbf{x}=(x,y,z)$ 在各个方向 $\mathbf{v}=(\theta,\phi)$ 的辐射亮度（颜色）和密度；通过优化一个（无卷积层）深度全连接神经网络（MLP，多层感知器，multilayer perceptron）的参数，根据一个 5D 输入 $(x,y,z,\theta,\phi)$，最小化渲染图像与真实图像的误差损失，回归学习输出空间该点处的体密度和此方向下的 RGB 颜色。流程图 （Pipeline）如下：</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>                <img src="/image/nerf/pipline.png"                        width="729.45" height="412.65"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><h1 id="Neural-Radiance-Field-Scene-Representation"><a href="#Neural-Radiance-Field-Scene-Representation" class="headerlink" title="Neural Radiance Field Scene Representation"></a>Neural Radiance Field Scene Representation</h1><ul><li>NeRF 通过将一个连续的静态场景表示为一个 5D 的向量值函数：</li></ul><script type="math/tex; mode=display">F_\Theta:(\mathbf{x},\mathbf{v})\in\mathbb{R}^5\to(\mathbf{c},\sigma)</script><p>函数输入为：3D 空间位置 $\mathbf{x}=(x,y,z)\in\mathbb{R}^3$ 以及 2D 视角观察方向 $\mathbf{v}=(\theta,\phi)$（一般表示为单位笛卡尔向量，3D向量，但是模长为1）；函数输出为：辐射颜色 $\mathbf{c}=(r,g,b)$ 和空间体密度 $\sigma$。</p><ul><li><p>为了将输出的依赖关系约束为：预测的体密度 $\sigma$ 仅依赖于空间位置 $\mathbf{x}$，RGB 颜色 $\mathbf{c}$ 同时依赖于空间位置 $\mathbf{x}$ 和视角方向 $\mathbf{v}$，（这是合理的，因为空间中任一点的密度不会随观察角度的变化而发生改变，但是颜色则会依赖从那个观察方向观看），以此通过这种依赖关系约束网格学习到场景的多视角连续表示，网格的具体进程如下：</p><ul><li>首先使用 8 层的全连接层（ReLU 作为激活函数，每层有 256 个通道），输入 3D 空间坐标 $\mathbf{x}$，输出 体密度 $\sigma$ 和一个 256 维的特征向量 $\xi$；</li><li>其次将上述特征向量 $\xi$ 和视角方向 $\mathbf{v}$ 连接起来作为输入，输入到另一个全连接层（ReLU 作为激活函数，每层有 256 个通道），输出和方向相关的 RGB 颜色 $\mathbf{c}$。</li></ul><p>（后面的工作已证明这是两个独立的网络）</p></li></ul><h1 id="Volume-Rendering-with-Radiance-Fields"><a href="#Volume-Rendering-with-Radiance-Fields" class="headerlink" title="Volume Rendering with Radiance Fields"></a>Volume Rendering with Radiance Fields</h1><p>使用经典体渲染技术可以渲染出任意射线穿过场景的颜色。体积密度 $\sigma(\mathbf{x})$ 可以解释为：光线在位置 $\mathbf{x}$ 处终止于无穷小粒子的可微概率。在最近 $t_n$ 与最远 $t_f$ 边界的条件下，相机光线 $\mathbf{r}(t)=\mathbf{o}+t\mathbf{d}$ 的颜色 $C(\mathbf{r})$ 为：</p><script type="math/tex; mode=display">C(\mathbf{r}) = \int_{t_n}^{t_f} T(t)\sigma(\mathbf{r}(t))c(\mathbf{r}(t),\mathbf{d}) dt, \qquad where\quad T(t)=e^{-\int_{t_n}^{t}\sigma(\mathbf{r}(s))} ds</script><p>其中函数 $T(t)$ 表示沿着光线从 $t_n$ 到 $t$ 所累积的透明度（accumulated transmittance），即光线从 $t_n$ 到 $t$ 未击中任何粒子的概率。视图渲染即需估计积分 $C(\mathbf{r})$，即光线通过每个像素所累积的颜色。</p><p>通过数值离散积分估计上述累积颜色：将 $[t_n,t_f]$ 区间 $N$ 等分，对每个区间段随机均匀采样，故第 $i$ 个采样点为</p><script type="math/tex; mode=display">t_i \sim \mathcal{U} [t_n+\frac{i-1}{N}(t_f-t_n),t_n+\frac{i}{N}(t_f-t_n)]</script><p>此时积分的离散形式为：</p><script type="math/tex; mode=display">C(\mathbf{r}) = \sum_{i=1}^{N} T_i \cdot (1-e^{-\sigma_i \delta_i}) \mathbf{c}_i,\quad where\ T_i=exp(-\sum_{j=1}^{i-1}\sigma_j\delta_j), \quad \delta_i=t_{i+1}-t_i</script><h1 id="Optimizing-a-Neural-Radiance-Field"><a href="#Optimizing-a-Neural-Radiance-Field" class="headerlink" title="Optimizing a Neural Radiance Field"></a>Optimizing a Neural Radiance Field</h1><p>如果只是采用上述这种策略来拟合场景，对于一些复杂的场景效果并不理想，一方面很难得到较高分辨率的结果，无法恢复场景中的高频细节，另一方面不能高效利用每条光线的采样点。所以论文提出了两个进一步的优化技巧：位置编码（Position Encoding）、分层采样（Hierarchical Sampling）。</p><h2 id="Position-Encoding"><a href="#Position-Encoding" class="headerlink" title="Position Encoding"></a>Position Encoding</h2><p>Rahaman 等人证明了深度网络倾向于学习到低频函数；使用高频函数把输入映射到更高维的空间中，再传递到神经网络，可以更好地拟合具有高频变化的数据。</p><p>因此为了拟合场景中的高频细节，应用到 NeRF 中，重建的网络表示为两个函数的复合形式：</p><script type="math/tex; mode=display">F_\Theta \to F_\Theta=F_\Theta^\prime\circ\gamma</script><p>其中 $F_\Theta^\prime$ 是普通的MLP，编码函数 $\gamma:\mathbb{R}\to\mathbb{R}^{2L}$ 如下：</p><script type="math/tex; mode=display">\gamma(p)=(\sin(2^0\pi p),\cos(2^0\pi p),\cdots,\sin(2^{L-1}\pi p),\cos(2^{L-1}\pi p))</script><p>该函数分别作用于空间位置 $\mathbf{x}$ 以及视角方向 $\mathbf{v}$ 的每个分量。（该论文中空间位置和视角方向分别为 $L=10,4$）</p><h2 id="Hierarchical-volume-sampling"><a href="#Hierarchical-volume-sampling" class="headerlink" title="Hierarchical volume sampling"></a>Hierarchical volume sampling</h2><p>前面提出的传统沿光线离散采样 $N$ 个点对于评估 NeRF 网络是不充分的：空白空间和遮挡区域对于渲染图片是没有贡献的，但是仍然被重复的采样评估。通过会早期体渲染工作的借鉴，论文提出了分层采样的策略来提升渲染效率：根据所期望的渲染效果，来按比例地分配采样点。所以并不是使用单个网络来表示场景，而是考虑同时优化两个网络：粗糙网络和精细网络。</p><ul><li><p>首先使用分层抽样，采样第一个集合，包含 $N_c$ 个位置点，用上述 $t_i$ 与 $C(\mathbf{r})$ 方程计算粗糙网络，并根据粗糙网络的输出，沿着光线生成更明智的采样点（更偏向与体积相关的部分）。在上述粗网格下，重写颜色公式：</p><script type="math/tex; mode=display">C_c(\mathbf{r}) = \sum_{i=1}^{N_c} \omega_i \mathbf{c}_i,\qquad \omega_i=T_i(1-e^{-\sigma_i \delta_i})</script><p>再归一化权重</p><script type="math/tex; mode=display">\hat{\omega_i}=\frac{\omega_i}{\sum_{i=1}^{N_c} \omega_i},</script><p>这样就可以把上式颜色公式看作是沿着光线的分段连续概率密度函数(PDF)，可以粗略的估计此光线上物体的分布情况。</p></li><li><p>进一步，使用逆变换采样（inverse transform sampling），从上述分布采样出第二个、包含 $N_f$ 个位置点，并在第一个采样点集合和第二个采样点集合的并集上计算精细网络，最后利用上述 $N_c+N_f$ 个采样点计算最终的光线颜色 $C_f(\mathbf{r})$。</p></li></ul><p>注：这种方法，可以分配更多的样本点在包含场景内容的区域内。它解决了与重要性抽样（importance sampling）相同的目标，但论文使用采样值作为整个积分域的非均匀离散化，而不是将每个样本视为整个积分的独立概率估计。</p><h2 id="Implementation-details"><a href="#Implementation-details" class="headerlink" title="Implementation details"></a>Implementation details</h2><ul><li><p>训练损失函数：</p><script type="math/tex; mode=display">\mathcal{L}=\sum_{\mathbf{r}\in\mathcal{R}} [\parallel C_c(\mathbf{r}) -C(\mathbf{r})\parallel^2_2+ \parallel C_f(\mathbf{r}) -C(\mathbf{r})\parallel^2_2]</script><p>注意到上述同时也最小化了 $C_c(\mathbf{r})$ 的损失，以便粗网络的权重分布可用于在细网络中分配样本。</p></li><li><p>论文中 $N_c=64$，$N_f=128$；</p></li><li><p>使用 Adam optimizer（优化器），学习率（learning rate）从 $5\times 10^{-4}$ 指数下降到 $5\times 10^{-5}$，其他优化器参数 $\beta_1=0.9,\beta_2=0.999,\epsilon=10^{-7}$。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeRF </tag>
            
            <tag> Paper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>体渲染相关知识</title>
      <link href="/2022/11/17/volume.html"/>
      <url>/2022/11/17/volume.html</url>
      
        <content type="html"><![CDATA[<h1 id="体渲染相对于表面渲染优势"><a href="#体渲染相对于表面渲染优势" class="headerlink" title="体渲染相对于表面渲染优势"></a>体渲染相对于表面渲染优势</h1><p>表面渲染和体渲染技术上最核心的区别在于：<strong>光线是否会在物体内部发生作用</strong>。体渲染会考虑光线在物体内部的传播，表面渲染的光线仅仅在物体表面发生作用。这导致的核心应用区别是：表面渲染只适合于渲染漫反射材质、金属材质、塑料材质、镜面材质等。但对于玉石、烟雾等半透明材质，只有可微体渲染才能支持。同时，体渲染的渲染代价要高于表面渲染。</p><h1 id="体素"><a href="#体素" class="headerlink" title="体素"></a>体素</h1><p>每一个体素都代表着空间中的一个单位，尽管可以把体素想象成一个很小的立方体，但更应该把每个体素认为是从连续的三维信号中对一个无穷小的点采样得到的样本。我们关注的体渲染一般可以理解为：刻画空间中每个体素的密度与颜色值。</p><h1 id="体渲染积分"><a href="#体渲染积分" class="headerlink" title="体渲染积分"></a>体渲染积分</h1><p>最基础的、也是我们最常用的体渲染算法是 <code>Ray-Casting</code> 算法，这是一种逐像素的直接体渲染算法：<strong>对要渲染的图片中的每一个像素，从眼睛发出一束光线，传过像素中心，进入体内部，然后对沿着射线遇到的体密度进行光学属性的积分（数值）。</strong>注意到，这种通用的描述假定了体和映射到体上的光学属性是连续的。在实际中，体素是离散的，并且积分的计算结果是数值近似的。</p><ul><li>对于一条光线 $\mathbf{x}(t)=\mathbf{o}+t\mathbf{v}\in\mathbb{R}^3$，$\mathbf{o}$ 为光线的原点，$\mathbf{v}$ 为光线的方向，利用距离 $t$ 进行参数化；</li><li>对沿光线的某个位置对应的标量，记为 $s(\mathbf{x}(t))\in\mathbb{R}$，常用的比如体密度、深度；</li><li>光线吸收系数 $k(t)=k(s(\mathbf{x}(t)))$；</li><li>辐射颜色 $c(t)=c(s(\mathbf{x}(t)))$；</li></ul><p>常用的光照模型认为粒子会发光，也会遮挡（吸收）到来的光，但是没有散射或间接光。所以此时体渲染方程即对吸收系数 $k(t)$、辐射颜色 $c(t)$ 沿着光线进行积分，即得某位置上单个粒子到达渲染图片的颜色为：</p><script type="math/tex; mode=display">c^\prime(t)=c(t)\cdot e^{-\int_0^tk(y)dy}.</script><p>当考虑到这条射线上所有可能位置 $t$ 发出的辐射总和，即有最终渲染：</p><script type="math/tex; mode=display">C^\prime=\int_0^\infty c(t)\cdot e^{-\int_0^tk(y)dy}dt.</script><p>离散化上述积分：</p><script type="math/tex; mode=display">C^\prime=\sum_i c(t_i) e^{-\sum_j k(j\cdot\Delta t)\Delta t}.</script><p>实际上，体渲染积分是通过以从前到后或从后到前的 <code>Alpha混合（合成）</code> 的方式来近似求解的。（下述记号是以均匀采样标写的，所以 $\delta<em>i=t</em>{i+1}-t_i=\Delta t$）</p><ul><li>累积透射度 $T(t)=e^{-\int_0^t k(y)dy}$，离散情形 $T_i=e^{-\sum_j k(j\cdot\Delta t)\Delta t}$；</li><li>累积不透明度 $A(t)=1-T(t)$，离散情形 $A_i=1-T_i$；</li><li>辐射颜色离散 $c_i=c(t_i)\Delta t$；</li></ul><p>此时最终辐射总和计算为：</p><script type="math/tex; mode=display">C^\prime=\sum_i c_i T_i.</script><p>上述这个公式能通过以从后到前或从前到后的顺序的 <code>Alpha混合（合成）</code> 来迭代计算。</p><h1 id="Alpha合成"><a href="#Alpha合成" class="headerlink" title="Alpha合成"></a>Alpha合成</h1><p>上述那个公式能通过以从后到前使用 $n-1$ 到 $0$ 的 $i$ 来迭代计算：</p><script type="math/tex; mode=display">C^\prime_i=c_i+(1-A_i)\cdot C^\prime_{i+1}.</script><p>新值 $C<em>i^\prime$ 可以由当前位置 $i$ 的颜色 $C_i$ 和不透明度 $A_i$ 以及前一个位置 $i+1$ 的复合颜色 $C</em>{i+1}^\prime$ 来算出。起始条件是 $C_n^\prime=0$。</p><ul><li>注意到，在所有混合方程中，我们使用带有不透明度权重的颜色，也就是所说的 <code>associated colors</code>。带有不透明度权重的颜色是已经提前乘好他们相关的不透明度的颜色。这是一种十分便利的符号，在用于插值时尤其重要。可以证明，对颜色和不透明度分别进行插值后会造成失真，但对不透明度加权的颜色进行插值就会得到正确的结果。</li></ul><p>下面是可选的从前到后顺序的迭代公式，$i$ 从 $1$ 递增至 $n$：</p><script type="math/tex; mode=display">C^\prime_i=c_{i-1}+(1-A_{i-1})\cdot C^\prime_{i} \\A^\prime_i=A_{i-1}^\prime+(1-A_{i-1}^\prime)\cdot A^\prime_{i}.</script><p>新值 $C<em>i^\prime,A_i^\prime$ 可以由当前位置 $i$ 的颜色 $C_i$ 和不透明度 $A_i$ 以及前一个位置 $i-1$ 的复合颜色 $C</em>{i-1}^\prime$ 和不透明度 $A_{i-1}^\prime$ 来算出。起始条件是 $C_0^\prime=0,A^\prime_0=0$。</p><ul><li>注意到从前到后复合需要追踪 <code>alpha</code> 的值，然而从后到前组合不需要。在硬件实现中，使用从前到后的复合的话，意味着终点 <code>alpha</code> 必须被帧缓冲所支持（比如一个 <code>alpha</code> 的值必须存在帧缓冲中，然后它必须能在混合操作中用作乘数）。然而，由于从前到后复合的主要优点是一种叫做提前射线终止的常见优化方法，这种方法会在沿射线累计的 <code>alpha</code> 到达 $0$ 时，将过程立刻终止。这在硬件上是难以执行的，使用 GPU 的体渲染通常使用从后到前的复合。</li></ul><h1 id="NeRF中的体渲染"><a href="#NeRF中的体渲染" class="headerlink" title="NeRF中的体渲染"></a>NeRF中的体渲染</h1><p>（综合 NeRF 原文和其他论文中的理解）</p><p>我们考虑恢复体素的颜色与密度：</p><ul><li>体密度 $\sigma(\mathbf{x}(t))$；</li><li>累积透射度 $T(t)=e^{-\int_0^t\sigma(\mathbf{x}(y))dy}$；</li><li>累积不透明度 $A(t)=1-T(t)$，将 $A(t)$ 看作是一个累积分布函数；</li><li>概率密度函数（PDF）$\tau(t)=\frac{d T(t)}{d t}=\sigma(\mathbf{x}(t))T(t)$；</li></ul><p>从而最终合成颜色为：</p><script type="math/tex; mode=display">C(\mathbf{o,v})=\int_0^{+\infty} c(\mathbf{x}(t),\mathbf{v})\cdot\sigma(\mathbf{x}(t))T(t) dt.</script>]]></content>
      
      
      <categories>
          
          <category> Basic Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Basic Knowledge </tag>
            
            <tag> NeRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NeRF数据采集与处理</title>
      <link href="/2022/11/16/nerf.html"/>
      <url>/2022/11/16/nerf.html</url>
      
        <content type="html"><![CDATA[<h1 id="拍摄要求"><a href="#拍摄要求" class="headerlink" title="拍摄要求"></a>拍摄要求</h1><ul><li>拍摄尽可能同一时间段拍完，避免巨变的光照条件变化、曝光失常、焦点失焦等问题；</li><li>可以选择拍摄图片，但需保证不同图片的相机焦距一定，避免手机滤镜、美颜等磨去纹理等；<strong>建议</strong>拍摄视频，可以保证相邻图片直接连续性；</li><li>小物件，可以物品为球心，在半球式多视角拍摄图片；对于较大物品，无法做到半球式拍摄顶部，可以柱面或部分球面的形式拍摄，但重建效果对于未采集区域必然是不好的；</li></ul><h1 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h1><p>（此处我提供了一些 <code>python</code> 脚本处理，例如抽帧、降分辨率等，具体请见：脚本积累）</p><ul><li>拍摄为视频时：保持镜头的连续性，以一定间隔进行抽帧（保证相邻图片特征匹配良好），并剔除失焦模糊、曝光失常等无效图片；</li><li>必要时可以将图片同比例的缩小；</li><li>（可选）如果只想单独重建物体，可以进行图像分割，将想要重建的物品抠图出来（PS、<a href="https://github.com/hkchengrex/MiVOS/tree/MiVOS-STCN">MiVOS-STCN</a>等方法）；</li></ul><h1 id="相机内外参"><a href="#相机内外参" class="headerlink" title="相机内外参"></a>相机内外参</h1><p>我们利用 <a href="(http://colmap.github.io/">colmap</a>) 图形界面获取相机内外参：</p><ul><li><p>最好这里保证如下文件结构：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+ --- scene               <span class="comment"># 待合成场景</span></span><br><span class="line">|     + --- image        <span class="comment"># 图片文件夹</span></span><br><span class="line">|     + --- mask         <span class="comment"># 如果进行了语义分割，mask文件夹</span></span><br><span class="line">|     + --- database.db  <span class="comment"># 从此开始,下述文件为位姿获取时生成</span></span><br><span class="line">|     + --- sparse       <span class="comment"># 相机位姿的二进制文件</span></span><br><span class="line">|     + --- text         <span class="comment"># 相机外参的.txt格式文件 </span></span><br><span class="line">|     + --- project.ini</span><br></pre></td></tr></table></figure></li><li><p>菜单栏 <code>File-&gt;New project</code>，第一栏创建 <code>database.db</code> 二进制进程文件，第二栏添加 <code>image</code> 图片文件夹路径；</p></li><li><p>菜单栏 <code>Processing-&gt;Feature extraction</code>，将 <code>camera model</code> 调整为 <code>PINHOLE</code>，选中下面的 <code>Shared for all images</code>（这也就是为什么我们要求不要改变内参，避免不必要的麻烦）。（如果有 <code>mask</code>，可以导入对应文件夹路径；如果有多个 GPU，可以在下面选择 <code>gpu_index</code> 参数）最后点击 <code>Extract</code> 提取特征；</p></li><li><p>菜单栏 <code>Processing-&gt;Feature matching-&gt;Exhaustive-&gt;Run</code>；</p></li><li><p>菜单栏 <code>Reconstruction-&gt;Automatic reconstruction</code>，<code>Workspace</code> 导入 <code>scene</code> 文件夹，<code>Image folder</code> 导入<code>image</code> 文件夹，选中 <code>Shared intrinsics</code>，<strong>取消</strong> <code>Dense model</code>，然后 <code>Run</code>；</p></li><li><p>直观的，你可以导出看看相机内外参，<code>File-&gt;Export model as text</code>；</p></li><li><p>关闭前会让你保存一个 <code>project.ini</code> 文件。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> NeRF </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NeRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>脚本积累</title>
      <link href="/2022/11/15/script.html"/>
      <url>/2022/11/15/script.html</url>
      
        <content type="html"><![CDATA[<p><strong>此博客用来存放、并简单介绍相关数据处理等脚本。</strong></p><h2 id="视频抽帧"><a href="#视频抽帧" class="headerlink" title="视频抽帧"></a>视频抽帧</h2><p><a href="/code/video2images.py">video2images.py</a>：以一定间隔从视频内提取帧；</p><ul><li><p>在第6行修改视频格式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> input_path.endswith(<span class="string">&#x27;.MP4&#x27;</span>):</span><br></pre></td></tr></table></figure></li><li><p>在第19行修改图片大小（尽量保持原比例缩放）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">frame_n = cv2.resize(frame, (<span class="number">1920</span>, <span class="number">1080</span>)) <span class="comment"># (W,H)</span></span><br></pre></td></tr></table></figure></li><li><p>第28、29、30分别为视频输入路径、图片储存文件夹、间隔帧数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input_path = <span class="string">&quot;/path/to/your/video&quot;</span>     <span class="comment"># 视频输入路径</span></span><br><span class="line">save_path = <span class="string">&quot;/path/to/your/image/&quot;</span>     <span class="comment"># 提取图片输出路径</span></span><br><span class="line">frame_interval = <span class="number">10</span>                    <span class="comment"># 抽帧间隔数</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="图片批量重命名"><a href="#图片批量重命名" class="headerlink" title="图片批量重命名"></a>图片批量重命名</h2><p><a href="/code/rename.py">rename.py</a>：将文件夹家内以一定顺序重命名；</p><ul><li><p>第4、5、6行分别为原始图片文件夹、重命名后文件夹、重命名后第一个图片序号</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">path = <span class="string">&#x27;/path/to/your/image/&#x27;</span>          <span class="comment"># 文件夹路径</span></span><br><span class="line">out_path = <span class="string">&#x27;/path/to/your/new/image/&#x27;</span>  <span class="comment"># 新文件夹路径</span></span><br><span class="line">start = <span class="number">0</span>                              <span class="comment"># 重命名后第一个图片序号</span></span><br></pre></td></tr></table></figure></li><li><p>第9行为命名原则</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">name = <span class="string">&quot;%05d&quot;</span> % start                  <span class="comment">#5位数 不足前面补零</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="依据mask图像分割"><a href="#依据mask图像分割" class="headerlink" title="依据mask图像分割"></a>依据mask图像分割</h2><p><a href="/code/extract.py">extract.py</a>：这里给出的是依据 <a href="https://github.com/hkchengrex/MiVOS/tree/MiVOS-STCN">MiVOS-STCN</a> 算法分割给出的（红色）mask进行图像分割，输出白色mask、分割后的图片；<strong>需要保持mask与原始图片名字一致！</strong></p><ul><li><p>只需修改第21—24行相关路径即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">original_path = <span class="string">&#x27;/path/to/ori/image/&#x27;</span> <span class="comment"># 原始图片路径</span></span><br><span class="line">ori_mask_path = <span class="string">&#x27;/path/to/ori/mask/&#x27;</span>  <span class="comment"># 原始红色mask路径</span></span><br><span class="line">images_path   = <span class="string">&#x27;/path/to/new/image/&#x27;</span> <span class="comment"># 分割后图片路径</span></span><br><span class="line">masks_path    = <span class="string">&#x27;/path/to/new/image/&#x27;</span> <span class="comment"># 修改颜色后mask路径</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="调整图像分辨率"><a href="#调整图像分辨率" class="headerlink" title="调整图像分辨率"></a>调整图像分辨率</h2><p><a href="/code/lowresolution.py">lowresolution.py</a>：降低图像分辨率</p><ul><li><p>修改5、6行路径</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sourceDir = os.path.join(curDir, <span class="string">&#x27;/path/to/ori/image/&#x27;</span>)</span><br><span class="line">resultDir = os.path.join(curDir, <span class="string">&#x27;/path/to/new/image/&#x27;</span>)</span><br></pre></td></tr></table></figure></li><li><p>第13行修改到指定分辨率（尽量保持原始比例）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pic_n = cv2.resize(pic, (<span class="number">960</span>, <span class="number">540</span>)) <span class="comment"># (W,H)</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="修改图片通道数"><a href="#修改图片通道数" class="headerlink" title="修改图片通道数"></a>修改图片通道数</h2><p><a href="/code/rgb2rgba.py">rgb2rgba.py</a>：有些项目需要<code>RGB</code>通道，有些则需要<code>RGBA</code>通道</p><ul><li><p>修改4、5行路径</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">path      = <span class="string">&quot;/path/to/ori/image/&quot;</span>     <span class="comment"># 原始路径</span></span><br><span class="line">save_path = <span class="string">&#x27;/path/to/new/image/&#x27;</span>     <span class="comment"># 保存路径</span></span><br></pre></td></tr></table></figure></li><li><p>第13行可以修改转换后通道数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pimg = img.convert(<span class="string">&quot;RGB&quot;</span>)  <span class="comment"># 4通道转化为rgb三通道</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="提取colmap匹配的有效图片"><a href="#提取colmap匹配的有效图片" class="headerlink" title="提取colmap匹配的有效图片"></a>提取colmap匹配的有效图片</h2><p><a href="/code/valid_imgs_from_imgstxt.py">valid_imgs_from_imgstxt.py</a>：colmap获取位姿可能因为某些原因造成部分图片无法匹配，这对有些项目影响很大！造成代码一些数组序号出问题、甚至可能无法运行，这里建议提取有效图片</p><ul><li><p>修改第25—27行相关路径即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">file_name   = <span class="string">&quot;/path/to/colmap/text/images.txt&quot;</span> <span class="comment"># images.txt 文件</span></span><br><span class="line">input_path  = <span class="string">&quot;/path/to/ori/image/&quot;</span>             <span class="comment"># 原始图片路径</span></span><br><span class="line">output_path = <span class="string">&quot;/path/to/valid/image/&quot;</span>           <span class="comment"># 提取后有效图片路径</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="图像转GIF"><a href="#图像转GIF" class="headerlink" title="图像转GIF"></a>图像转GIF</h2><p><a href="/code/image2gif.py">image2gif.py</a>：将一定顺序的图片序列转换为GIF动图</p><ul><li><p>修改13—15行相关路径</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">img_dir  = <span class="string">&#x27;/path/to/image&#x27;</span>  <span class="comment"># 图片路径</span></span><br><span class="line">duration = <span class="number">0.05</span>              <span class="comment"># 图片间隔,每秒20帧,即1/20</span></span><br><span class="line">gif_name = img_dir+<span class="string">&#x27;.gif&#x27;</span>    <span class="comment"># 输出gif名字</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="图像转视频"><a href="#图像转视频" class="headerlink" title="图像转视频"></a>图像转视频</h2><p><a href="/code/image2video.py">image2video.py</a>：将一定顺序的图片序列转换为视频</p><ul><li><p>修改16行帧率、17行视频存储路径及名字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fps = <span class="number">30</span></span><br><span class="line">file_path = <span class="string">r&quot;/pathto/video/name.mp4&quot;</span> </span><br></pre></td></tr></table></figure></li><li><p>第30行图片路径及分辨率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">picvideo(<span class="string">r&#x27;/path/to/image/&#x27;</span>, (<span class="number">1920</span>, <span class="number">1080</span>)) <span class="comment"># (W,H)</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="相机位姿添加噪声"><a href="#相机位姿添加噪声" class="headerlink" title="相机位姿添加噪声"></a>相机位姿添加噪声</h2><p><a href="/code/add_noise_in_txt.py">add_noise_in_txt.py</a>：直接给colamp跑出的 <code>image.txt</code> 添加高斯噪声</p><ul><li><p>修改第9、10行噪声方差</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sigma_r = <span class="number">5.0</span> <span class="comment"># 旋转矩阵的噪声方差</span></span><br><span class="line">sigma_t = <span class="number">0.1</span> <span class="comment"># 位移向量的噪声方差</span></span><br></pre></td></tr></table></figure></li><li><p>修改第49、50行相关路径</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">file_name = <span class="string">&quot;/path/to/colmap/text/images.txt&quot;</span> <span class="comment"># 原始colmap的image.tx</span></span><br><span class="line">output_name = <span class="string">&quot;/path/to/new/images.txt&quot;</span>       <span class="comment"># 添加噪声后的image.txt路径</span></span><br></pre></td></tr></table></figure></li></ul><p><a href="/code/add_noise_in_json.py">add_noise_in_json.py</a>：给 <code>tansform.json</code> 添加高斯噪声</p><ul><li><p>修改第41、42行噪声方差</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sigma_r = <span class="number">5.0</span> <span class="comment"># 旋转矩阵的噪声方差</span></span><br><span class="line">sigma_t = <span class="number">0.1</span> <span class="comment"># 位移向量的噪声方差</span></span><br></pre></td></tr></table></figure></li><li><p>修改第98、99行相关路径</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">file_name = <span class="string">&quot;/path/to/transform.json&quot;</span>         <span class="comment"># 原始位姿</span></span><br><span class="line">output_name = <span class="string">&quot;/path/to/new/transform.json&quot;</span>   <span class="comment"># 添加噪声后</span></span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Code </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Script </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch基本教程</title>
      <link href="/2022/11/14/pytorch.html"/>
      <url>/2022/11/14/pytorch.html</url>
      
        <content type="html"><![CDATA[<p><strong>此博客以机器学习中的一些简单模型为基础，学习Pytorch的基本架构，不做过多深入每个函数的研究与探讨，这部分只需要面向具体的项目代码即可！文章最后面会附上一些相关链接用来参考学习。</strong></p><h1 id="Pytorch基础：Tensor（张量）"><a href="#Pytorch基础：Tensor（张量）" class="headerlink" title="Pytorch基础：Tensor（张量）"></a>Pytorch基础：Tensor（张量）</h1><p>对于张量（Tensor），就可以理解为多为矩阵，也只是一种特别的存储方式而已，当然也可以表示一个元素的张量（或，标量）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.tensor([<span class="number">3.1433223</span>]) </span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br><span class="line">tensor.size()</span><br><span class="line">tensor.item()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">3.1433</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>])</span><br><span class="line"><span class="number">3.143322229385376</span></span><br></pre></td></tr></table></figure><h2 id="Tensor-的基本类型"><a href="#Tensor-的基本类型" class="headerlink" title="Tensor 的基本类型"></a>Tensor 的基本类型</h2><p>Tensor的基本数据类型有五种： </p><ul><li>32位浮点型：torch.FloatTensor。 (默认) </li><li>64位整型：torch.LongTensor。</li><li>32位整型：torch.IntTensor。</li><li>16位整型：torch.ShortTensor。</li><li>64位浮点型：torch.DoubleTensor。</li></ul><p>除以上数字类型外，还有 byte和chart型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">3.1426</span>], dtype=torch.float16)</span><br></pre></td></tr></table></figure><h2 id="设备转换"><a href="#设备转换" class="headerlink" title="设备转换"></a>设备转换</h2><p>相对于 <code>Numpy</code> 中多维矩阵的表示 <code>ndarray</code> 只能在 CPU 上运行，<code>Tensor</code> 可以在 GPU 上运行。所以有时候需要统一好每个 Tensor 所在设备。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用torch.cuda.is_available()来确定是否有cuda设备</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(device)</span><br><span class="line"><span class="comment">#将tensor传送到设备</span></span><br><span class="line">gpu_b=cpu_b.to(device)</span><br><span class="line">gpu_b.<span class="built_in">type</span>()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cuda</span><br><span class="line"><span class="string">&#x27;torch.cuda.FloatTensor&#x27;</span></span><br></pre></td></tr></table></figure><h1 id="Autograd计算梯度数值"><a href="#Autograd计算梯度数值" class="headerlink" title="Autograd计算梯度数值"></a>Autograd计算梯度数值</h1><p>在创建张量时，可以通过设置 <code>requires_grad=True</code>来告诉 Pytorch 对该张量进行自助求导，Pytorch 会记录该张量的每一步操作历史并自动计算梯度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">x</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[0.0403, 0.5633, 0.2561, 0.4064, 0.9596],</span></span><br><span class="line"><span class="comment">#         [0.6928, 0.1832, 0.5380, 0.6386, 0.8710],</span></span><br><span class="line"><span class="comment">#         [0.5332, 0.8216, 0.8139, 0.1925, 0.4993],</span></span><br><span class="line"><span class="comment">#         [0.2650, 0.6230, 0.5945, 0.3230, 0.0752],</span></span><br><span class="line"><span class="comment">#         [0.0919, 0.4770, 0.4622, 0.6185, 0.2761]], requires_grad=True)</span></span><br><span class="line"></span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">z=x**<span class="number">2</span>+y**<span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[3.3891e-01, 4.9468e-01, 8.0797e-02, 2.5656e-01, 2.9529e-01],</span></span><br><span class="line"><span class="comment">#        [7.1946e-01, 1.6977e-02, 1.7965e-01, 3.2656e-01, 1.7665e-01],</span></span><br><span class="line"><span class="comment">#        [3.1353e-01, 2.2096e-01, 1.2251e+00, 5.5087e-01, 5.9572e-02],</span></span><br><span class="line"><span class="comment">#        [1.3015e+00, 3.8029e-01, 1.1103e+00, 4.0392e-01, 2.2055e-01],</span></span><br><span class="line"><span class="comment">#        [8.8726e-02, 6.9701e-01, 8.0164e-01, 9.7221e-01, 4.2239e-04]],</span></span><br><span class="line"><span class="comment">#       grad_fn=&lt;AddBackward0&gt;)</span></span><br></pre></td></tr></table></figure><p>在张量进行操作后，<code>grad_fn</code> 已经被赋予了一个新的函数，这个函数引用了一个创建了这个 Tensor 类的 Function 对象。 Tensor 和 Function 互相连接生成了一个非循环图，它记录并且编码了完整的计算历史。每个张量都有一个 <code>`.grad_fn</code> 属性，如果这个张量是用户手动创建的那么这个张量的 <code>grad_fn</code> 是 <code>None</code>。</p><p>当计算完成后调用 <code>.backward()</code> 方法自动计算梯度并且将计算结果保存到 <code>grad</code> 属性中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">z.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad,y.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.]]) </span></span><br><span class="line"><span class="comment"># tensor([[1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1., 1.]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们的返回值不是一个标量，所以需要输入一个大小相同的张量作为参数，</span></span><br><span class="line"><span class="comment"># 这里我们用ones_like函数根据x生成一个张量</span></span><br><span class="line">z.backward(torch.ones_like(x))</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([[0.2087, 1.3554, 0.5560, 1.0009, 0.9931],</span></span><br><span class="line"><span class="comment">#        [1.2655, 0.1223, 0.8008, 1.1127, 0.7261],</span></span><br><span class="line"><span class="comment">#        [1.1052, 0.2579, 1.8006, 0.1544, 0.3646],</span></span><br><span class="line"><span class="comment">#        [1.8855, 1.2296, 1.9061, 0.9313, 0.0648],</span></span><br><span class="line"><span class="comment">#        [0.5952, 1.6190, 0.8430, 1.9213, 0.0322]])</span></span><br></pre></td></tr></table></figure><p>我们可以使用 <code>with torch.no_grad()</code> 上下文管理器临时禁止对已设置 <code>requires_grad=True</code> 的张量进行自动求导。这个方法在测试集计算准确率的时候会经常用到，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="built_in">print</span>((x +y*<span class="number">2</span>).requires_grad)</span><br><span class="line">   </span><br><span class="line"><span class="comment"># False</span></span><br></pre></td></tr></table></figure><p>使用 <code>.no_grad()</code> 进行嵌套后，代码不会跟踪历史记录，也就是说保存的这部分记录会减少内存的使用量并且会加快少许的运算速度。</p><h1 id="数据的加载和预处理"><a href="#数据的加载和预处理" class="headerlink" title="数据的加载和预处理"></a>数据的加载和预处理</h1><p>PyTorch通过 <code>torch.utils.data</code> 对一般常用的数据加载进行了封装，可以很容易地实现多线程数据预读和批量加载。 并且 <code>torchvision</code> 已经预先实现了常用图像数据集，包括前面使用过的CIFAR-10，ImageNet、COCO、MNIST、LSUN等数据集，可通过 <code>torchvision.datasets</code> 方便的调用。这里不具体介绍。</p><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>Dataset 是一个抽象类，为了能够方便的读取，需要将要使用的数据包装为 Dataset 类。 自定义的 Dataset 需要继承它并且实现两个成员方法： </p><pre><code>- `__getitem__()` 该方法定义用索引(`0` 到 `len(self)`)获取一条数据或一个样本-  `__len__()` 该方法返回数据集的总长度</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#引用</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义一个数据集</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BulldozerDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 数据集演示 &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, csv_file</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;实现初始化方法，在初始化的时候将数据读载入&quot;&quot;&quot;</span></span><br><span class="line">        self.df=pd.read_csv(csv_file)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        返回df的长度</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.df)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        根据 idx 返回一行数据</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="keyword">return</span> self.df.iloc[idx].SalePrice</span><br><span class="line"><span class="comment"># ------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># 实例化一个对象访问它</span></span><br><span class="line">ds_demo= BulldozerDataset(<span class="string">&#x27;median_benchmark.csv&#x27;</span>)</span><br><span class="line"><span class="comment"># 实现了 __len__ 方法所以可以直接使用len获取数据总数</span></span><br><span class="line"><span class="built_in">len</span>(ds_demo)</span><br><span class="line"><span class="comment"># 用索引可以直接访问对应的数据，对应 __getitem__ 方法</span></span><br><span class="line">ds_demo[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>自定义的数据集已经创建好了，下面我们使用官方提供的数据载入器，读取数据。</p><h2 id="Dataloader"><a href="#Dataloader" class="headerlink" title="Dataloader"></a>Dataloader</h2><p>DataLoader 为我们提供了对 Dataset 的读取操作，常用参数有：</p><ul><li>batch_size(每个batch的大小)、 </li><li>shuffle(是否进行shuffle操作)、 </li><li>num_workers(加载数据的时候使用几个子进程)。</li></ul><p>下面做一个简单的操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">dl = torch.utils.data.DataLoader(ds_demo, batch_size=<span class="number">10</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># DataLoader返回的是一个可迭代对象，我们可以使用迭代器分次获取数据</span></span><br><span class="line">idata=<span class="built_in">iter</span>(dl)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(idata))</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000., 24000.], dtype=torch.float64)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 常见的用法是使用for循环对其进行遍历</span></span><br><span class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(dl):</span><br><span class="line">    <span class="built_in">print</span>(i,data)</span><br></pre></td></tr></table></figure><p>我们已经可以通过 Dataset 定义数据集，并使用 Datalorder 载入和遍历数据集，除了这些以外， PyTorch 还提供能 torcvision 的计算机视觉扩展包。</p><h2 id="torchvision-包"><a href="#torchvision-包" class="headerlink" title="torchvision 包"></a>torchvision 包</h2><p>torchvision 是PyTorch中专门用来处理图像的库。</p><h3 id="torchvision-datasets"><a href="#torchvision-datasets" class="headerlink" title="torchvision.datasets"></a>torchvision.datasets</h3><p>torchvision.datasets 可以理解为PyTorch团队自定义的dataset，这些dataset帮我们提前处理好了很多的图片数据集，我们拿来就可以直接使用： - MNIST - COCO - Captions - Detection - LSUN - ImageFolder - Imagenet-12 - CIFAR - STL10 - SVHN - PhotoTour 我们可以直接使用，示例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> datasets</span><br><span class="line">trainset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, <span class="comment"># 表示 MNIST 数据的加载的目录</span></span><br><span class="line">                          train=<span class="literal">True</span>,    <span class="comment"># 表示是否加载数据库的训练集，false的时候加载测试集</span></span><br><span class="line">                          download=<span class="literal">True</span>, <span class="comment"># 表示是否自动下载 MNIST 数据集</span></span><br><span class="line">                          transform=<span class="literal">None</span>)<span class="comment"># 表示是否需要对数据进行预处理，none为不进行预处理</span></span><br></pre></td></tr></table></figure><h3 id="torchvision-models"><a href="#torchvision-models" class="headerlink" title="torchvision.models"></a>torchvision.models</h3><p>torchvision 不仅提供了常用图片数据集，还提供了训练好的模型，可以加载之后，直接使用，或者在进行迁移学习 <code>torchvision.models</code> 模块的 子模块中包含以下模型结构。 - AlexNet - VGG - ResNet - SqueezeNet - DenseNet</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#我们直接可以使用训练好的模型，当然这个与datasets相同，都是需要从服务器下载的</span></span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line">resnet18 = models.resnet18(pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="torchvision-transforms"><a href="#torchvision-transforms" class="headerlink" title="torchvision.transforms"></a>torchvision.transforms</h3><p>transforms 模块提供了一般的图像转换操作类，用作数据处理和数据增强</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="keyword">as</span> transforms</span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.RandomCrop(<span class="number">32</span>, padding=<span class="number">4</span>), <span class="comment">#先四周填充0，在把图像随机裁剪成32*32</span></span><br><span class="line">    transforms.RandomHorizontalFlip(),    <span class="comment">#图像一半的概率翻转，一半的概率不翻转</span></span><br><span class="line">    transforms.RandomRotation((-<span class="number">45</span>,<span class="number">45</span>)),  <span class="comment">#随机旋转</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>), (<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>)), <span class="comment">#R,G,B每层的归一化用到的均值和方差</span></span><br><span class="line">])</span><br></pre></td></tr></table></figure><p>肯定有人会问：(0.485, 0.456, 0.406), (0.2023, 0.1994, 0.2010) 这几个数字是什么意思？官方的这个帖子有详细的说明: <a href="https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/21">https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/21</a> 这些都是根据ImageNet训练的归一化参数，可以直接使用，我们认为这个是固定值就可以。</p><h1 id="神经网络包nn和优化器optimizer"><a href="#神经网络包nn和优化器optimizer" class="headerlink" title="神经网络包nn和优化器optimizer"></a>神经网络包nn和优化器optimizer</h1><p><code>torch.nn</code> 是专门为神经网络设计的模块化接口。<code>nn</code> 构建于 <code>Autograd</code> 之上，可用来定义和运行神经网络。 这里我们主要介绍几个一些常用的类。除了nn别名以外，我们还引用了nn.functional，这个包中包含了神经网络中使用的一些常用函数，这些函数的特点是，不具有可学习的参数(如ReLU，pool，DropOut等)，这些函数可以放在构造函数中，也可以不放，但是这里建议不放。一般情况下我们会<strong>将nn.functional 设置为大写的F</strong>，这样缩写方便调用。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 首先要引入相关的包</span><br><span class="line">import torch</span><br><span class="line"># 引入torch.nn并指定别名</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br></pre></td></tr></table></figure><h2 id="定义一个网络"><a href="#定义一个网络" class="headerlink" title="定义一个网络"></a>定义一个网络</h2><p>PyTorch 中已经为我们准备好了现成的网络模型，只要继承 <code>nn.Module</code>，并实现它的 <code>forward</code> 方法，PyTorch 会根据 <code>autograd</code>，自动实现 <code>backward</code> 函数，在 <code>forward</code> 函数中可使用任何 tensor 支持的函数，还可以使用 if、for 循环、print、log 等 Python 语法，写法和标准的 Python 写法一致。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># nn.Module子类的函数必须在构造函数中执行父类的构造函数</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 卷积层 &#x27;1&#x27;表示输入图片为单通道， &#x27;6&#x27;表示输出通道数，&#x27;3&#x27;表示卷积核为3*3</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">3</span>) </span><br><span class="line">        <span class="comment">#线性层，输入1350个特征，输出10个特征</span></span><br><span class="line">        self.fc1   = nn.Linear(<span class="number">1350</span>, <span class="number">10</span>)  <span class="comment">#这里的1350是如何计算的呢？这就要看后面的forward函数</span></span><br><span class="line">    <span class="comment">#正向传播 </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): </span><br><span class="line">        <span class="built_in">print</span>(x.size()) <span class="comment"># 结果：[1, 1, 32, 32]</span></span><br><span class="line">        <span class="comment"># 卷积 -&gt; 激活 -&gt; 池化 </span></span><br><span class="line">        x = self.conv1(x) <span class="comment">#根据卷积的尺寸计算公式，计算结果是30，具体计算公式后面第二章第四节 卷积神经网络 有详细介绍。</span></span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        <span class="built_in">print</span>(x.size()) <span class="comment"># 结果：[1, 6, 30, 30]</span></span><br><span class="line">        x = F.max_pool2d(x, (<span class="number">2</span>, <span class="number">2</span>)) <span class="comment">#我们使用池化层，计算结果是15</span></span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        <span class="built_in">print</span>(x.size()) <span class="comment"># 结果：[1, 6, 15, 15]</span></span><br><span class="line">        <span class="comment"># reshape，‘-1’表示自适应</span></span><br><span class="line">        <span class="comment">#这里做的就是压扁的操作 就是把后面的[1, 6, 15, 15]压扁，变为 [1, 1350]</span></span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], -<span class="number">1</span>) </span><br><span class="line">        <span class="built_in">print</span>(x.size()) <span class="comment"># 这里就是fc1层的的输入1350 </span></span><br><span class="line">        x = self.fc1(x)        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Net(</span></span><br><span class="line"><span class="comment">#   (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=1350, out_features=10, bias=True)</span></span><br><span class="line"><span class="comment"># )</span></span><br></pre></td></tr></table></figure><p>网络的可学习参数通过 <code>net.parameters()</code> 返回</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> parameters <span class="keyword">in</span> net.parameters():</span><br><span class="line">    <span class="built_in">print</span>(parameters)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># ----------输出----------</span></span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[[[ <span class="number">0.2745</span>,  <span class="number">0.2594</span>,  <span class="number">0.0171</span>],</span><br><span class="line">          [ <span class="number">0.0429</span>,  <span class="number">0.3013</span>, -<span class="number">0.0208</span>],</span><br><span class="line">          [ <span class="number">0.1459</span>, -<span class="number">0.3223</span>,  <span class="number">0.1797</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ <span class="number">0.1847</span>,  <span class="number">0.0227</span>, -<span class="number">0.1919</span>],</span><br><span class="line">          [-<span class="number">0.0210</span>, -<span class="number">0.1336</span>, -<span class="number">0.2176</span>],</span><br><span class="line">          [-<span class="number">0.2164</span>, -<span class="number">0.1244</span>, -<span class="number">0.2428</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ <span class="number">0.1042</span>, -<span class="number">0.0055</span>, -<span class="number">0.2171</span>],</span><br><span class="line">          [ <span class="number">0.3306</span>, -<span class="number">0.2808</span>,  <span class="number">0.2058</span>],</span><br><span class="line">          [ <span class="number">0.2492</span>,  <span class="number">0.2971</span>,  <span class="number">0.2277</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ <span class="number">0.2134</span>, -<span class="number">0.0644</span>, -<span class="number">0.3044</span>],</span><br><span class="line">          [ <span class="number">0.0040</span>,  <span class="number">0.0828</span>, -<span class="number">0.2093</span>],</span><br><span class="line">          [ <span class="number">0.0204</span>,  <span class="number">0.1065</span>,  <span class="number">0.1168</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ <span class="number">0.1651</span>, -<span class="number">0.2244</span>,  <span class="number">0.3072</span>],</span><br><span class="line">          [-<span class="number">0.2301</span>,  <span class="number">0.2443</span>, -<span class="number">0.2340</span>],</span><br><span class="line">          [ <span class="number">0.0685</span>,  <span class="number">0.1026</span>,  <span class="number">0.1754</span>]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ <span class="number">0.1691</span>, -<span class="number">0.0790</span>,  <span class="number">0.2617</span>],</span><br><span class="line">          [ <span class="number">0.1956</span>,  <span class="number">0.1477</span>,  <span class="number">0.0877</span>],</span><br><span class="line">          [ <span class="number">0.0538</span>, -<span class="number">0.3091</span>,  <span class="number">0.2030</span>]]]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([ <span class="number">0.2355</span>,  <span class="number">0.2949</span>, -<span class="number">0.1283</span>, -<span class="number">0.0848</span>,  <span class="number">0.2027</span>, -<span class="number">0.3331</span>],</span><br><span class="line">       requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[ <span class="number">2.0555e-02</span>, -<span class="number">2.1445e-02</span>, -<span class="number">1.7981e-02</span>,  ..., -<span class="number">2.3864e-02</span>,</span><br><span class="line">          <span class="number">8.5149e-03</span>, -<span class="number">6.2071e-04</span>],</span><br><span class="line">        [-<span class="number">1.1755e-02</span>,  <span class="number">1.0010e-02</span>,  <span class="number">2.1978e-02</span>,  ...,  <span class="number">1.8433e-02</span>,</span><br><span class="line">          <span class="number">7.1362e-03</span>, -<span class="number">4.0951e-03</span>],</span><br><span class="line">        [ <span class="number">1.6187e-02</span>,  <span class="number">2.1623e-02</span>,  <span class="number">1.1840e-02</span>,  ...,  <span class="number">5.7059e-03</span>,</span><br><span class="line">         -<span class="number">2.7165e-02</span>,  <span class="number">1.3463e-03</span>],</span><br><span class="line">        ...,</span><br><span class="line">        [-<span class="number">3.2552e-03</span>,  <span class="number">1.7277e-02</span>, -<span class="number">1.4907e-02</span>,  ...,  <span class="number">7.4232e-03</span>,</span><br><span class="line">         -<span class="number">2.7188e-02</span>, -<span class="number">4.6431e-03</span>],</span><br><span class="line">        [-<span class="number">1.9786e-02</span>, -<span class="number">3.7382e-03</span>,  <span class="number">1.2259e-02</span>,  ...,  <span class="number">3.2471e-03</span>,</span><br><span class="line">         -<span class="number">1.2375e-02</span>, -<span class="number">1.6372e-02</span>],</span><br><span class="line">        [-<span class="number">8.2350e-03</span>,  <span class="number">4.1301e-03</span>, -<span class="number">1.9192e-03</span>,  ..., -<span class="number">2.3119e-05</span>,</span><br><span class="line">          <span class="number">2.0167e-03</span>,  <span class="number">1.9528e-02</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([ <span class="number">0.0162</span>, -<span class="number">0.0146</span>, -<span class="number">0.0218</span>,  <span class="number">0.0212</span>, -<span class="number">0.0119</span>, -<span class="number">0.0142</span>, -<span class="number">0.0079</span>,  <span class="number">0.0171</span>,</span><br><span class="line">         <span class="number">0.0205</span>,  <span class="number">0.0164</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><code>net.named_parameters</code> 可同时返回可学习的参数及名称</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name,parameters <span class="keyword">in</span> net.named_parameters():</span><br><span class="line"></span><br><span class="line"><span class="comment"># conv1.weight : torch.Size([6, 1, 3, 3])</span></span><br><span class="line"><span class="comment"># conv1.bias : torch.Size([6])</span></span><br><span class="line"><span class="comment"># fc1.weight : torch.Size([10, 1350])</span></span><br><span class="line"><span class="comment"># fc1.bias : torch.Size([10])</span></span><br></pre></td></tr></table></figure><p>forward函数的输入和输出都是Tensor</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>) <span class="comment"># 这里的对应前面fforward的输入是32</span></span><br><span class="line">out = net(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">input</span>.size()</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.Size([1, 1, 32, 32])</span></span><br><span class="line"></span><br><span class="line">out.size()</span><br><span class="line"></span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">6</span>, <span class="number">30</span>, <span class="number">30</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">6</span>, <span class="number">15</span>, <span class="number">15</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">1350</span>])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>在反向传播前，先要将所有参数的梯度清零</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad() </span><br><span class="line">out.backward(torch.ones(<span class="number">1</span>,<span class="number">10</span>)) <span class="comment"># 反向传播的实现是PyTorch自动实现的，我们只要调用这个函数即可</span></span><br></pre></td></tr></table></figure><p><strong>注意</strong>：<code>torch.nn</code> 只支持 <code>mini-batches</code>，不支持一次只输入一个样本，即一次必须是一个batch。也就是说，就算我们输入一个样本，也会对样本进行分批，所以，所有的输入都会增加一个维度，我们对比下刚才的 <code>input</code>，<code>nn</code> 中定义为3维，但是我们人工创建时多增加了一个维度，变为了4维，最前面的1即为batch-size。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>在nn中PyTorch还预制了常用的损失函数，下面我们用MSELoss用来计算均方误差：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">y = torch.arange(<span class="number">0</span>,<span class="number">10</span>).view(<span class="number">1</span>,<span class="number">10</span>).<span class="built_in">float</span>()</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">loss = criterion(out, y)</span><br><span class="line"><span class="comment">#loss是个scalar，我们可以直接用item获取到他的python类型的数值</span></span><br><span class="line"><span class="built_in">print</span>(loss.item()) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 28.92203712463379</span></span><br></pre></td></tr></table></figure><h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><p>在反向传播计算完所有参数的梯度后，还需要使用优化方法来更新网络的权重和参数，例如随机梯度下降法(SGD)的更新策略如下：</p><p><code>weight = weight - learning_rate * gradient</code></p><p>在 <code>torch.optim</code> 中实现大多数的优化方法，例如 RMSProp、Adam、SGD等，下面我们使用SGD做个简单的样例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim</span><br><span class="line"></span><br><span class="line">out = net(<span class="built_in">input</span>) <span class="comment"># 这里调用的时候会打印出我们在forword函数中打印的x的大小</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">loss = criterion(out, y)</span><br><span class="line"><span class="comment">#新建一个优化器，SGD只需要要调整的参数和学习率</span></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr = <span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># 先梯度清零(与net.zero_grad()效果一样)</span></span><br><span class="line">optimizer.zero_grad() </span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment">#更新参数</span></span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><p>这样，神经网络的数据的一个完整的传播就已经通过PyTorch实现了。</p><h1 id="深度学习基础"><a href="#深度学习基础" class="headerlink" title="深度学习基础"></a>深度学习基础</h1><h2 id="监督学习和无监督学习"><a href="#监督学习和无监督学习" class="headerlink" title="监督学习和无监督学习"></a>监督学习和无监督学习</h2><p>监督学习、无监督学习、半监督学习、强化学习是我们日常接触到的常见的四个机器学习方法：</p><ul><li>监督学习：通过已有的训练样本（即已知数据以及其对应的输出）去训练得到一个最优模型（这个模型属于某个函数的集合，最优则表示在某个评价准则下是最佳的），再利用这个模型将所有的输入映射为相应的输出。</li><li>无监督学习：它与监督学习的不同之处，在于我们事先没有任何训练样本，而需要直接对数据进行建模。</li><li>半监督学习 ：在训练阶段结合了大量未标记的数据和少量标签数据。与使用所有标签数据的模型相比，使用训练集的训练模型在训练时可以更为准确。</li><li>强化学习：我们设定一个回报函数（reward function），通过这个函数来确认否越来越接近目标，类似我们训练宠物，如果做对了就给他奖励，做错了就给予惩罚，最后来达到我们的训练目的。</li></ul><p>这里我们只着重介绍监督学习，因为我们后面的绝大部们课程都是使用的监督学习的方法，在训练和验证时输入的数据既包含输入x，又包含x对应的输出y，即学习数据已经事先给出了正确答案。</p><h2 id="线性回归-（Linear-Regreesion）"><a href="#线性回归-（Linear-Regreesion）" class="headerlink" title="线性回归 （Linear Regreesion）"></a>线性回归 （Linear Regreesion）</h2><p>这里不解释具体原理，直接看写法，体会训练过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意，这里我们使用了一个新库叫 seaborn 如果报错找不到包的话请使用pip install seaborn 来进行安装</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Linear, Module, MSELoss</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> SGD</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面我生成一些随机的点，来作为我们的训练数据，回归：y = 5*x + 7</span></span><br><span class="line">x = np.random.rand(<span class="number">256</span>)</span><br><span class="line">noise = np.random.randn(<span class="number">256</span>) / <span class="number">4</span></span><br><span class="line">y = x * <span class="number">5</span> + <span class="number">7</span> + noise</span><br><span class="line">df = pd.DataFrame()</span><br><span class="line">df[<span class="string">&#x27;x&#x27;</span>] = x</span><br><span class="line">df[<span class="string">&#x27;y&#x27;</span>] = y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们随机生成了一些点，下面将使用PyTorch建立一个线性的模型来对其进行拟合，这就是所说的训练的过程，由于只有一层线性模型，所以我们就直接使用了:</span></span><br><span class="line">model=Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 其中参数(1,1)代表输入输出的特征(feature)数量都是1</span></span><br><span class="line"><span class="comment"># Linear 模型的表达式是y=wx+b，其中w代表权重，b代表偏置</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数我们使用均方损失函数</span></span><br><span class="line">criterion = MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器我们选择最常见的优化方法 SGD，就是每一次迭代计算 mini-batch 的梯度，然后对参数进行更新，学习率 0.01</span></span><br><span class="line">optim = SGD(model.parameters(), lr = <span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练3000次</span></span><br><span class="line">epochs = <span class="number">3000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备训练数据: x_train, y_train 的形状是(256, 1)， </span></span><br><span class="line"><span class="comment"># 代表mini-batch大小为256，feature为1. astype(&#x27;float32&#x27;) 是为了下一步可以直接转换为 torch.float</span></span><br><span class="line">x_train = x.reshape(-<span class="number">1</span>, <span class="number">1</span>).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">y_train = y.reshape(-<span class="number">1</span>, <span class="number">1</span>).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="comment"># 整理输入和输出的数据，这里输入和输出一定要是torch的Tensor类型</span></span><br><span class="line">    inputs = torch.from_numpy(x_train)</span><br><span class="line">    labels = torch.from_numpy(y_train)</span><br><span class="line">    <span class="comment">#使用模型进行预测</span></span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    <span class="comment">#梯度置0，否则会累加</span></span><br><span class="line">    optim.zero_grad()</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 使用优化器默认方法优化</span></span><br><span class="line">    optim.step()</span><br><span class="line">    <span class="keyword">if</span> (i%<span class="number">100</span>==<span class="number">0</span>):</span><br><span class="line">        <span class="comment">#每 100次打印一下损失函数，看看效果</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch &#123;&#125;, loss &#123;:1.4f&#125;&#x27;</span>.<span class="built_in">format</span>(i,loss.data.item()))</span><br></pre></td></tr></table></figure><p>训练完成了，看一下训练的成果是多少。用 <code>model.parameters()</code> 提取模型参数。 $w$， $b$ 是我们所需要训练的模型参数，我们期望的数据 $w=5$，$b=7$ 可以做一下对比</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[w, b] = model.parameters()</span><br><span class="line"><span class="built_in">print</span> (w.item(),b.item())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.994358062744141 7.0252156257629395</span></span><br></pre></td></tr></table></figure><h2 id="损失函数-Loss-Function"><a href="#损失函数-Loss-Function" class="headerlink" title="损失函数(Loss Function)"></a>损失函数(Loss Function)</h2><p>损失函数（loss function）是用来估量模型的预测值(我们例子中的output)与真实值（例子中的y_train）的不一致程度，它是一个非负实值函数，损失函数越小，模型的鲁棒性就越好。 我们训练模型的过程，就是通过不断的迭代计算，使用梯度下降的优化算法，使得损失函数越来越小。损失函数越小就表示算法达到意义上的最优。</p><p>这里有一个重点：因为PyTorch是使用mini-batch来进行计算的，所以损失函数的计算出来的结果已经对mini-batch取了平均。</p><p>常见（PyTorch内置）的损失函数有以下几个：</p><h3 id="nn-L1Loss"><a href="#nn-L1Loss" class="headerlink" title="nn.L1Loss:"></a>nn.L1Loss:</h3><p>输入x和目标y之间差的绝对值，要求 x 和 y 的维度要一样（可以是向量或者矩阵），得到的 loss 维度也是对应一样的</p><script type="math/tex; mode=display">loss(x,y)=1/n\sum|x_i-y_i|</script><h3 id="nn-NLLLoss"><a href="#nn-NLLLoss" class="headerlink" title="nn.NLLLoss:"></a>nn.NLLLoss:</h3><p>用于多分类的负对数似然损失函数$loss(x, class) = -x[class]$；</p><p>NLLLoss中如果传递了weights参数，会对损失进行加权，公式就变成了</p><script type="math/tex; mode=display">loss(x, class) = -weights[class] * x[class]</script><h3 id="n-MSELoss"><a href="#n-MSELoss" class="headerlink" title="n.MSELoss:"></a>n.MSELoss:</h3><p>均方损失函数 ，输入x和目标y之间均方差</p><script type="math/tex; mode=display">loss(x,y)=1/n\sum(x_i-y_i)^2</script><h3 id="nn-CrossEntropyLoss"><a href="#nn-CrossEntropyLoss" class="headerlink" title="nn.CrossEntropyLoss:"></a>nn.CrossEntropyLoss:</h3><p>多分类用的交叉熵损失函数，LogSoftMax 和 NLLLoss 集成到一个类中，会调用 nn.NLLLoss 函数，我们可以理解为 CrossEntropyLoss()=log_softmax() + NLLLoss()</p><script type="math/tex; mode=display"> \begin{aligned} loss(x, class) &= -\text{log}\frac{exp(x[class])}{\sum_j exp(x[j]))}\ &= -x[class] + log(\sum_j exp(x[j])) \end{aligned}</script><p>因为使用了NLLLoss，所以也可以传入weight参数，这时loss的计算公式变为：</p><script type="math/tex; mode=display">loss(x, class) = weights[class] * (-x[class] + log(\sum_j exp(x[j])))</script><p>所以一般多分类的情况会使用这个损失函数；</p><h3 id="nn-BCELoss"><a href="#nn-BCELoss" class="headerlink" title="nn.BCELoss:"></a>nn.BCELoss:</h3><p>计算 x 与 y 之间的二进制交叉熵。$loss(o,t)=-\frac{1}{n}\sum_i(t[i] <em>log(o[i])+(1-t[i])</em> log(1-o[i]))$ 与NLLLoss类似，也可以添加权重参数：</p><script type="math/tex; mode=display">loss(o,t)=-\frac{1}{n}\sum_iweights[i] *(t[i]* log(o[i])+(1-t[i])* log(1-o[i]))</script><p>用的时候需要在该层前面加上 Sigmoid 函数。</p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>在介绍损失函数的时候我们已经说了，梯度下降是一个使损失函数越来越小的优化算法，在无求解机器学习算法的模型参数，即约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一。所以梯度下降是我们目前所说的机器学习的核心，了解了它的含义，也就了解了机器学习算法的含义。</p><h3 id="Mini-batch的梯度下降法"><a href="#Mini-batch的梯度下降法" class="headerlink" title="Mini-batch的梯度下降法"></a>Mini-batch的梯度下降法</h3><p>对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候处理速度会很慢，而且也不可能一次的载入到内存或者显存中，所以我们会把大数据集分成小数据集，一部分一部分的训练，这个训练子集即称为Mini-batch。 在PyTorch中就是使用这种方法进行的训练，可以看看上一章中关于dataloader的介绍里面的batch_size就是我们一个Mini-batch的大小。</p><p>为了介绍的更简洁，使用 吴恩达老师的 <a href="https://www.deeplearning.ai/deep-learning-specialization/">deeplearning.ai</a> 课程板书。</p><p>对于普通的梯度下降法，一个epoch只能进行一次梯度下降；而对于Mini-batch梯度下降法，一个epoch可以进行Mini-batch的个数次梯度下降。 <img src="https://handbook.pytorch.wiki/chapter2/2.png" alt="img"> 普通的batch梯度下降法和Mini-batch梯度下降法代价函数的变化趋势，如下图所示： <img src="https://handbook.pytorch.wiki/chapter2/3.png" alt="img"></p><ul><li><p>如果训练样本的大小比较小时，能够一次性的读取到内存中，那我们就不需要使用Mini-batch，</p></li><li><p>如果训练样本的大小比较大时，一次读入不到内存或者现存中，那我们必须要使用 Mini-batch来分批的计算 </p></li><li>Mini-batch size的计算规则如下，在内存允许的最大情况下使用2的N次方个size <img src="https://handbook.pytorch.wiki/chapter2/4.png" alt="img"></li></ul><p><code>torch.optim</code>是一个实现了各种优化算法的库。大部分常用优化算法都有实现，我们直接调用即可。</p><h3 id="torch-optim-SGD"><a href="#torch-optim-SGD" class="headerlink" title="torch.optim.SGD"></a>torch.optim.SGD</h3><p>随机梯度下降算法，带有动量（momentum）的算法作为一个可选参数可以进行设置，样例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#lr参数为学习率，对于SGD来说一般选择0.1 0.01.0.001，如何设置会在后面实战的章节中详细说明</span></span><br><span class="line"><span class="comment">##如果设置了momentum，就是带有动量的SGD，可以不设置</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><h3 id="torch-optim-RMSprop"><a href="#torch-optim-RMSprop" class="headerlink" title="torch.optim.RMSprop"></a>torch.optim.RMSprop</h3><p>除了以上的带有动量Momentum梯度下降法外，RMSprop（root mean square prop）也是一种可以加快梯度下降的算法，利用RMSprop算法，可以减小某些维度梯度更新波动较大的情况，使其梯度下降的速度变得更快</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#我们的课程基本不会使用到RMSprop所以这里只给一个实例</span></span><br><span class="line">optimizer = torch.optim.RMSprop(model.parameters(), lr=<span class="number">0.01</span>, alpha=<span class="number">0.99</span>)</span><br></pre></td></tr></table></figure><h3 id="torch-optim-Adam"><a href="#torch-optim-Adam" class="headerlink" title="torch.optim.Adam"></a>torch.optim.Adam</h3><p>Adam 优化算法的基本思想就是将 Momentum 和 RMSprop 结合起来形成的一种适用于不同深度学习结构的优化算法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里的lr，betas，还有eps都是用默认值即可，所以Adam是一个使用起来最简单的优化方法</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>, betas=(<span class="number">0.9</span>, <span class="number">0.999</span>), eps=<span class="number">1e-08</span>)</span><br></pre></td></tr></table></figure><h2 id="方差-偏差"><a href="#方差-偏差" class="headerlink" title="方差/偏差"></a>方差/偏差</h2><ul><li>偏差度量了学习算法的期望预测与真实结果的偏离程序，即刻画了学习算法本身的拟合能力</li><li>方差度量了同样大小的训练集的变动所导致的学习性能的变化，即模型的泛化能力 <img src="https://handbook.pytorch.wiki/chapter2/5.png" alt="img"></li></ul><p>从图中我们可以看出 - 高偏差（high bias）的情况，一般称为欠拟合（underfitting），即我们的模型并没有很好的去适配现有的数据，拟合度不够。 - 高方差（high variance）的情况一般称作过拟合（overfitting），即模型对于训练数据拟合度太高了，失去了泛化的能力。</p><p>如何解决这两种情况呢？</p><ul><li><p>欠拟合： </p><ul><li>增加网络结构，如增加隐藏层数目； </li><li>训练更长时间；</li><li>寻找合适的网络架构，使用更大的NN结构；</li></ul></li><li><p>过拟合 ： </p><ul><li>使用更多的数据；</li><li>正则化（ regularization）； </li><li>寻找合适的网络结构；</li></ul></li></ul><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>利用正则化来解决High variance 的问题，正则化是在 Cost function 中加入一项正则化项，惩罚模型的复杂度，这里我们简单的介绍一下正则化的概念</p><h3 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h3><p>损失函数基础上加上权重参数的绝对值 $L=E_{in}+\lambda{\sum_j} \left|w_j\right|$；</p><h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p>损失函数基础上加上权重参数的平方和 $L=E_{in}+\lambda{\sum_j} w^2_j$；</p><p>需要说明的是：$L_1$ 相比于 $L_2$ 会更容易获得稀疏解。</p><h1 id="卷积神经网络经典模型LeNet-5"><a href="#卷积神经网络经典模型LeNet-5" class="headerlink" title="卷积神经网络经典模型LeNet-5"></a>卷积神经网络经典模型LeNet-5</h1><p>卷积神经网路的开山之作，麻雀虽小，但五脏俱全，卷积层、pooling层、全连接层，这些都是现代CNN网络的基本组件 - 用卷积提取空间特征； </p><ul><li>由空间平均得到子样本； </li><li>用 tanh 或 sigmoid 得到非线性； </li><li>用 multi-layer neural network（MLP）作为最终分类器；</li><li>层层之间用稀疏的连接矩阵，以避免大的计算成本。 <img src="https://handbook.pytorch.wiki/chapter2/lenet5.jpg" alt="img"></li></ul><p>输入：图像Size为32*32。</p><p>输出：10个类别，分别为0-9数字的概率</p><ol><li>C1层是一个卷积层，有6个卷积核（提取6种局部特征），核大小为5 * 5</li><li>S2层是pooling层，下采样（区域:2 * 2 ）降低网络训练参数及模型的过拟合程度。</li><li>C3层是第二个卷积层，使用16个卷积核，核大小:5 * 5 提取特征</li><li>S4层也是一个pooling层，区域:2*2</li><li>C5层是最后一个卷积层，卷积核大小:5 * 5 卷积核种类:120</li><li>最后使用全连接层，将C5的120个特征进行分类，最后输出0-9的概率</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet5</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LeNet5, self).__init__()</span><br><span class="line">        <span class="comment"># 1 input image channel, 6 output channels, 5x5 square convolution</span></span><br><span class="line">        <span class="comment"># kernel</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>) <span class="comment"># 这里论文上写的是conv,官方教程用了线性层</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># If the size is a square you can only specify a single number</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">num_flat_features</span>(<span class="params">self, x</span>):</span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = LeNet5()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"></span><br><span class="line"><span class="comment"># LeNet5(</span></span><br><span class="line"><span class="comment">#   (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#   (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#   (fc1): Linear(in_features=400, out_features=120, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc2): Linear(in_features=120, out_features=84, bias=True)</span></span><br><span class="line"><span class="comment">#   (fc3): Linear(in_features=84, out_features=10, bias=True)</span></span><br><span class="line"><span class="comment"># )</span></span><br></pre></td></tr></table></figure><h1 id="相关链接"><a href="#相关链接" class="headerlink" title="相关链接"></a>相关链接</h1><p><a href="http://www.feiguyunai.com/index.php/2019/06/13/python-ml-pytorch/">《Python深度学习基于PyTorch》 | Python技术交流与分享 (feiguyunai.com)</a></p><p><a href="https://github.com/ShusenTang/Dive-into-DL-PyTorch">ShusenTang/Dive-into-DL-PyTorch: 本项目将《动手学深度学习》(Dive into Deep Learning)原书中的MXNet实现改为PyTorch实现。 (github.com)</a></p>]]></content>
      
      
      <categories>
          
          <category> Code </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络简介</title>
      <link href="/2022/11/13/network.html"/>
      <url>/2022/11/13/network.html</url>
      
        <content type="html"><![CDATA[<p><strong>说明</strong>：本文以机器学习中最基本的分类网络为背景，介绍神经网络的基本结构；基本只需要知道这些基本组成便可以着手具体的项目代码，而一些特殊的网络组成，只需遇到时自行搜索学习即可。</p><p>传统<strong>k-近邻（KNN）算</strong>法：</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/k-neighbor.png"                        width="178" height="146.5"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><ul><li><p><strong>问：</strong>绿色是属于方块还是三角？——取决于 k。</p></li><li><p><strong>算法流程：</strong></p><ul><li>（1）计算已知类别数据集中的点与当前点的距离，并依次排序</li><li>（2）选取与当前点距离最小的 k 个点，并确定类别；</li><li>（3）返回前 k 个点出现频率最高的类别作为预测。</li></ul></li></ul><h2 id="设计网络需要考虑"><a href="#设计网络需要考虑" class="headerlink" title="设计网络需要考虑"></a>设计网络需要考虑</h2><ul><li>数据的预处理与初始化如何更高效？</li><li>网络结构：隐藏层层数、每层神经元个数？</li><li>损失函数？正则化项？</li><li>优化策略？</li></ul><h2 id="（全连接）神经网路"><a href="#（全连接）神经网路" class="headerlink" title="（全连接）神经网路"></a>（全连接）神经网路</h2><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/all.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><h3 id="数据预处理和参数初始化"><a href="#数据预处理和参数初始化" class="headerlink" title="数据预处理和参数初始化"></a>数据预处理和参数初始化</h3><ul><li>一般对输入数据进行预处理或归一化（例如映射到 $[0,1]$ 范围内），再作为网络输入；</li><li>对网络参数初始化时进行随机初始化（尽量浮动小一些，类似初始学习率也取很小）。</li></ul><h3 id="线性函数（得分函数）"><a href="#线性函数（得分函数）" class="headerlink" title="线性函数（得分函数）"></a>线性函数（得分函数）</h3><p>​        假设分类有10个类别，权重和偏移分别为 $W\in\mathbb{R}^{10\times d},b\in\mathbb{R}^{10},x\in\mathbb{R}^d$，得分函数即为</p><script type="math/tex; mode=display">f(x,W)=W x+b:input \to score,</script><p>相当于每个类别、每个像素点对应不同参数权重。</p><p>​        事实上，这个权重 $W$、偏移 $b$ 是上面网络中层与层之间连线部分计算的线性部分：从某一层输出，经过线性变换之后作为下一层的输入。</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>​        <strong>目的</strong>：如果只是一直的线性变换，那只是简单的线性拟合，无法应对非线性数据；</p><p>​        <strong>方法</strong>：经过上述线性变换后，在传输到下一组神经元之前，进行非线性变换，也即<strong>激活</strong>。事实上这个激活函数就是上面每层网络中的圆圈部分。下面是两个最常用的两种激活函数：</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/sigmod.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>​        用于监督与反向传播，衡量计算当前得分的权重的好坏，例如这里 ：</p><script type="math/tex; mode=display">L_i=\sum_{j\neq y_i}\max(0,s_j-s_{y_i}+1)+\lambda R(W)</script><p>后面 $\lambda R(W)$ 为正则项，是为了削减过拟合。</p><h3 id="softmax-分类器"><a href="#softmax-分类器" class="headerlink" title="$softmax$ 分类器"></a>$softmax$ 分类器</h3><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/softmax.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p>从而计算损失，如图中取的是交叉熵，因为概率越靠近1，损失越小。</p><h3 id="反向传播（可微）"><a href="#反向传播（可微）" class="headerlink" title="反向传播（可微）"></a>反向传播（可微）</h3><p>​        链式法则逐层计算偏导，从网络的输出反向进行到网络的输入，以进行后续的优化，例如<strong>梯度下降法</strong>：</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/grad.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><h3 id="Drop-out"><a href="#Drop-out" class="headerlink" title="Drop-out"></a>Drop-out</h3><p>​        为了削弱过拟合部分，随机在某些训练步内失活一定的神经元：</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/drop-out.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/cnn.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr>    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/cnn-all.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>​        <strong>目的</strong>：增加关联性，考虑到像素与周围像素是有关联性的；</p><p>​        <strong>策略</strong>：利用卷积块作为权重参数对指定大小的像素块做加权组合；注意，每个颜色通道是单独做卷积。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/cnn-1.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/cnn-11.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/cnn-12.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p><strong>边缘填充</strong>，是因为边界在做卷积时，边缘只计算一次，但是内部可能计算多次，为了平衡重要性，在边缘添加一圈0。</p><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>​        <strong>效果</strong>：卷积一定层后，可能发生维度爆炸，利用池化层降低维度。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/cnn-2.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/cnn-21.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><h3 id="特征图变化"><a href="#特征图变化" class="headerlink" title="特征图变化"></a>特征图变化</h3><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/cnn-3.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><h3 id="感受野"><a href="#感受野" class="headerlink" title="感受野"></a>感受野</h3><p>​        可以看到，到第二个卷积层后，一个像素块已经<strong>感受</strong>到了周围像素块的关联性。</p><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/feelfield.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/feelfield2.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p>（可跳过）下面的经典网络只是学习时候遇见，罗列在此。</p><h2 id="经典网络"><a href="#经典网络" class="headerlink" title="经典网络"></a>经典网络</h2><h3 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h3><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/vgg.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><h3 id="Resnet-残差网络"><a href="#Resnet-残差网络" class="headerlink" title="Resnet 残差网络"></a>Resnet 残差网络</h3><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/renet.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><h3 id="递归神经网络"><a href="#递归神经网络" class="headerlink" title="递归神经网络"></a>递归神经网络</h3><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/rnn.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/rnn2.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><ul><li><p>保留上一步时间的特征，和下一时间特征一起输入到下一层；</p></li><li><p>一般只选择最后一层输出结果。</p></li></ul><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/lstm.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/lstm2.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/lstm3.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/lstm4.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/lstm5.png"                        width="543.75" height="267"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table>]]></content>
      
      
      <categories>
          
          <category> Basic Knowledge </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Basic Knowledge </tag>
            
            <tag> Network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Anaconda基本教程</title>
      <link href="/2022/11/12/conda.html"/>
      <url>/2022/11/12/conda.html</url>
      
        <content type="html"><![CDATA[<p>先简单说一下：<code>Anaconda</code> 是对 <code>Python</code> 环境进行管理的一个工具，我们知道每个 Python 环境中需要一个解释器和一个包集合，但是随着不同项目需要的解释器以及各种包的版本不同，需要多个环境管理，这时就需要 <code>Anaconda</code> 的存在，将多个 Python 环境分隔开管理。</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="Windows安装"><a href="#Windows安装" class="headerlink" title="Windows安装"></a>Windows安装</h2><ol><li><p><a href="https://www.anaconda.com/">Anaconda官网</a> 下载，除以下两步外，其他一切默认即可：</p><ul><li>有一步为 <code>Install for</code>，选择第二项 <code>All Users</code>;</li><li>有一步是 <code>Advanced Options</code>，选择第二项（第一项写着 Not Recommended），我们要自己添加环境变量，不然后面有问题。</li><li>（当然你可以修改安装路径）</li></ul></li><li><p>配置环境变量：<code>此电脑-&gt;属性-&gt;高级系统设置-&gt;环境变量-&gt;系统变量-&gt;Path-&gt;新建</code>，将如下几条文件（当然要是你自己的路径）添加进去：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">E:\Anaconda3</span><br><span class="line">E:\Anaconda3\Scripts</span><br><span class="line">E:\Anaconda3\Library\bin</span><br><span class="line">E:\Anaconda\Library\mingw-w64\bin</span><br><span class="line">E:\Anaconda\Library\usr\bin</span><br></pre></td></tr></table></figure></li><li><p>测试是否安装成功，快捷键 <code>Ctrl+R</code> 输入 <code>cmd</code>，再输入 <code>conda --version</code>，输出版本正常就成功。</p></li></ol><h2 id="Linux安装"><a href="#Linux安装" class="headerlink" title="Linux安装"></a>Linux安装</h2><ol><li><p><a href="https://www.anaconda.com/">Anaconda官网</a> 下载，如果嫌慢，这里提供 <a href="https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/">清华大学开源软件镜像站</a>，下载后进入相应目录即可；当然你也可以用 <code>wget</code> 命令行下载（下面这个2021年的可用）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-x86_64.sh</span><br></pre></td></tr></table></figure></li><li><p><code>bash</code> 运行安装包，比如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash Anaconda3-2021.11-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><p>一路直接回车或者 <code>&quot;yes&quot;</code> 即可，到达 <code>Do you wish the installer to initialize Anaconda3 by running conda init ?</code>，一定选择 <strong>no</strong>！</p></li><li><p>添加环境变量</p><ul><li><p>可以用 <code>vim ~/.bashrc</code> 编辑</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH=&quot;/root/anaconda3/bin:$PATH” #添加你自己的路径即可</span><br></pre></td></tr></table></figure><p>也可以直接用文本编辑器修改，方便一些！</p></li><li><p><code>source</code> 一下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure></li></ul></li><li><p>检查是否安装成功 <code>conda --version</code>。</p></li></ol><h1 id="换源"><a href="#换源" class="headerlink" title="换源"></a>换源</h1><p>解释一下：如果你不换源，那每次安装各种包的时候将非常的慢！（毕竟国内要科学上网）</p><ul><li><p><code>Windows</code> 下路径是 <code>C:\Users\your_name\.condarc</code>，用记事本打开这个文件，添加下面的镜像源（清华源）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/peterjc123/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/</span><br><span class="line">  - defaults</span><br><span class="line">show_channel_urls: true</span><br></pre></td></tr></table></figure><p>我只添加了清华源，不过下面附上其他源：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#中国科大镜像源</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/pkgs/main/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/Anaconda/pkgs/free/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/Anaconda/cloud/msys2/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/Anaconda/cloud/bioconda/</span><br><span class="line">  - https://mirrors.ustc.edu.cn/Anaconda/cloud/menpo/  </span><br></pre></td></tr></table></figure></li><li><p><code>Linux</code> 下路径 <code>~/.condarc</code>，你可以用 <code>vim ~/.condarc</code> 进行编辑，当然最简单的就是用文本编辑器打开，然后添加上述源即可。</p></li><li><p>当然你也可以通过命令行逐一添加，太麻烦了。。。（以其中一个为例）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.ustc.edu.cn/Anaconda/pkgs/main/</span><br></pre></td></tr></table></figure></li></ul><h1 id="创建虚拟环境"><a href="#创建虚拟环境" class="headerlink" title="创建虚拟环境"></a>创建虚拟环境</h1><p>如果你没创建任何环境的话，运行下述命令，会显示你已经有了 <code>base</code> 环境，但我们一般在这个环境下安装什么，而是为具体的项目定制具体的环境：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda info --envs</span><br></pre></td></tr></table></figure><p>当然，事不绝对，你要是在 <code>base</code> 环境下安装什么包也是没问题的，没人拦着你。</p><p>下面命令行进行虚拟环境创建：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 创建虚拟环境</span><br><span class="line"># xxx为python解释器版本,一般都是3.7/8/9,出于稳定考虑，不要太新，不然某些包版本不兼容</span><br><span class="line">conda create -n your_env_name python=xxx</span><br><span class="line"></span><br><span class="line"># 安装完成后，进入环境</span><br><span class="line"># 不过建议首次激活用 source activate your_env_name</span><br><span class="line">conda activate your_env_name</span><br><span class="line"></span><br><span class="line"># 如果是windows,有可能就是 activate your_env_name</span><br><span class="line"># Linux下如果不是在默认base环境下，可能需要 source activate your_env_name 才能进入环境</span><br></pre></td></tr></table></figure><h1 id="基本常用命令"><a href="#基本常用命令" class="headerlink" title="基本常用命令"></a>基本常用命令</h1><h2 id="查看已经安装的包"><a href="#查看已经安装的包" class="headerlink" title="查看已经安装的包"></a>查看已经安装的包</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure><h2 id="安装指定包"><a href="#安装指定包" class="headerlink" title="安装指定包"></a>安装指定包</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conda install package_name</span><br><span class="line"></span><br><span class="line"># 当然可以指定包的版本，例如</span><br><span class="line">conda install scrapy==1.3</span><br><span class="line"></span><br><span class="line"># 在conda指定的某个环境中安装包</span><br><span class="line">conda install -n your_env_name package_name</span><br></pre></td></tr></table></figure><p>这里说一下，上述 <code>conda install</code> 如果不成功，可能是由于相应包由 <code>pip</code> 管理</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install package_name</span><br></pre></td></tr></table></figure><h2 id="查看当前有哪些环境"><a href="#查看当前有哪些环境" class="headerlink" title="查看当前有哪些环境"></a>查看当前有哪些环境</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 任选其一皆可</span><br><span class="line">conda env list</span><br><span class="line">conda info -e</span><br><span class="line">conda info --envs</span><br></pre></td></tr></table></figure><h2 id="删除某一环境的某一包"><a href="#删除某一环境的某一包" class="headerlink" title="删除某一环境的某一包"></a>删除某一环境的某一包</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">onda remove --name your_env_name package_name</span><br></pre></td></tr></table></figure><h2 id="删除某一环境"><a href="#删除某一环境" class="headerlink" title="删除某一环境"></a>删除某一环境</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda remove -n your_env_name --all</span><br></pre></td></tr></table></figure><h2 id="检查更新conda"><a href="#检查更新conda" class="headerlink" title="检查更新conda"></a>检查更新conda</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 检查更新当前conda</span><br><span class="line">conda update conda</span><br><span class="line"></span><br><span class="line"># 更新anaconda</span><br><span class="line">conda update anaconda</span><br></pre></td></tr></table></figure><h2 id="更新包"><a href="#更新包" class="headerlink" title="更新包"></a>更新包</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 更新所有库，轻易不要用这个，不然不就丧失了conda多版本管理的意义</span><br><span class="line">conda update --all</span><br><span class="line"></span><br><span class="line"># 更新指定包</span><br><span class="line">conda update package_name</span><br><span class="line"></span><br><span class="line"># pip更新</span><br><span class="line">pip install --upgrade package_name</span><br><span class="line"></span><br><span class="line"># 也可以更新python</span><br><span class="line">conda update python</span><br></pre></td></tr></table></figure><h2 id="导出环境"><a href="#导出环境" class="headerlink" title="导出环境"></a>导出环境</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 导出当前环境的包信息，当然路径就在你所在的目录</span><br><span class="line">conda env export &gt; environment.yaml</span><br></pre></td></tr></table></figure><h2 id="用配置文件创建环境"><a href="#用配置文件创建环境" class="headerlink" title="用配置文件创建环境"></a>用配置文件创建环境</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda env create -f environment.yaml</span><br></pre></td></tr></table></figure><h2 id="安装requirements-txt依赖"><a href="#安装requirements-txt依赖" class="headerlink" title="安装requirements.txt依赖"></a>安装requirements.txt依赖</h2><p>解释：一般 <code>requirements.txt</code> 是项目作者将环境提取出来，用于他人复刻</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt # 当然要求你运行这个命令行所在目录有这个文件</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Code </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Anaconda </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git教程</title>
      <link href="/2022/11/11/git.html"/>
      <url>/2022/11/11/git.html</url>
      
        <content type="html"><![CDATA[<h1 id="Git安装"><a href="#Git安装" class="headerlink" title="Git安装"></a>Git安装</h1><p>先注册 <code>Github</code> 主页，在学习使用之前，注意：<strong>如果你涉及git只是简单的保存代码且文件大小&lt;25M，github的图形界面完全可以搞定！没必要从头一点一点学命令行。</strong></p><p>然后官网下载<a href="https://git-scm.com/">Git (git-scm.com)</a>，一切默认选项，不要乱改！</p><h1 id="Git最基本使用"><a href="#Git最基本使用" class="headerlink" title="Git最基本使用"></a>Git最基本使用</h1><h2 id="配置并生成密钥"><a href="#配置并生成密钥" class="headerlink" title="配置并生成密钥"></a>配置并生成密钥</h2><p>（这是为了你能 <code>clone</code> 以及推送远程仓库做准备）</p><p>（下述命令行都是在指定文件夹下用 <code>Git Bash</code> 输入并运行）</p><ol><li><p>用如下命令检查一下用户名和邮箱是否配置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global  --list</span><br></pre></td></tr></table></figure></li><li><p>配置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global  user.name 用户名</span><br><span class="line">git config --global user.email 邮箱</span><br></pre></td></tr></table></figure></li><li><p>生成秘钥：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C 邮箱</span><br></pre></td></tr></table></figure><p>执行命令后需要进行3次或4次确认：</p><ul><li>(1) 确认秘钥的保存路径（如果不需要改路径则直接回车）；</li><li>(2) 如果上一步置顶的保存路径下已经有秘钥文件，则需要确认是否覆盖（如果之前的秘钥不再需要则直接回车覆盖，如需要则手动拷贝到其他目录后再覆盖）；</li><li>(3) 创建密码（如果不需要密码则直接回车）；</li><li>(4) 确认密码；</li><li>在指定的保存路径（一般是 <code>C:/users/用户名/.ssh</code>）下会生成2个名为 <code>id_rsa</code> 和 <code>id_rsa.pub</code> 的文件；</li></ul></li><li><p>网页打开 Github，进入 <code>Settings -&gt; SSH and GPG keys -&gt; New SSH key</code>；</p></li><li><p>然后用文本工具（记事本之类的）打开之前生成的 <code>id_rsa.pub</code> 文件，把内容拷贝到 <code>key</code> 下面的输入框，并为这个 <code>key</code> 定义一个名称（通常用来区分不同主机），然后保存即可。</p></li></ol><h2 id="创建本地仓库"><a href="#创建本地仓库" class="headerlink" title="创建本地仓库"></a>创建本地仓库</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir sth  #创建文件夹,sth即为文件夹名称,建议英文</span><br><span class="line">cd sth     #进入文件夹</span><br><span class="line">git init   #初始化为git仓库,此时文件夹下会创建.ssh隐藏文件夹</span><br></pre></td></tr></table></figure><p>可以将指定文件添加到这个仓库内并记录每次更改</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git add any_file #将指定文件添加到Git仓库管理,any_file即为文件名字,例如 temp.txt</span><br><span class="line">git commit -m &quot;description&quot; #-m后面&quot;&quot;内的描述是本次提交的说明(英文),用这个简要说明帮你记录每次提交的原因和目的</span><br></pre></td></tr></table></figure><p>也可以多次提交并记录说明</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git add any_file_1.txt</span><br><span class="line">git add any_file_2.txt</span><br><span class="line">git commit -m &quot;description&quot;</span><br></pre></td></tr></table></figure><p>或者本文件夹下内的文件全部提交</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br></pre></td></tr></table></figure><p>当然你也可以一直用下面这个命令查看仓库状态：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git status</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：如果你只是把文件放在 <code>sth</code> 文件夹下，<code>Git</code> 仓库不知道你放进去了，就没办法进行版本管理（回退等），只有你进行提交之后才能进行记录与管理，只有提交的文件才能被推送到指定远程仓库。</p><h2 id="添加远程仓库"><a href="#添加远程仓库" class="headerlink" title="添加远程仓库"></a>添加远程仓库</h2><ol><li><p>在自己的 Github 网页版建一个远程仓库：点击 <code>New repository</code>，起好名字之后，你可以选择该仓库是否公开 <code>Public</code> 或私有 <code>Private</code>，也可以选择是否为这个仓库添加一个说明问价 <code>Add a README file</code>，当然也可以什么都不点击就直接 <code>Create repository</code>；</p></li><li><p>在你想要连接的本地仓库内（文件夹下），添加这个仓库远程地址：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin git@github.com:your_github_name/repository_name.git </span><br><span class="line"># 或者下面这种添加也可以</span><br><span class="line">git remote add origin https://github.com/your_github_name/repository_name</span><br></pre></td></tr></table></figure><p>可以调用命令行看看当前文件夹或仓库的远程库信息：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git remote -v</span><br><span class="line"># 输出可能如下，或没链接远程仓库</span><br><span class="line">origin git@github.com:xxxxxxx.git (fetch)</span><br></pre></td></tr></table></figure><p>也可以删除远程仓库（比如添加的时候写错地址等），根据名字删除 <code>origin</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote rm origin</span><br></pre></td></tr></table></figure></li><li><p>仍在本地仓库内（文件夹下），推送到远程仓库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">git add .                    #如果你之前已经添加，可以不进行此步</span><br><span class="line">git commit -m &quot;description&quot;</span><br><span class="line"></span><br><span class="line">#由于你新建的远程仓库是空的，所以有-u这个参数</span><br><span class="line">git push -u origin main      #推送到main分支，日后你可以创建多个分支管理</span><br><span class="line"></span><br><span class="line">#日后就可以取消-u参数来进行推送</span><br><span class="line">git push origin main</span><br></pre></td></tr></table></figure></li></ol><h2 id="拉取远程仓库"><a href="#拉取远程仓库" class="headerlink" title="拉取远程仓库"></a>拉取远程仓库</h2><ul><li><p>一般的，你可以直接运行下述命令，将连接的远程仓库直接拉取下来：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull origin main</span><br></pre></td></tr></table></figure><p><strong>但是注意</strong>：这会直接覆盖掉你本地仓库，当然你可以新建一个本地空仓库，链接到远程这个仓库，在拉取也行！</p></li><li><p>可以换一种方法：先将本地代码放到暂存区，拉取之后再将暂存区代码放回本地</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git stash save &quot;description&quot;  #放到暂缓区</span><br><span class="line">git pull                      #拉取</span><br><span class="line">git stash pop                 #将暂缓区放回本地</span><br></pre></td></tr></table></figure><ul><li><strong>但是</strong>：事实上这是为了解决这个问题存在的：甲乙两个程序员正在合作开发一款程序，甲晚上10点push了自己编辑的 test.txt 文档到GitHub；乙11点也想要 push 自己编辑过后的 test.txt 文档到 GitHub，因为乙本地仓库不是最新，乙push时会报错，乙如果使用 git pull 拉取最新版本，则自己修改的东就会丢失！</li><li>所以解决思路是：先把自己修改好的代码存放在缓存里，等最新代码拉取下来以后再恢复缓存里的自己修改的代码，乙再push，前提是两人不要修改相同的地方，可能会产生冲突！</li></ul></li></ul><h2 id="拉取远程仓库指定单个文件夹"><a href="#拉取远程仓库指定单个文件夹" class="headerlink" title="拉取远程仓库指定单个文件夹"></a>拉取远程仓库指定单个文件夹</h2><p>现在有一个 <code>test</code> 仓库<a href="https://github.com/mygithub/test，你要">https://github.com/mygithub/test，你要</a> <code>gitclone</code> 里面的 <code>tt</code> 子目录。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git config core.sparsecheckout true                     #设置允许克隆子目录</span><br><span class="line">echo &#x27;tt*&#x27; &gt;&gt; .git/info/sparse-checkout                 #设置要克隆的仓库的子目录路径 !空格别漏!</span><br><span class="line">git remote add origin git@github.com:mygithub/test.git  #这里换成你要克隆的项目和库，就是将你想拉取的远程仓库添加</span><br><span class="line">git pull origin master                                  #下载</span><br></pre></td></tr></table></figure><h2 id="克隆仓库"><a href="#克隆仓库" class="headerlink" title="克隆仓库"></a>克隆仓库</h2><p>除了上述必须添加远程仓库的克隆方法，最简单的可哦那个仓库方法应该是</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/mygithub/test.git #其实候直接复制仓库链接没有.git也可以</span><br></pre></td></tr></table></figure><h2 id="指定分支上克隆代码"><a href="#指定分支上克隆代码" class="headerlink" title="指定分支上克隆代码"></a>指定分支上克隆代码</h2><p><code>git clone 地址</code> 默认从 <code>main/master</code> 分支上克隆，如下展示从指定分支上克隆代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone -b 分支名称 地址</span><br></pre></td></tr></table></figure><table frame=void>  <!--使用table标签，且frame=void消除外边框-->    <tr>           <!--<tr>一行的内容<\tr>，<td>一个格子的内容<\td>-->        <td>            <center>            <img src="/image/git_1.png"                        width="542" height="309"/>            </center>        </td>   <!--<center>标签将图片居中-->    </tr></table><p>示例：<code>git clone -b v2.8.1 https://git.oschina.net/oschina/android-app.git</code></p><h2 id="子模块下载or更新"><a href="#子模块下载or更新" class="headerlink" title="子模块下载or更新"></a>子模块下载or更新</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone --recursive 地址</span><br></pre></td></tr></table></figure><p>下面这个命令是在已经克隆下这个库后，开始下载子模块；当然如果你不确定子模块是否都已经下载完全了，也可以执行一遍下面这个命令确认一下。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git submodule update --init --recursive</span><br></pre></td></tr></table></figure><p>更新子模块</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git submodule sync --recursive</span><br><span class="line">git submodule update --init --recursive</span><br></pre></td></tr></table></figure><h1 id="上传大文件（-gt-100M）"><a href="#上传大文件（-gt-100M）" class="headerlink" title="上传大文件（&gt;100M）"></a>上传大文件（&gt;100M）</h1><p>话说在前头，<strong>真不建议用 Github 储存这么大的文件！很费事！三思！</strong></p><ol><li><p>安装 <a href="https://git-lfs.github.com/">Git Large File Storage(LFS)</a> 工具，官网直接下载安装即可；</p></li><li><p>安装后把里面的 <code>git-lfs.exe</code> 放到你要上传的项目/仓库文件夹（最好是新建的文件夹仓库）；</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure></li><li><p><strong>如果你是在push完后被提示需要用LFS文件的话，还需要回退版本，这步非常重要</strong>；如果你还没有push过大文件，可以忽略这个步骤。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git log</span><br><span class="line"># 查看版本，选择你最后成功push的一次版本</span><br><span class="line">git reset commit号</span><br></pre></td></tr></table></figure></li><li><p>对于本地 git 仓库下，安装 <code>lfs</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git lfs install</span><br></pre></td></tr></table></figure></li><li><p>上传文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">git lfs track *        #追踪要上传的大文件，*表示路径下的所有文件</span><br><span class="line">git add .gitattributes #添加先上传的属性文件(要先上传属性文件，不然有可能失败)</span><br><span class="line">git commit -m &quot;pre&quot;    #添加属性文件上传的说明</span><br><span class="line">git remote add origin https://github.com/name/repo.git #建立本地和Github仓库的链接</span><br><span class="line">git push origin master #上传属性文件</span><br><span class="line">git add *              #添加要上传的大文件，*表示路径下的所有文件</span><br><span class="line">git commit -m &quot;Git LFS commit&quot; #添加大文件上传的说明</span><br><span class="line">git push origin master         #上传大文件</span><br></pre></td></tr></table></figure></li></ol><h1 id="常见问题及方法"><a href="#常见问题及方法" class="headerlink" title="常见问题及方法"></a>常见问题及方法</h1><h2 id="error-10054"><a href="#error-10054" class="headerlink" title="error 10054"></a>error 10054</h2><p><code>fatal: unable to access ‘https://github.com/…’: OpenSSL SSL_read: Connection was reset, errno 10054</code>产生原因：一般是这是因为服务器的SSL证书没有经过第三方机构的签署，所以才报错。参考网上解决办法：解除ssl验证后，再次git即可</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global http.sslVerify false</span><br></pre></td></tr></table></figure><h2 id="推送或克隆超时（设置、取消代理）"><a href="#推送或克隆超时（设置、取消代理）" class="headerlink" title="推送或克隆超时（设置、取消代理）"></a>推送或克隆超时（设置、取消代理）</h2><p>设置代理：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global https.proxy</span><br></pre></td></tr></table></figure><p>取消代理：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global --unset https.proxy</span><br></pre></td></tr></table></figure><p>网上有人建议，先设置代理，再取消代理，然后再克隆。</p><h2 id="GIT推送错误error-RPC-failed；-curl-92-HTTP-2-stream-0-was-not-closed-cleanly-CANCEL-err-8"><a href="#GIT推送错误error-RPC-failed；-curl-92-HTTP-2-stream-0-was-not-closed-cleanly-CANCEL-err-8" class="headerlink" title="GIT推送错误error: RPC failed； curl 92 HTTP/2 stream 0 was not closed cleanly: CANCEL (err 8)"></a>GIT推送错误error: RPC failed； curl 92 HTTP/2 stream 0 was not closed cleanly: CANCEL (err 8)</h2><p>类似状况如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Counting objects: 100% (25515/25515), done.</span><br><span class="line">Delta compression using up to 4 threads</span><br><span class="line">Compressing objects: 100% (18794/18794), done.</span><br><span class="line">error: RPC failed; curl 92 HTTP/2 stream 0 was not closed cleanly: CANCEL (err 8)</span><br><span class="line">fatal: the remote end hung up unexpectedlyiB | 19.00 KiB/s</span><br><span class="line">Writing objects: 100% (25503/25503), 28.46 MiB | 298.00 KiB/s, done.</span><br><span class="line">Total 25503 (delta 5409), reused 25463 (delta 5393), pack-reused 0</span><br><span class="line">fatal: the remote end hung up unexpectedly</span><br><span class="line">Everything up-to-date</span><br></pre></td></tr></table></figure><p>错误原因：error:RPC failed; curl 92 HTTP/2 stream 0 was not closed cleanly: CANCEL (err 8)<br>错误：RPC失败;HTTP/2 stream 0没有完全关闭:CANCEL (err 8)</p><p>解决方法：<br>方法一：将git 远程地址改为自己的ssh地址</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote set-url origin git@github.com:github用户名/仓库名.git</span><br></pre></td></tr></table></figure><p>方法二：增加git缓冲区大小</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global http.postBuffer 524288000</span><br></pre></td></tr></table></figure><h2 id="无法推送到某分支"><a href="#无法推送到某分支" class="headerlink" title="无法推送到某分支"></a>无法推送到某分支</h2><p>问题</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git push -u origin main</span><br><span class="line">error: src refspec main does not match any</span><br><span class="line">error: failed to push some refs to &#x27;https:</span><br></pre></td></tr></table></figure><p>原因是没发现main分支,切换到main分支,再次测试即可</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout main</span><br><span class="line">Switched to branch &#x27;main&#x27;</span><br><span class="line">Your branch is up to date with &#x27;origin/main&#x27;</span><br></pre></td></tr></table></figure><p>或者可以用git branch查看有哪些分支</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 查看本地分支</span><br><span class="line">$ git branch</span><br><span class="line">* main</span><br><span class="line">  master</span><br><span class="line"></span><br><span class="line"># 查看远程分支</span><br><span class="line">$ git branch -r</span><br><span class="line">  origin/main</span><br><span class="line">  origin/master</span><br><span class="line">  </span><br><span class="line"># 查看所有分支,包括本地和远程</span><br><span class="line">$ git branch -a</span><br><span class="line">* main</span><br><span class="line">  master</span><br><span class="line">  remotes/origin/main</span><br><span class="line">  remotes/origin/master</span><br></pre></td></tr></table></figure><p>总结：无法推送到某分支,先确认是否有该分支</p><h2 id="查看仓库信息"><a href="#查看仓库信息" class="headerlink" title="查看仓库信息"></a>查看仓库信息</h2><p>config 配置有system级别 global（用户级别） 和local（当前仓库）三个 设置先从system-&gt;global-&gt;local 底层配置会覆盖顶层配置，分别使用 <code>--system/global/local</code> 可以定位到配置文件：</p><p>查看系统config：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --system --list</span><br></pre></td></tr></table></figure><p>查看当前用户（global）配置：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global --list</span><br></pre></td></tr></table></figure><p>查看当前仓库配置信息</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --local --list</span><br></pre></td></tr></table></figure><h2 id="查看、修改用户名和邮箱"><a href="#查看、修改用户名和邮箱" class="headerlink" title="查看、修改用户名和邮箱"></a>查看、修改用户名和邮箱</h2><p>查看用户名和邮箱地址：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config user.name</span><br><span class="line">git config user.email</span><br></pre></td></tr></table></figure><p>修改用户名和邮箱地址：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;username&quot;</span><br><span class="line">git config --global user.email &quot;email&quot;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Code </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
